#+TITLE: Improvements of reaching definitions analysis in Symbiotic
#+AUTHOR: Tomáš Jašek
#+LATEX_CLASS:         fithesis
#+OPTIONS:             todo:nil toc:nil
#+LATEX_CLASS_OPTIONS: [nolot,nolof,digital,twoside]
#+LATEX_HEADER:        \input{setup.tex}
* TODO Introduction

Nowadays, computer programs control gadgets everywhere around
us. Programs are used in wide variety of fields that impose diverse
requirements on their reliability. For example, in medical
applications it is crucial that a program which controls a piece of
medical equipment does not misbehave or crash under any circumstances.

On the other hand, programmers make mistakes very often. For example,
a programmer could easily forget to close a brace or add a semicolon
at the end of a line. Mistakes similar to those result into syntax
errors. Compiler is able to detect them while processing the program
and sometimes offer a suitable replacement, too. Compilers, however,
do not understand intents of a human programmer. Therefore, it is
still possible for the programmer to express their intents in an
incorrect way producing a logical error. Usually, compiler is also
unable to detect mistakes that make the program access memory which is
owned by, for example, some other program. On modern operating
systems, these mistakes usually cause the program to crash.

Despite the availability of safe modern low-level programming
languages, such as Rust, the C programming language is still very
widespread. It is being used for critical purposes such as operating
systems, embedded systems and device drivers. With its flexibility, it
has some shortcomings that cause programmers to make mistakes
often. One of the shortcomings is, that memory management is left to
the programmer. A programmer could easily forget to free allocated
memory, unintentionally free the same memory twice or forget to check
the result of a memory allocation function, which may fail.

Several formal verification tools are able to detect such errors. One
of them is \sbt{}. \sbt{} firstly inserts error checks into the
program, then reduces the size by removing irrelevant parts of the
program and passes it to symbolic execution framework klee which
performs the verification itself. As formal verification is an
expensive process, it is suitable to remove parts of the program that
do not influence result of the verification process. \sbt{} uses
program slicing based on the DG library\nbsp{}\cite{ChalupaDG} for this.

One of the analyses used during slicing is reaching definitions
analysis. It identifies all instructions that may generate a value of
a variable before the place where the value is used. Computing
reaching definitions in programs without pointers is trivial. However,
computing reaching definitions for in the presence of pointers is more
complex, as the analysis needs to deal with dynamic data structures,
pointers that might point to multiple variables and arrays.

The aim of this thesis is to study modern techniques of computing
reaching definitions, implement a faster reaching definitions analysis
to\nbsp{}\sbt{} and compare the new implementation with the original one
experimentally using a non-trivial set of benchmarks.

This thesis is divided into individual chapters as follows: Chapter
\ref{ch:ProgAnalysis} introduces the reader to program analysis in
general. Key terms, such as program, control flow graph or static
single assignment form are defined there. Chapter\nbsp{}\ref{ch:RDA} starts
by defining what a reaching definition is, what is a reaching
definitions analysis and discusses the simplest naive algorithm to
compute reaching definitions in programs without pointers. Analyzing
programs with pointers is discussed next. Then, the chapter continues
by discussing various algorithms to compute reaching definitions. In
Chapter\nbsp{}\ref{ch:Symbiotic}, \sbt{} is introduced and its architecture
is explained. Chapter\nbsp{}\ref{ch:Implementation} discusses LLVM, the DG
library and the chosen implementation approach based on Marker
algorithm\nbsp{}\cite{BraunSSA}. Chapter\nbsp{}\ref{ch:Experiment}
shows experimental comparison of the original implementation of
reaching definitions analysis and the new
implementation. Implementations are compared in terms of time and
accuracy. Chapter\nbsp{}\ref{ch:Experiment} offers experimental evaluation
of the new analysis and comparison with the original
implementation. Chapter\nbsp{}\ref{ch:Summary} presents possibilities for
future work and summarizes the thesis.

* DONE Program analysis
\label{ch:ProgAnalysis}

Tools that find errors in programs use results of program analysis to
decide whether or not there is an error in the program. There are two
kinds of program analysis: dynamic analysis and static analysis. An
error-finding tool may use either approach or some combination of
both. The following sub-chapter briefly introduces both.

** DONE Static and dynamic program analysis

*Dynamic analysis* is capable of detecting an erroneous state in the
program while it is running. For example, one could replace C =malloc=
and =free= functions by functions that track the allocations. This is
how valgrind[fn::http://valgrind.org/] works. Another familiar example
of a dynamic analysis is unit testing. During unit testing, the
program(or its part) is started with a user-specified input and after
finishing, its output is checked. If a dynamic analysis uncovers an
error, there is definitely an error in the program. The main
disadvantage is that if the analysis did not uncover any error, there
could still be an input that produces an error.

*Static analysis* derives program properties from some program's
representation without directly executing it. It could be the source
code, compiled machine code or some kind of intermediate
representation (such as Java bytecode or LLVM bitcode). Static
analysis is able to uncover errors without knowing the input of the
program. If an error is discovered, static analysis can synthesize an
instance of input data for which the program enters the erroneous
state. The main advantage is, that this technique can discover errors
overlooked both by programmers and testers. Another advantage is, it
does not require instance of input data, as it is not based on running
the program. However, it can be very expensive, especially
when the analyzed program is too big or complex.

This thesis focuses on reaching definitions analysis, which is a
static analysis. The following sub-chapter continues with a more in-depth
introduction to the static analysis.

** DONE Key terms of static program analysis
\label{ch:KTPA}
#+BEGIN_LaTeX
  \begin{figure}
    \begin{minipage}[b]{0.5\textwidth}
      \begin{lstlisting}[language=C]
        int $i$;
        scanf("%d", &i);
        if ($i$ % 2 == 0)
            puts("even");
        else
            puts("odd");
        puts("exit");
      \end{lstlisting}
    \end{minipage}
    \begin{minipage}[t]{0.5\textwidth}
      \begin{tikzpicture}
      \tikzstyle{arr} = [->,shorten <=1pt,>=stealth',semithick]
        \node[draw, rectangle] (A) at (0, 0) {int $i$};
        \node[draw, rectangle] (B) at (0, -1.2) {scanf("\%d", \&$i$)};
        \node[draw, rectangle] (C) at (0, -2.4) {if $i$ \% 2 == 0};
        \node[draw, rectangle] (D) at (-1.5, -3.6) {puts("even")};
        \node[draw, rectangle] (E) at (1.5, -3.6) {puts("odd")};
        \node[draw, rectangle] (F) at (0, -4.8) {puts("exit")};
        \draw[arr] (A) -- (B);
        \draw[arr] (B) -- (C);
        \draw[arr] (C) -- (D);
        \draw[arr] (C) -- (E);
        \draw[arr] (D) -- (F);
        \draw[arr] (E) -- (F);
      \end{tikzpicture}
    \end{minipage}
    \caption{Program in C language and its control flow graph}
    \label{fig:programCFG}
  \end{figure}
#+END_LaTeX

A\nbsp{} /program/ is a sequence of elementary instructions.  Program's
structure is reflected in its /control flow graph/. Formally, /control
flow graph/ (CFG for short\index{CFG}) of a program $\mathcal P$ is a
graph $G = (V, E)$, where $V$ is a finite set of vertices and $E
\subseteq V \times V$ is a set of edges. Each instruction of $\mathcal P$
is represented by a vertex. If there exists a run of the program
$\mathcal P$ where instruction $I_2$ is executed immediately after
instruction $I_1$, then $(I_1, I_2) \in E$. We ignore labels on branches,
as they are not needed for reaching definitions
analysis.
A /path/ in a CFG $(V, E)$ is a sequence $v_1, v_2, v_3, \cdots, v_n$ such, that:

- $v_1,v_2, v_3, \cdots, v_n \in V$, where $n \in \mathbb N$
- $\forall 1 \le i < n: (v_i, v_{i+1}) \in E$

Figure\nbsp{}\ref{fig:programCFG} shows a simple program in C
language and its control flow graph.


\label{domTree} Algorithms that process CFG of a program make use of
some relations defined on nodes of the CFG. Dominance\nbsp{}\cite{TarjanDom}
is one of the most important relations for algorithms presented in
this thesis, so let us define it. Let $(V, E)$ be a CFG. $x \in V$
/dominates/ $y \in V$ if and only if $x \in V$ is on every path from the
entry node to $y$. If $x$ dominates $y$ and $x \ne y$, then $x$ is a
/strict dominator/ of $y$. The closest strict dominator of $y$ is the
/immediate dominator/ of $y$ on any path from entry node to $y$ in
CFG. Dominator tree is a graph $(V, E_d)$, where vertices are from CFG
and $(x, y) \in E_d$ if and only if $x$ is immediate dominator of $y$.

Executable programs that are not libraries have an entry point. In C
programs, this is a function called =main=. The first CFG node of the
main function is referred to as /entry node/.

Programs are commonly divided into /basic blocks/. A basic block is a
maximum sequence of elementary instruction that does not contain a
jump instruction. Every instruction of the program is part of some
basic block.

A\nbsp{} /variable/ is a fixed-size storage cell in memory. A\nbsp{}
/definition/ of a variable is any instruction that 
modifies its value. A\nbsp{} /use/ of a variable is any instruction
that reads its value.

** DONE Static single assignment form
Programs may be transformed without changing their behaviour. One of
transformations that do not change program's behaviour is
transformation to Static Single Assignment form (or SSA for
short)\nbsp{}\cite{CytronSSA}. The transformation itself yields some useful data about the
program and the SSA form is particularly useful for compilers and code
analyzers.

#+BEGIN_LaTeX
    \begin{figure}[]
    \begin{minipage}[t]{0.5\textwidth}
      \begin{lstlisting}[language=C]
        int $i$ = 1;
        int $j$ = 1;
        $i$ = $i$ + $j$;
        $j$ = $j$ + $i$;
        foo($i$, $j$);
      \end{lstlisting}
    \end{minipage}
    \begin{minipage}[t]{0.5\textwidth}
      \begin{lstlisting}[language=C]
      int $i_1$ = 1;
      int $j_1$ = 1;
      $i_2$ = $i_1$ + $j_1$;
      $j_2$ = $j_1$ + $i_2$;
      foo($i_2$, $j_2$);
      \end{lstlisting}
    \end{minipage}
    \caption{Program and its SSA form}
    \label{fig:programSSA}
    \end{figure}
#+END_LaTeX

A program $\mathcal P$ is in /Static Single Assignment form/ if, and
only if for each variable in $\mathcal P$, there is exactly one
definition\nbsp{}\cite{RosenGVNRC}. Figure\nbsp{}\ref{fig:programSSA} shows a simple program and its
SSA form.

#+BEGIN_LaTeX
  \begin{figure}
      \begin{lstlisting}[language=C]
  int $i$ = 0; /\encircle{1}/
  while ($i$ < 10) {
      printf("%d\n", $i$); /\encircle{2}/
      $i$++;  /\encircle{3}/
  }
      \end{lstlisting}

    \caption{Simple C program with loops}
    \label{fig:loop1}
    \end{figure}
#+END_LaTeX

Constructing SSA form is a little more interesting in case the CFG of
a program contains loops. Consider program in Figure\nbsp{}\ref{fig:loop1}.

\noindent While constructing SSA form of this program, the use of $i$ variable
at location \encircle{2} could be replaced by the assignment to $i$ at location
\encircle{1} or \encircle{3}. The problem is, that both of these statements contribute
to the value of $i$ at location \encircle{2}. It is, therefore, necessary to
use some kind of combination of values from \encircle{1} and \encircle{3}. This is what
a \phi function is for. $i_3 = \phi(i_1, i_2)$ denotes, that the value
of $i_3$ could be either $i_1$ or $i_2$. After transforming the program from figure\nbsp{}\ref{fig:loop1}
to SSA form, it looks as shown in figure\nbsp{}\ref{fig:loop2}.

#+BEGIN_LaTeX
  \begin{figure}[h]
    \begin{lstlisting}[language=C]
      int $i_1$ = 0;
      int $i_2$;
      int $i_3$;

      while ($i_2 = \phi(i_1, i_3), i_2 < 10$) {
        printf("%d\n", $i_2$);
        $i_3$ = $i_2$ + 1;
      }
    \end{lstlisting}
\caption{SSA form of the program from figure~\ref{fig:loop1}}
\label{fig:loop2}
  \end{figure}
#+END_LaTeX

* TODO Reaching definitions analysis
\label{ch:RDA}
This chapter starts by explaining what a reaching definition is and
demonstrating the simplest naive algorithm for computing reaching
definitions. It continues by discussing properties of reaching
definitions analyses and introduces various algorithms to compute
reaching definitions.

\label{def:RD}Let $\mathcal P$ be a program. A /reaching definition/
\index{RD} of variable $\mathcal V$ used by instruction $I_1$ is an
instruction $I_2$ such, that:
+ $I_1, I_2$ are part of $\mathcal P$
+ $I_1$ is a use of variable $\mathcal V$
+ $I_2$ is a definition of variable $\mathcal V$
+ there exists a run of $\mathcal P$ where the value of $\mathcal V$ was not
  overwritten by any instruction on path from $I_2$ to $I_1$ in the CFG

#+BEGIN_LaTeX
  \begin{figure}[h]
    \begin{minipage}[b]{0.5\textwidth}
      \begin{lstlisting}[language=C]
        int i = 5;
        int j = 4;
        
        if (i == 0) {
          j = 1;
        } else if (i == 2) {
          j = 3;
        }
        printf("%d", j);
      \end{lstlisting}
    \end{minipage}
    \begin{minipage}[t]{0.5\textwidth}
      \begin{tikzpicture}

        \tikzstyle{arr} = [->,shorten <=1pt,>=stealth',semithick];
        \tikzstyle{rd} = [->,shorten <=1pt,>=stealth',dashed];

        \node[draw, rectangle] (declI) at (0, 0) {int $i = 5$};
        \node[draw, rectangle] (declJ) at (0, -1.2) {int $j = 4$};

        \node[draw, rectangle] (C) at (0, -2.4) {if $i$ == 0};
        \node[draw, rectangle] (D) at (-2.0, -3.6) { j = 1 };
        \node[draw, rectangle] (E) at (1.5, -3.6) { if $i == 2$ };
        \node[draw, rectangle] (F) at (0, -4.8) { $j = 3$ };
        \node[draw, rectangle] (G) at (-1.0, -6) { printf("\%d", $j$ ) };

        \draw [arr] (declI) -- (declJ);
        \draw [arr] (declJ) -- (C);
        \draw [arr] (C) -- (D);
        \draw [arr] (C) -- (E);
        \draw [arr] (D) -- (G);
\draw [arr] (E) -- (F);
        \draw [arr] (F) -- (G);
        \draw [arr] (E.south) to [out=-90,in=0] (G.east);
        \draw [rd]  (C.west) to [out=150,in=180] (declI.west);
        \draw [rd]  (E.east) to [out=0,in=0] (declI.east);
        \draw [rd] (G.north) to [out=90,in=0] (D.east);
        \draw [rd] (G.east) to [out=0,in=0] (F.east);
        \draw [rd] (G.east) to [out=0,in=0] (declJ.east);
      \end{tikzpicture}
    \end{minipage}
    \caption{Program in C language, its CFG and reaching definitions. Solid edges are part of CFG, dashed edges represent reaching definitions.}
    \label{fig:programRD}
  \end{figure}
#+END_LaTeX

Figure \ref{fig:programRD} shows program and its CFG with reaching
definitions.

Reaching definitions are calculated by a reaching definitions analysis
(RDA for short).

** TODO Dense reaching definitions analysis
# TODO rework
\label{denseRDA} One of the ways to compute reaching definitions is
to ``follow'' the control flow graph of the program while remembering
the last definition for each variable for each of its vertices. This
is a traditional approach used by many tools.

Literature commonly uses $KILL$ and $GEN$ sets to describe how the RDA
operates. Let $(V, E)$ be a CFG. For every $x \in V$, $GEN(x)$ is set of
variables for which $x$ is a definition. $KILL(x) \subseteq GEN(x)$ is
set of variables that are overwritten in this vertex.

Figure \ref{fig:denseRDA} demonstrates the algorithm.

#+BEGIN_LaTeX
  \begin{figure}[H]
    \begin{algorithm}[H]
      \SetAlgoVLined
      \KwData{Control Flow Graph as $(V, E)$, for every $v \in V$, $v.defs$ is a set of variables defined in $v$}
      \KwResult{for every $v \in V$ and every variable $x$, $v.rd(x)$ is a set of reaching definitions for variable $x$ in $v$}
      
      \For{$v \in V$} {
        \For{$def(x) \in v.defs$} {
          $v.rd(x) \gets v.rd(x) \cup \{ v \}$ \;
        }
      }
      \While{\text{not fixpoint}} {
        \For{$v \in V$ in DFS order} {
          \For{$(u, v) \in E$} {
            \For{$def(x) \in u.defs$} {
              $v.rd(x) \gets v.rd(x) \cup \{ u \}$ \;
            }
          }
        }
      }
    \end{algorithm}
    \caption{Dense reaching definitions analysis algorithm}
    \label{fig:denseRDA}
  \end{figure}
#+END_LaTeX

The algorithm starts by adding reaching definitions to CFG nodes that
are definitions. Then, the reaching definitions are propagated
throughout the entire CFG of the program until fixpoint is
reached.

** DONE Properties of reaching definitions analyses
It is impossible for reaching definitions analyses to find precise
definitions of a specified ``variable''. Because of that, it is
necessary to perform an abstraction 

Reaching definitions analyses have some properties\nbsp{}\cite{rptRDA} that
affect their accuracy. Less accurate analyses need to make some
conservative assumptions about the program in order to be
correct. This sub-chapter describes three properties of reaching
definitions analyses: instance-wiseness, field sensitivity and ability
to recognize execution patterns.

*** DONE Instance-wise and statement-wise analysis
When analyzing programs with a cyclic CFG, there are multiple
/instances/ of instructions that can be executed repeatedly. Each
execution of an instruction creates a new instance of the instruction.

Along with the definition, use and variable, an instance-wise reaching
definitions analysis is able to tell which instance of the
instructions are involved. The information about instance might
involve for example the for loop indexing variable $i$. There might be
more variables in case the instruction is inside of a nested loop.

#+BEGIN_LaTeX
  \begin{figure}
    \begin{lstlisting}[language=C]
      int $a$ = 0; /\encircle{1}/

      for(int $i$ = 0; $i$ < 5; ++$i$) {
        int $b$ = $a$ + $i$; /\encircle{2}/
        $a$ = $b$; /\encircle{3}/
      }
      \end{lstlisting}
      \caption{Demonstration of differences between statement-wise and instance-wise analysis}
      \label{fig:instWise}
      \end{figure}
#+END_LaTeX

Differences between instance-wise analysis and statement-wise analysis
will be demonstrated on a simple program in figure
\ref{fig:instWise}. Reaching definitions for $a$ at location
\encircle{2} are \encircle{1} and \encircle{3}. However, there are
multiple instances of instructions at \encircle{2} and
\encircle{3}. Firstly, both instance-wise and statement-wise analyses
would report, that \encircle{1} is a reaching definition of $a$ at
\encircle{2}. The difference is, how much information the analysis is
able to provide about the reaching definition \encircle{3} at
\encircle{2}. Statement-wise analysis would simply state, that
\encircle{3} is a reaching definition of $a$ at
\encircle{2}. Instance-wise analysis goes a little further by
reporting, that $\encircle{3}^{i+1}$ is a reaching definition of $a$ at
$\encircle{2}^i$. The upper index denotes the index of iteration.

*** DONE Field sensitivity
Usage of aggregated data structures, such as arrays or C language
=struct=-s introduces another issue that needs to be addressed by a
reaching definitions analysis. Precision of analysis for programs that
use aggregated data structures depends on whether the analysis can
distinguish between individual elements of the data structure.

#+BEGIN_LaTeX
  \begin{figure}
    \begin{lstlisting}[language=C]
      int $a$[5];
      $a$[0] = 1; /\encircle{1}/
      $a$[1] = 2; /\encircle{2}/
      foo($a$[2]); /\encircle{3}/
    \end{lstlisting}
    \caption{Demonstration of field-sensitive reaching definitions analysis}
    \label{fig:rdaFS}
    \end{figure}
#+END_LaTeX

Consider the program in Figure\nbsp{}\ref{fig:rdaFS}. Locations \encircle{1}
and \encircle{2} in the program define the first and the second
element of $a$. After that, location \encircle{3} contains a function
call that uses the third element of the array. This element has no
definitions in the program, so an accurate reaching definitions should
find no definitions for it.

A field-sensitive analysis considers array indices and correctly
reports no reaching definitions for $a[2]$ at location \encircle{3}.

A field-insensitive analysis ignores indices of the array and for
location \encircle{3}, it would report, that reaching definitions of
$a[2]$ are \encircle{1} and \encircle{2}. This is an
over-approximation that has to be performed by the field-insensitive
analysis.
*** DONE Execution patterns recognition

#+BEGIN_LaTeX
  \begin{figure}
    \label{fig:execPatterns}
    \begin{lstlisting}[language=C]
      int foo(int $a$) {
        int $c$ = 0;
        if ($a$ < 0) {
          $c$ = 1; /\encircle{1}/
        }
        if (a >= 0) {
          $c$ = 2; /\encircle{2}/
        }
        return $c$; /\encircle{3}/
      }
    \end{lstlisting}
    \caption{Demonstration of effects of execution patterns recognition on reaching definitions analysis}
  \end{figure}
#+END_LaTeX

Reaching definitions analysis is often not the only analysis that is
part of a program analysis framework. More often than not, there are
more analyses that derive various properties of program or its
parts. Reaching definitions analysis can sometimes take advantage of
results of previously ran analyses and achieve better accuracy or
speed.

Consider the program in figure\nbsp{}\ref{fig:execPatterns}. If an external
analysis reports that there is no program execution where $a < 0$, the
reaching definitions analysis could take this into account and derive
that \encircle{1} is not a reaching definition of $c$ at \encircle{3}
even despite the fact it is a definition of a simple
variable. Analysis that does not take it into account would report
that both \encircle{1} and \encircle{2} are reaching definitions of
$c$ at \encircle{3}.

In this case, an analysis that does not recognize execution patterns
yields an over-approximation, which is not a problem.

*** TODO Using strong and weak definitions
# TODO polish
Accuracy of the analysis affects performance. Because of that, it is
desirable to trade some accuracy for better performance in some
cases. In order not to sacrifice too much accuracy, analyses
distinguish between strong and weak definitions.

A\nbsp{}strong definition over-writes the variable with a new value. When
a\nbsp{}strong definition is encountered, it invalidates all previous
definitions of the variable. Weak definition, on the other hand, does
not necessarily over-write the variable, so it does not invalidate
previous definitions.

** TODO Analyzing programs that use pointers
One of the most important features of programming languages are
pointers. They can be utilized to implement dynamic data structures,
which are very widely used. As pointers make it possible to create
variables that refer to variables, they inherently make programs more
difficult to understand and analyze. In order to compute reaching
definitions in programs that use pointers, an RDA may use information
from pointer analysis which took place prior to the RDA.

*** DONE Pointer analysis
Pointer analysis\nbsp{}\cite{ChalupaPTA} is, similarly to reaching
definitions analysis, a static program analysis. It computes a set
$\mathcal V$ of variables for each pointer $p$. This set will be
referred to as /points-to/ set. If $p$ may point to some variable $v$,
then $v \in \mathcal V$.

Reaching definitions analysis uses these data from pointer analysis to
recognize possible uses and definitions of variables. Accuracy of the
reaching definitions analysis, therefore, depends on accuracy of the
underlying pointer analysis. Namely, when the pointer analysis
performs an over-approximation, so will the reaching definitions
analysis.

*** TODO Weak definitions in programs with pointers
\label{strongWeakUpdate} Reaching definitions analyses that process
programs with pointers can take advantage of weak definitions to solve
some challenges connected with processing pointers.

The first case is, that a pointer could point to multiple
variables. In this case, every definition of such pointer can be
considered as a weak definition.

# TODO
- pointer might point to multiple variables
- memory allocated on heap

** DONE Demand-driven reaching definitions analysis
Apart from the dense algorithm, several other algorithms to compute
reaching definitions have been introduced. Other algorithms are
generally based on traversing the CFG of a program and processing only
definitions and uses of variables. They also attempt to eliminate need
to use fixpoint in the computation. This subchapter briefly introduces
demand-driven reaching definitions analysis\nbsp{}\cite{SootDDRDA}.

The main idea of this approach is to answer the question ``can a
definition $d$ of variable $v$ reach a program point $p$?''. This
question is referred to as /query/ and it is represented by a triple
$(d, p, v)$. After a query is generated, it is propagated backwards
along nodes of the CFG. Each node may either answer the query or
continue the propagation to its predecessors. If a node $x$ contains a
definition of $v$, the query propagation stops. The answer is yes, if
and only if $x = d$. If $x \ne d$, then node $x$ kills the definition
$d$ before it can reach $p$ along the path.

In case a program point $p$ has $n$ predecessors, it is sufficient
that the reachability of $d$ is reported by at least one of them.

It is worth noting, that this approach has a special property that
makes it suitable for a slicer: It is able to start from the slicing
criterion and gradually find all definitions that affect the
criterion. This way, it can avoid computing irrelevant information.

** TODO Sparse dataflow analysis
# TODO polish, maybe extend
Another approach to computing reaching definitions was introduced by
Madsen and M\o{}ller \cite{MadsenSDAPR}. This approach requires
pre-computing dominator tree\nbsp{}\cite{CytronSSA} for nodes of the
CFG, as explained in section\nbsp{}\ref{domTree}.

When the algorithm encounters a use of a variable, it searches
dominator tree of the program backwards until it finds a definition of
the same variable. The triple $(d, v, u)$ where $d$ is a definition of
a variable $v$ and $u$ is a use of $v$, is then added to $DU$ set.

When a new definition $d_n$ of variable $v$ is encountered, the
algorithm finds a set $\mathcal D_p$ of previous definitions of
$v$. Then, for each $d_p \in \mathcal D_p$ where $d_n$ is a strict
dominator of $d_p$, all triples $(d_p, v, u) \in DU$ are removed from
$DU$.

While processing definitions and uses, the algorithm places \phi nodes
for variables when necessary. As a side-effect, SSA form of the
program is produced.

The input program is processed by the algorithm until fixpoint -- there is no new use
discovered.

** TODO Algorithms based on static single assignment form
\label{SSArd} Algorithms that transform a program into SSA form
replace modified variables in assignments by new, artificially-created
variables representing a new ``version'' of the variable. They also
replace variables in uses by the most recent definition -- reaching
definition. In a program that is already transformed into SSA form, it
is possible

# TODO program, SSA form, reaching definitions

For the purpose of this thesis, we have studied two algorithms for
computing SSA form. One of them has been introduced by Cytron et
al\nbsp{}\cite{CytronSSA}.  The second algorithm, invented by Braun et
al\nbsp{}\cite{BraunSSA}, is simpler and has been experimentally proven to
be as fast as the Cytron et al. algorithm\nbsp{}\cite{BraunSSA}.

*** TODO Cytron et al algorithm

Algorithm introduced by Cytron et al.\nbsp{}\cite{CytronSSA} uses dominance
information to pre-calculate locations of \phi nodes. In the later
phase, variables are numbered using a simple stack of counters and \phi
nodes are filled with operands.

This approach was proven to produce minimal SSA form.

*** TODO Braun et al algorithm
\label{marker}

Algorithm by Braun et al.\nbsp{}\cite{BraunSSA} will be used as a base for
implementation of the new analysis, so it is discussed more in depth.
It operates in two phases: local value numbering and global value
numbering. Both of these phases process basic blocks of the program in
the execution order.

During *local value numbering*, it computes SSA form of every basic
block of the program. For every basic block, it iterates through all
instructions in execution order. If an instruction $I$ defines some
variable $\mathcal V$, $I$ is remembered as the current definition of
$\mathcal V$. If an instruction $I$ uses some variable $\mathcal V$,
the algorithm looks up its definition. If there is a current
definition $\mathcal D$, the use of variable $\mathcal V$ is replaced
by use of the numbered variable that corresponds to $\mathcal D$.

*Global value numbering* is involved once no definition for the
specified variable can be found in the current basic block. The
algorithm places a \phi node on top of the current basic block and starts
recursively searching the CFG for the latest definition in all
predecessors of the current basic block. Once a definition is found,
it is added as an operand to the \phi node.

When looking up a definition of a variable from a predecessor basic
block, the basic block might not be processed by global value
numbering. If that is the case, the algorithm does not have any idea
about which variables are defined in that basic block. This happens
when the program's CFG is cyclic -- e.g. recursive function is called
or for loop is used. Because of that, the algorithm remembers the last
definition of variable in basic blocks during local value
numbering. If there is no last definition in a block, the lookup
continues to all predecessors recursively.

Along with the algorithm, Braun et al. present a way to reduce the
number of added \phi nodes, which allows their algorithm to produce
minimal SSA form.

#+BEGIN_LaTeX
  \begin{figure}
    \begin{algorithm}[H]
      \SetAlgoVLined
      \Function{\WriteVariable}{}
    \end{algorithm}
  \end{figure}
#+END_LaTeX

* DONE Symbiotic
\label{ch:Symbiotic} \sbt{} is a modular tool for formal verification
of programs working on top of the LLVM compiler
infrastructure\nbsp{}\cite{LLVM}. It is being developed at Faculty of
Informatics, Masaryk University. \sbt{} works by combining three
well-known techniques:

1. *Instrumentation* is responsible for inserting various error checks
   into the program. For example, when checking memory access errors,
   instrumentation is responsible for adding the allocated memory
   along with allocation size into a global data structure. When
   dereferencing a pointer, instrumentation inserts a check to verify
   whether this pointer is inside allocated bounds or not. An
   assertion that crashes the program if a dereference is out of
   bounds of allocated memory is inserted, too.
2. *Slicing*\nbsp{}\cite{ChalupaDG} is a technique that reduces the size of
   the program by removing parts that do not influence its behaviour
   with respect to a specified /slicing criterion/. Slicing criterion is an =assert=
   instruction. The slicer computes which instructions the slicing
   criterion is dependent on. For that, it uses results of reaching
   definitions analysis.
3. *Symbolic execution* is the last step. It is a technique that
   decides whether the program could violate a condition of some
   assertion in the program. Rather than requiring user input, it uses
   so-called symbolic values. Whenever there is a program branching
   based on the symbolic value, the symbolic virtual machine remembers
   a constraint of the value based on the branching condition. When an
   erroneous state is reached, the symbolic virtual machine reports
   the path in the program that leads to the error.

* TODO Implementation
\label{ch:Implementation} This chapter starts by introduction of the
implemented algorithm, then describes the DG library and the LLVM
infrastructure. Finally, an in-depth discussion of the new reaching
definitions analysis implementation.

** DONE Implemented reaching definitions analysis

The implemented reaching definitions analysis is based on the Marker
algorithm\nbsp{}\cite{BraunSSA}. As described in\nbsp{}\ref{marker}, the algorithm
transforms a program into SSA form, which is not exactly what we
need. We will start by adapting the algorithm to compute reaching
definitions.

*** DONE Computing reaching definitions from Marker algorithm
The Marker algorithm is capable of computing SSA form of a
program. The program, however, does not need to be transformed into
SSA form. The only task of an RDA is to obtain reaching
definitions. We split up the computation into two phases:
1. In the first phase, the implementation constructs sparse RD graph
   separately for every allocated variable. Sparse RD graph is a
   graph, where for every reaching definition $(I_1, I_2)$ exists a path
   $P = (p_1, p_2, \cdots, p_n)$ where $p_1 = I_1$ and $p_n = I_2$. Each node $p \in
   P$ is either a definition, use or a \phi node. The path may consist of
   multiple \phi nodes, but it might be trivial as well. The construction
   is straightforward: whenever a variable use $u$ is encountered,
   lookup all definitions of the variable using =readVariable=. When
   =\phi.appendOperand(x)= is called, add an edge $(x, \phi)$ to the
   sparse RD graph. Then, for the definition $d$ of the variable
   returned by =readVariable=, add an edge $(u, d)$ to the sparse RD
   graph.
2. In the second phase, the control flow graph $(V, E)$ of the program
   is traversed once again. For every use $u \in V$ of variable $v$, a
   BFS search of the sparse RD graph for $v$ is started in $u$. If the
   definition found is not a \phi node, it is added as a reaching
   definition. If it is a \phi node, the search continues to its
   predecessors.

The original analysis is field-sensitive. In the next section, we
modify the algorithm to be field-sensitive.

*** DONE Field sensitivity

Every definition and use have an associated interval of memory that is
being accessed by the instruction. Data structure used for
=current_def= and =last_def= does not consider the interval when
looking up definitions in =readVariable=. We have decided to design a
new custom data structure that considers the intervals while looking
up variables. The data structure works similarly to a map which maps
intervals to values of some type -- in this case CFG nodes. We call it
=IntervalMap=.

When a definition is encountered, it is necessary to save the interval
of the definition along with the CFG node where the definition is to
the custom data structure.

When a use is encountered, modified =readVariable= function looks up
overlapping definitions from the custom data structure. =readVariable=
is modified to return a set of definitions rather than a single
definition. That is because two or more subintervals of the used
interval could be defined by different instructions and all of the
instructions are reaching definitions, as they do not over-write one
another completely.

When =readVariable= finds a definition of a subinterval, which is
smaller than the use interval $i_U$, the lookup must continue to
predecessor blocks and attempt to find a set of intervals $\mathcal I$
such, that by uniting all $i \in \mathcal I$, we get an interval $i_R$
such, that $i_U \subseteq i_R$. The search for definition ends once the
set is found for every predecessor basic block of the current basic
block or when an entry node of the CFG is reached.

The =readVariableRecursive= function adds \phi nodes for the variable
when necessary. Whenever a \phi node is created, it has the same interval
of the variable as the use node it is created for.

Sometimes, the accessed interval of memory is not known at the time of
compilation. In this case, the interval is stretched to the whole
interval of allocation variable, if known. If the allocation size is
not known either, maximum allocation size is used. When there is a
definition of an unknown interval, the analysis must assume it could
be definition of any part of the interval. Multiple definitions of
unknown intervals should not kill each other, as they could both be
reaching definitions for all uses reachable in the CFG by a path where
the whole range of the variable is not over-written. This issue is
addressed in the following section.

*** DONE Strong and weak definitions
As the algorithm needs to remember multiple definitions in case the
interval is unknown or a pointer might point to multiple variables, we
have chosen to use weak definitions to achieve that. Marker algorithm
will again need to be modified to consider them.

We extend the Marker algorithm with two new work structures:
=current_weak_def= and =last_weak_def=. The semantics is similar to
=current_def= and =last_def= from the Marker algorithm.

In =writeVariable=, the choice of the structure where to save the
definition gets a little more complex again. Weak updates will be
saved to =last_weak_def= or =current_weak_def= depending on the
context, while strong updates will be saved to =last_def= or
=current_def=. When encountering a strong definition, intervals of
weak definitions need to be modified not to overlap with the strong
definition. This way, the strong definition ``kills'' the weak
definition. We extend the =IntervalMap= data structure to allow this.

In the previous section, we have mentioned that =readVariable= can
stop the search for definitions once it finds a set of definitions
that ``covers'' the interval of use. We may not add the weak
definition in the set of intervals $\mathcal I$. Only strong
definitions will be added to the set of intervals. Had we added the
weak definitions too, it could happen that a definition of unknown
interval of an array is definition of single element of the array and
the weak definition would be reported as the only reaching definition
of a use of different element of the array. This could result in the
strong definition being marked as irrelevant, which should not happen.

*** DONE Sealed blocks
The Marker algorithm is capable of constructing SSA form of programs
while loading the program representation from a file. Because of this,
it maintains a set of blocks called =sealedBlocks=, that holds all
blocks that already have all their predecessors added. In our case, we
already have the whole program loaded, so we can consider all of our
basic blocks to be loaded -- Braun et al refer to this as
/sealed/\nbsp{}\cite{BraunSSA}.

** DONE DG Library
The slicer used in \sbt{} uses the DG library\nbsp{}\cite{ChalupaDG} to
create dependence graph and slice away unnecessary parts of
verified program. New reaching definitions analysis has been
implemented to the DG library, so it can be used with any software
that uses DG.

Before processing any program, DG loads the program into its own
framework. Analyses that are part of DG are independent of the program
representation, because they only use DG framework which handles the
details. DG currently supports only LLVM intermediate representation.

*** DONE LLVM

LLVM\nbsp{}\cite{LLVM} is an infrastructure for compilers and optimizers. It
consists of multiple libraries and tools. One of the tools is clang --
a compiler of C language.

LLVM defines its own intermediate representation(LLVM IR) of a program. The
representation looks very similar to assembler. This representation is
processed by a subgraph builder. The subgraph builder adds information
into every node of the program's CFG about which variables are defined
and used in the node.

\label{partialSSA} Any program in LLVM IR is guaranteed to be in
/partial SSA form/. Partial SSA form means, that there is at most one
definition for each register. This form of program, however, makes no
guarantees about variables in memory. Those are *not* in SSA
form. Thanks to the partial SSA transformation, LLVM already provides
reaching definitions information for its register variables.

*** DONE Pointer analysis in DG
The new reaching definitions analysis requires information from a
pointer analysis. DG already contains a pointer analysis, which can be
utilized. However, there are two important implementation details that
need to be adressed by any RDA that uses results of this pointer
analysis.

In some cases, the pointer analysis is unable to determine which
variables to pointer points to. It happens for example in case the
pointer is returned from a function from an external library that is
not part of the program. The pointer analysis returns that the pointer
points to a virtual node called ``unknown memory''. This has to be
addressed later in the reaching definitions analysis.


The pointer analysis in DG is field-sensitive, which opens a
possibility to implement a field-sensitive RDA as well. There are
multiple approaches to addressing field-sensitivity. One of them
involves considering each element of an aggregated data structure as a
separate variable. The pointer analysis in DG uses another approach:
it reports which memory object is being accessed and what part of the
object is being accessed. The part of the object is specified by a
pair $(offset, length)$, where both $offset$ and $length$ are in
bytes. In some cases, the $offset$ can be unknown.
# TODO implies?

*** DONE Reaching definitions analysis framework in the DG library
DG uses reaching definitions analysis to calculate data dependencies
between instructions. The original reaching definitions analysis in DG
uses the dense approach, as described in section \ref{denseRDA}.

Prior to the reaching definitions analysis itself, DG builds a
subgraph of program's control flow graph\index{CFG}. The subgraph does
not contain all types of instructions. Rather, it consists only of
store instructions, call instructions, return instructions and all
memory allocations. In spite of not containing all instructions, it
reflects structure of the program. Each instruction in the subgraph
that defines some memory object already has an associated points-to
information from pointer analysis. Thanks to this, it is possible to
tell which variables are strongly or weakly defined in a particular
CFG node.

** TODO New reaching definitions analysis implementation
This chapter describes how the new reaching definitions
analysis has been implemented in the existing framework.

Thanks to LLVM's transformation to partial SSA form (as described in
\ref{partialSSA}), there is no need to compute reaching definitions of
LLVM register variables. Reaching definitions for register variables
have already been computed while translating the C program into LLVM
Intermediate Representation (LLVM IR). Therefore, the implementation
focuses on address-taken variables.

*** DONE Subgraph builder abstractions
Each reaching definitions analysis in the DG library could require
different set of information in the reaching definitions subgraph. The
new analysis requires information about uses in the graph, which are
not added by the current subgraph builder. With that in mind, we have
decided to allow each RDA to use different subgraph builder. A
subgraph builder builds a reaching definitions subgraph from some
representation.

The goal is to allow the user of =ReachingDefinitions= class to run
any reaching definitions analysis they choose. The pointer analysis
framework in the DG library already allows the user to specify pointer
analysis to run using templates. We will do something similar to the
reaching definitions analysis.

We have designed and implemented an interface for subgraph builders
from the LLVM IR called =LLVMRDBuilder=. This interface allows us to
implement a =build= function, that returns the entry node of the
reaching definitions subgraph. The implementation of the new subgraph
builder is very similar to the original implementation, with two major
differences. The new subgraph builder splits up LLVM basic blocks when
a function call is encountered and it also adds information about
which memory is used in which CFG node. These additions are discussed
in the following two sections.

*** DONE Adding use information to control flow graph
Now, the subgraph builder can add information about uses of variables
to the reaching definitions subgraph. Pointer analysis is utilized
here to find out which variables are being used. As one pointer could
simply point to multiple variables, it is necessary to add information
about all variables that could potentially be used.

In the new subgraph builder used with the new analysis, we have
included LLVM's instructions that use memory pointed to by a
pointer. For each node that is a use of some memory, it queries the
underlying pointer analysis for all variables the pointer operand
could point to. For looking up the variables, it uses a
newly-introduced method =getPointsTo=, which fetches the information
from the pointer analysis.

The instruction that is a use could possibly use a smaller portion of
the memory than the allocation size. This is the case when accessing
an individual element of a larger data structure. A field-sensitive
reaching definitions analysis requires the length to be set to the
length that is being used. This is done by determining size of the
type the value is being loaded to.

*** TODO Splitting basic blocks on function calls
The original RDA does not need information about basic blocks in the
program. This is required by the new analysis, so the new
implementation of subgraph builder has to add the information into the
subgraph.

The basic block used by LLVM IR is more or less suitable for the new
analysis, with a major problem: When a function is called, the call
instruction does not end a LLVM IR basic block. This is against the
definition of a basic block introduced in\nbsp{}\ref{ch:KTPA}, as a call
instruction is a jump to a different address.

#+BEGIN_LaTeX
  \begin{figure}[H]
    \begin{lstlisting}[language=LLVM]
      %1 = alloca i32 align 4
      store i32 1, i32* %1
      call void foo(i32* %1)
      store i32 2, i32* %1
    \end{lstlisting}
    \caption{Demonstration of an LLVM basic block}
    \label{fig:llvmBlocks}
  \end{figure}
#+END_LaTeX

Consider the program in figure\nbsp{}\ref{fig:llvmBlocks}. The block calling the function would be
processed first and =foo= would then see the =store i32 2, %1= instruction
as a reaching definition of =%1=. This is, however, not correct as the
instruction has not been executed yet. Because of that, we have
decided to split up an LLVM IR basic block with every call statement,
too.

# TODO figure needs rework, but it has a point
#+BEGIN_LaTeX
    \begin{figure}
      \begin{minipage}[]{0.5\textwidth}
        \begin{lstlisting}[language=LLVM]
          /\hline/
          % 1 = alloca i32 align 4
          store i32 1, i32* %1
          call void foo(i32* %1)
          store i32 2, i32* %1
          /\hline/
        \end{lstlisting}
      \end{minipage}
      \begin{minipage}[]{0.5\textwidth}
        \begin{lstlisting}[language=LLVM]
          /\hline/
          %1 = alloca i32 align 4
          store i32 1, i32* %1 /\encircle{1}/
          /\hline/
          call void foo(i32* %1)
          /\hline/
          store i32 2, i32* %1/\encircle{2}/
          /\hline/
        \end{lstlisting}
      \end{minipage}
      \label{fig:basicBlocks}
      \caption{Demonstration of program division into basic blocks in LLVM(left) and the new implementation(right)}
    \end{figure}
#+END_LaTeX

Figure\nbsp{}\ref{fig:basicBlocks} shows LLVM's way of splitting basic
blocks in the program from figure\nbsp{}\ref{fig:llvmBlocks} along with the
newly-implemented way. The new implementation of subgraph builder
splits up blocks when there is a function call. Block \encircle{1}
gets one predecessor, which is the first basic block of the function
=foo=. Basic block \encircle{2} is then added as a successor of the
last basic block of the function =foo=.

Basic block splitting is only necessary if the function's definition
is part of the program. In case the function is external, there is no
need to split up the basic block and the implementation does not split
it up.

*** DONE Treating unknown memory
Sometimes, pointer analysis is unable to tell where a pointer may
point, so the analysis has to make some conservative assumptions about
the program in order to be correct. In this case, the analysis assumes
that such pointer could point to any variable and treats the CFG node
as if it was a definition or a use of all variables in the
program. Whether it is a definition or a use is decided based on
semantics of the instructions and how the pointer is used.

After the subgraph is built, it is searched by a separate class
=AssignmentFinder=, which does exactly what was explained above. It
uses a two-phase algorithm to do that: In the first phase, all
variables in the program are added to a list. In the second phase,
every store to an unknown pointer and load from an unknown pointer is
turned into a weak definition of all variables in the program or use of
all variables in the program, respectively. Doing this removes some
complex handling of unknown pointers from the next phase of the
analysis.

*** TODO Using intervals to handle field-sensitivity
# TODO polish and connect to rest of the text
\label{chap:intervals} The Marker algorithm itself does not consider
aggregate data structures. In order to support analyzing them, it
needs to be modified. Each definition or use of a variable have an
associated interval of affected bytes. This interval is later used to
look up reaching definitions of a variable. An interval has a start
and a length.

The first intermediate data structure that is part of the new
framework is =DisjointIntervalSet=. The set allows to insert intervals
while maintaining an invariant, that all intervals inside are
disjoint. When inserting an interval that has a non-empty intersection
with some of the intervals inside, the set ensures that these two
intervals are united into a single interval.

=IntervalMap= is the second important data structure of the
framework. It provides functionality that makes the analysis
field-sensitive. =IntervalMap= on the first sight looks similarly to
=std::map= available in C++. It allows to save arbitrary types under
=Interval= keys. The difference is in the lookup
functions. =IntervalMap= offers 3 main functions: =collect=,
=collectAll= and =killOverlapping=.

The =collect= function is designed to work with strong updates. It
searches the entries backwards, starting by the last entry added. 
It collects all values from the interval map such, that the specified
interval is covered by union of key intervals of the values returned.

=collectAll= works with weak updates. As opposed to =collect=, it
does not stop when the specified interval is subset of union of the
result key intervals. Rather, it searches the whole IntervalMap and
returns all values which are saved under intervals that overlap with
the specified interval.

=killOverlapping= deletes definitions with intervals that overlap with
specified interval. After =killOverlapping=, calling =collectAll= with the
same interval or any of its subsets returns an empty result.

=IntervalMap= is used as a data structure for structures that are
needed by Braun et al. algorithm -- that is =current_def=,
=current_weak_def=, =last_def= and =last_weak_def=. This way, the
field-sensitivity is considered in the phase of building the sparse RD
graph.

*** DONE Treating unknown offset
When the pointer analysis returns an unknown offset of a definition or
a use of a variable, the RDA needs to address it. In case there is a
definition of an unknown offset of a variable, it could be definition
of any of its bytes, so the new analysis performs an
over-approximation. In the over-approximation, the analysis assumes
that the whole variable is defined. However, this definition may not
be considered as a strong definition.

#+BEGIN_LaTeX
    \begin{figure}
      \begin{lstlisting}[language=C]
        int $i$, $j$;
        int $a$[10];
        $a$[$i$] = 0;
        $a$[$j$] = 1;
        printf("%d", $a$[0]); /\encircle{1}/
      \end{lstlisting}
      \caption{Using weak definitions to handle unknown offset}
      \label{fig:unknownOffset}
      \end{figure}
#+END_LaTeX

Consider the program in figure\nbsp{}\ref{fig:unknownOffset}. Assuming the
values of $i$ and $j$ are unknown, both of those definitions could be
reaching definitions of $a[0]$ at \encircle{1}. Thus, the analysis has
to assume they are weak definitions despite the fact that $a$
points to a single memory object -- the array.

* TODO Experimental evaluation of the new analysis
\label{ch:Experiment} In this chapter, the new implementation is
evaluated experimentally. For the evaluation, we have used subset of
benchmarks from the software verification competition
SV-COMP[fn::https://sv-comp.sosy-lab.org]. Each benchmark is given as
a C program with a list of properties it satisfies. \sbt{} is run with
the whole set of benchmarks. After running single benchmark, output of
\sbt{} is inspected and compared with the expected output. We also
measure the CPU time it took \sbt{} to compute the result.

** TODO Time
The goal of this thesis is to design and implement a *faster* reaching
definitions analysis. This subchapter compares the new and the
original implementation in terms of speed. 

In order to evaluate time, we have run the same set of benchmarks with
the same version of \sbt{} twice. The only difference between the two
runs was the reaching definitions analysis used. The new
implementation did not provide significant improvements in terms of
the overall run time. The problem is, that the RDA itself is too
insignificant to provide improvements in terms of run time.

Let us evaluate how much time was saved in the phase of RDA. How much
time can really be saved. From the 50 hours of running our 

** TODO Memory Used
During the experiments, there have been cases where the new
implementation ran out of memory faster than the original one. There
are two possible causes for this:
1. Unknown memory treatment consumes too much memory.
2. Intervals framework used for field-sensitivity consumes too much
   memory.

** TODO Accuracy
There should be no difference between the new and the original
analysis in terms of accuracy. However, thanks to the ``interval
framework'' introduced in\nbsp{}\ref{chap:intervals}, the new implementation
of semi-sparse analysis is more accurate than the original
implementation. Consider this program:

  #+BEGIN_SRC c
    int a[] = {0, 1, 2, 3}; // A
    a[0] = 5; // B
    a[1] = 6; // C
    a[2] = 7; // D
    a[3] = 8; // E

    for (size_t i = 0; i < 4; ++i) {
        printf("%d\n", a[i]); // RD(a) = ???
    }
  #+END_SRC
  - the original analysis reports $RD(a) = \{ A, B, C, D, E\}$
  - the new analysis is able to tell that $\{B,C,D,E\}$ together
    overwrite the whole range of =a= and therefore reports $RD(a) =
    \{B,C,D,E\}$

* TODO Conclusion
\label{ch:Summary} This chapter summarizes the work done as part of
this thesis and presents opportunities for possible future work.

** DONE Summary of work done
As a part of this thesis, we studied four algorithms for computing
reaching definitions. Then, we chose to implement an algorithm based
on the Braun et al. algorithm into \sbt{}. Prior to implementation, we
have designed modifications for the algorithm to compute reaching
definitions, work with aggregate data structures and weak updates. The
modified algorithm has been implemented into \sbt{}. The new
implementation is then compared with the original implementation in
terms of accuracy, time and memory used.

** TODO Possible future work

We believe the new implementation could be optimized even further

It is possible to further speed up computation of Reaching Definitions
by incorporating the trivial phi node removal
algorithm\nbsp{}\cite{BraunSSA}. The sparse graph contains many redundant \phi
functions that could be removed to speed up the final phase of
reaching definitions propagation.

As the algorithm is implemented in a slicer, it could be optimized
even further by starting at the slicing criterion and searching the
CFG backwards only for definitions of variables that affect the
slicing criterion, which is what the slicer needs to derive the
control dependencies.

Performance of the =IntervalMap= data structure could be definitely
improved.

The reaching definitions analysis could benefit from additional
accuracy it could gain by considering different instances of
statements.

Newer versions of LLVM support a pass called
mem2reg[fn::https://llvm.org/docs/Passes.html#mem2reg-promote-memory-to-register]. This
pass is able to convert local variables into registers, which are in
SSA form. It would be interesting to use mem2reg pass whenever
possible and then run this analysis to obtain results for arrays and
other structures mem2reg is unable to handle.

Another interesting LLVM pass to test would be scalar replacement of
aggregates[fn::https://llvm.org/docs/Passes.html#sroa-scalar-replacement-of-aggregates]. This
pass replaces arrays and structures by scalar values in case it is
possible.

\printbibliography[heading=bibintoc]
