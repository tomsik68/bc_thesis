#+TITLE: Improvements of reaching definitions analysis in Symbiotic
#+AUTHOR: Tomáš Jašek
#+LATEX_CLASS:         fithesis
#+OPTIONS:             todo:nil toc:nil
#+LATEX_CLASS_OPTIONS: [nolot,nolof,color,oneside]
#+LATEX_HEADER:        \input{setup.tex}
* Introduction

Nowadays, computer programs are used to control gadgets everywhere
around us. Programs are used in wide variety of fields that impose
diverse requirements on their reliability. For example, in medical
applications, it is crucial that a program, which controls a piece of
medical equipment does not misbehave or crash under any circumstances.

On the other hand, programmers make mistakes very often. For example,
a programmer could easily forget to close a curly brace or add a
semicolon at the end of a line. Mistakes similar to those result into
syntax errors. Compiler is able to detect them while processing the
program and sometimes offer a suitable replacement, too.

Compilers, however, do not understand intents of a human
programmer. Therefore, it is still possible for the programmer to
express their intents in an incorrect way producing a logical
error. Compiler is also unable to detect mistakes that make the
program access memory which is owned by, for example, some other
program. On modern operating systems, these mistakes usually cause the
program to crash.

Despite the availability of safe modern low-level programming
languages, such as Rust, the C programming language is still very
widespread. It is being used for critical purposes such as
operating systems, embedded systems and device drivers. With its
flexibility, it has some shortcomings that cause programmers to make
mistakes often. One of the shortcomings is, that memory management is
left to the programmer. A programmer could easily forget to free
allocated memory, unintentionally free the same memory twice or forget
to check the result of a memory allocation function.

Several formal verification tools are able to detect such errors. One
of them is \sbt{}. \sbt{} preprocesses the program in
a specific way and then passes it to symbolic execution framework klee
which performs the verification itself. As formal verification is an
expensive process, it is suitable to remove parts of the program that
do not influence result of the verification process. \sbt{} uses
program slicing based on the DG library\nbsp{}\cite{ChalupaDG} for this.

One of the analyses used during slicing is reaching definitions
analysis. It finds all definitions of a variable that modify it before
the place where it is being used. Reaching definitions in programs
without pointers is trivial and is done automatically by
LLVM\nbsp{}\cite{LLVM} -- the compiler infrastructure used by
\sbt{}. However, reaching definitions for address-taken variables is
more complex, as the analysis needs to deal with dynamic data
structures, pointers that might point to multiple variables and
arrays.

The aim of this thesis is to study modern techniques of computing
reaching definitions, implement a faster reaching definitions analysis
to\nbsp{}\sbt{} and compare the new implementation with the original one
experimentally using a non-trivial set of benchmarks.

This thesis is divided into individual chapters as follows: Chapter
\ref{ch:ProgAnalysis} introduces the reader to program analysis in
general. Key terms, such as program, control flow graph or static
single assignment form are defined there.

Chapter\nbsp{}\ref{ch:RDA} starts by defining what a reaching definition is,
what is a reaching definitions analysis and discusses the simplest
naive algorithm to compute reaching definitions in programs without
pointers. Analyzing programs with pointers is discussed next. Then,
the chapter continues by discussing various algorithms to compute
reaching definitions.

In chapter\nbsp{}\ref{ch:Symbiotic}, \sbt{} is introduced and its
architecture is explained.

The next chapter\nbsp{}\ref{ch:Implementation} discusses LLVM, the DG
library and the chosen implementation approach based on Marker
algorithm\nbsp{}\cite{BraunSSA} is discussed.

Chapter\nbsp{}\ref{ch:Experiment} shows experimental comparison of the
original implementation of reaching definitions analysis and the new
implementation. Implementations are compared in terms of time and
accuracy. SV-COMP benchmarks set is used for the comparison.

Chapter\nbsp{}\ref{ch:Summary} offers a summary of work done.

* Introduction to program analysis
\label{ch:ProgAnalysis}

Tools that find errors in programs use results of program analysis to
decide whether or not there is an error in the program. There are two
sub-techniques of program analysis:
- dynamic
- static

An error-finding tool may use either approach or some combination
of both. The following sub-chapter briefly introduces both techniques.

** DONE Static and dynamic program analysis

*Dynamic analysis* is capable of detecting an erroneous state in the
program at runtime. For example, one could replace C =malloc= and
=free= functions. Custom =malloc= function would add the allocated
memory to a global list. Custom =free= function would remove the
allocated memory from the list. After the program finishes, the list
would be checked for emptiness. If the list is empty, every memory
that was result of allocation was free'd. If the dynamic analysis
finds an error, there is definitely an error in the program. The main
disadvantage is, that if dynamic analysis did not uncover any error,
an error could still occur if it was started with some different input.

*Static analysis* derives program properties from some program's
representation without directly executing it. It could be the source
code, compiled machine code or some kind of intermediate
representation (such as Java bytecode or LLVM bitcode). Static
analysis is able to uncover errors without knowing the input of the
program. If an error is discovered, static analysis can synthesize an
instance of input data for which the program enters the erroneous
state. The main advantage is, that this technique can discover errors
that are overlooked both by programmers and testers. Another advantage
is, it does not require instance of input data, as it is not based on
running the program. However, this technique can be very expensive,
especially when the analyzed program is too big or complex.

This thesis focuses on reaching definitions analysis, which is part of
static analysis. The following sub-chapter continues with a more in-depth
introduction to the static analysis.

** TODO Key terms of static program analysis

A /program/ is a sequence of some elementary instructions. This thesis
focuses on analysing programs written in the C programming language.
# TODO say that the term /program/ will refer to C program?
# TODO is it necessary to define blocks? - yes
Program's structure is reflected in its /Control flow
graph/. Formally, /control flow graph/ (CFG\index{CFG}) of a program
$\mathcal P$ is a graph $G = (V, E)$, where each instruction of
$\mathcal P$ is represented by a vertex. If instruction $I_2$ /may/ be
executed immediately after instruction $I_1$ in $\mathcal P$, then
$(I_1, I_2) \in E$. Figure\nbsp{}\ref{fig:programCFG} shows a simple program in
C language and its control flow graph.

#+BEGIN_LaTeX
  \begin{figure}[]
    \begin{minipage}[b]{0.5\textwidth}
      \begin{lstlisting}[language=C]
        int i;
        scanf("%d", &i);
        if (i % 2 == 0)
            puts("even");
        else
            puts("odd");
        puts("exit");
      \end{lstlisting}
    \end{minipage}
    \begin{minipage}[t]{0.5\textwidth}
      \begin{tikzpicture}
      \tikzstyle{arr} = [->,shorten <=1pt,>=stealth',semithick]
        \node[draw, rectangle] (A) at (0, 0) {int i};
        \node[draw, rectangle] (B) at (0, -1.2) {scanf("\%d", \&i)};
        \node[draw, rectangle] (C) at (0, -2.4) {if i \% 2 == 0};
        \node[draw, rectangle] (D) at (-1.5, -3.6) {puts("even")};
        \node[draw, rectangle] (E) at (1.5, -3.6) {puts("odd")};
        \node[draw, rectangle] (F) at (0, -4.8) {puts("exit")};
        \draw[arr] (A) -- (B);
        \draw[arr] (B) -- (C);
        \draw[arr] (C) -- (D);
        \draw[arr] (C) -- (E);
        \draw[arr] (D) -- (F);
        \draw[arr] (E) -- (F);
      \end{tikzpicture}
    \end{minipage}
    \caption{Program in C language and its control flow graph}
    \label{fig:programCFG}
  \end{figure}
#+END_LaTeX

A /variable/ is a fixed-size storage cell for value of some type.  A
/definition/ of a variable is any instruction that can
modify its value. A /use/ of a variable is any instruction
that can read its value. 

** TODO Static single assignment form
Programs may be transformed without changing their behaviour. One of
transformations that do not change program's behaviour is
transformation to Static Single Assignment form (or SSA for
short). The transformation itself yields some useful data about the
program and the SSA form is particularly useful for compilers and code
analyzers.

A program $\mathcal P$ is in /Static Single Assignment form/ if, and
only if for each variable in $\mathcal P$, there is at most one
definition. Figure\nbsp{}\ref{fig:programSSA} shows a simple program and its
SSA form.

#+BEGIN_LaTeX
    \begin{figure}[]
    \begin{minipage}[t]{0.5\textwidth}
      \begin{lstlisting}[language=C]
        int i = 1;
        int j = 1;
        i = i + j;
        j = j + i;
        foo(i);
      \end{lstlisting}
    \end{minipage}
    \begin{minipage}[t]{0.5\textwidth}
      \begin{lstlisting}[language=C]
      int i_1 = 3;
      int j_1 = 4;
      i_2 = i_1 + j_1;
      j_2 = j_1 + i_2;
      foo(i_2);
      \end{lstlisting}
    \end{minipage}
    \caption{Program and its SSA form}
    \label{fig:programSSA}
    \end{figure}
#+END_LaTeX

Constructing the SSA form is a little more complex in case the CFG of
a program contains loops. Consider program in Figure\nbsp{}\ref{fig:loop}.

#+BEGIN_LaTeX
  \begin{figure}[h]
    \begin{minipage}[b]{0.5\textwidth}
      \begin{lstlisting}[language=C]
  int i = 0;
  while (i < 10) {
      printf("%d\n", i);
      i++;
  }
      \end{lstlisting}
    \end{minipage}
    \begin{minipage}[b]{0.5\textwidth}
      \begin{lstlisting}[language=C]
  int i_1 = 0;
  int i_2;
  int i_3;

  while (i_2 = \phi(i_1, i_3), i_2 < 10) {
      printf("%d\n", i_2);
      i_3 = i_2 + 1;
  }
        \end{lstlisting}
      \end{minipage}
    \caption{Simple C program with loops and its SSA form}
    \label{fig:loop}
    \end{figure}
#+END_LaTeX

While constructing SSA form of this program, the use of =i= variable
at location =B= could be replaced by the assignment to =i= at location
=A= or =C=. The problem is, that both of these statements contribute
to the value of =i= at location =B=. It is, therefore, necessary to
use some kind of combination of values from =A= and =C=. This is what
a \phi function is for. $i_3 \gets \phi(i_1, i_2)$ denotes, that the value
of $i_3$ could be either $i_1$ or $i_2$.

* Reaching Definitions Analysis
\label{ch:RDA}
# TODO invent notation for =z= is a reaching definition of =a= at =y=.
Reaching definitions analysis(RDA for short) is a part of static program analysis.

This chapter explains what a reaching definition is, discusses
properties of reaching definitions analyses and introduces various
algorithms to calculate reaching definitions.

\label{def:RD}A /reaching definition/ \index{RD} of variable $\mathcal V$ used in
instruction $I_1$ is an instruction $I_2$ such, that:
+ $I_1, I_2 \in \mathcal P$
+ $I_1$ is a use of variable $\mathcal V$
+ $I_2$ is a definition of variable $\mathcal V$

Figure \ref{fig:programRD} shows program's CFG with reaching definitions.

# TODO figure: program and reaching definitions
#+BEGIN_LaTeX
  \begin{figure}
    \begin{minipage}[b]{0.5\textwidth}
      \begin{lstlisting}[language=C]
        int i;
        scanf("%d", &i);
        if (i % 2 == 0)
            puts("even");
        else
            puts("odd");
        puts("exit");
      \end{lstlisting}
    \end{minipage}
    \begin{minipage}[t]{0.5\textwidth}
      \begin{tikzpicture}
      \tikzstyle{arr} = [->,shorten <=1pt,>=stealth',semithick]
        \node[draw, rectangle] (A) at (0, 0) {int i};
        \node[draw, rectangle] (B) at (0, -1.2) {scanf("\%d", \&i)};
        \node[draw, rectangle] (C) at (0, -2.4) {if i \% 2 == 0};
        \node[draw, rectangle] (D) at (-1.5, -3.6) {puts("even")};
        \node[draw, rectangle] (E) at (1.5, -3.6) {puts("odd")};
        \node[draw, rectangle] (F) at (0, -4.8) {puts("exit")};
        \draw[arr] (A) -- (B);
        \draw[arr] (B) -- (C);
        \draw[arr] (C) -- (D);
        \draw[arr] (C) -- (E);
        \draw[arr] (D) -- (F);
        \draw[arr] (E) -- (F);
      \end{tikzpicture}
    \end{minipage}
    \caption{Program in C language and its reaching definitions. Solid edges are part of CFG, dashed edges are reaching definitions.}
    \label{fig:programRD}
  \end{figure}
#+END_LaTeX

** TODO Strong and weak definitions, kill and gen sets
\label{strongWeakUpdate}
A reaching definitions analysis processes stores and loads in some
way. There are two different views on the same problem.
- pointer points at multiple variables
  - definition of that pointer is weak definition
  - kill set does not contain the pointer, gen does

Each definition of a variable can be either /strong/ or /weak/. Strong
definition over-writes the variable with a new value. When a strong
definition is encountered, it invalidates all previous definitions of
the variable. Weak definition, on the other hand, does not necessarily
over-write the variable, so it does not invalidate previous definitions.

That means, for each use of a variable there might be multiple weak
definitions, but at most one strong definition.

In programs that do not use pointers, all definitions of variables are
strong.

** TODO Properties of reaching definitions analyses
Reaching definitions analyses have some properties\nbsp{}\cite{rptRDA} that
affect their accuracy. Less accurate analyses need to make some
conservative assumptions about the program in order to be
correct. This sub-chapter describes three properties of reaching
definitions analyses: field-sensitivity, instance-wiseness and ability
to recognize execution patterns.

*** DONE Instance-wise and statement-wise analysis
When analyzing programs with a cyclic CFG, there are multiple
/instances/ of instructions that are inside of a loop. Each iteration
of the loop creates a new instance of each instruction in the loop's
body.

Along with the definition, use and variable, an instance-wise reaching
definitions analysis is able to tell which instance of the
instructions are involved. The information about instance might
involve for example the for loop indexing variable =i=. There might be
more variables in case the instruction is inside of a nested loop.

Differences between instance-wise analysis and statement-wise analysis
will be demonstrated on a simple program in figure \ref{fig:instWise}.

#+BEGIN_SRC c
  int a = 0; // x

  for(int i = 0; i < 5; ++i) {
      int b = a + i; // y
      a = b; // z
  }
#+END_SRC
\label{fig:instWise}

Reaching definitions for =a= at location =y= should be =x= and
=z=. However, there are multiple instances of instructions =y= and
=z=. Firstly, both instance-wise and statement-wise analyses would
report, that =x= is a reaching definition of =a= at =y=. The
difference is, how much information the analysis is able to provide
about the reaching definition =z= at =y=. Statement-wise analysis
would simply state, that =z= is a reaching definition of =a= at
=y=. Instance-wise analysis goes a little further by reporting, that
$z^{i+1}$ is a reaching definition of =a= at $y^i$. The upper index
denotes the index of iteration.

*** DONE Field sensitivity
Usage of aggregated data structures, such as arrays or C language
=struct=-s introduces another issue that needs to be addressed by a
reaching definitions analysis. Precision of analysis for programs that
use aggregated data structures depends on whether the analysis can
distinguish between individual elements of the data structure.

Consider the program in Figure\nbsp{}\ref{fig:rdaFS}. Locations =x= and =y= in the
program define the first and the second element of the array =a=. After
that, location =z= contains a function call that uses the third
element of the array. This element has no definitions in the program,
so an accurate reaching definitions should find no definitions for it.

A field-sensitive analysis considers indices and correctly reports no
reaching definitions for =a[2]= at location =z=.

A field-insensitive analysis ignores indices of the array and for
location =z=, it would report, that reaching definitions of =a[2]= are
=x= and =y=. This is an over-approximation that has to be performed by
the field-insensitive analysis.

#+BEGIN_LaTeX
  \begin{figure}[H]
    \begin{lstlisting}[language=C]
      int a[5];
      a[0] = 1; // x
      a[1] = 2; // y
      foo(a[2]); // z
    \end{lstlisting}
    \caption{Demonstration of field-sensitive reaching definitions analysis}
    \label{fig:rdaFS}
    \end{figure}
#+END_LaTeX

**** TODO Multiple definitions of a variable
#+BEGIN_SRC c
  int a[5];
  a[0] = 1;  // x
  a[0] = 2;  // y
  foo(a[0]); // z
#+END_SRC

FI: weak updates(it does not know about indices)
x -> z
y -> z

FS: strong updates
y -> z

**** TODO Unknown offsets and over-approximation
If there are multiple instances of an instruction (e.g. it is part of
a loop or recursive function), it could happen, that the index is
simply unknown. Consider a simple program:

#+BEGIN_SRC c
  int a[5];

  a[2] = 15;
  for(int i = 0; i < 2; ++i) {
      a[i] = i; // x
  }

  for(int i = 0; i < 3; ++i) {
      printf("%d\n", a[i]); // y
  }
#+END_SRC

Assuming that no for loop is unrolled by the compiler, reaching
definitions analysis is unable to tell which part of =a= is being
defined at location =x=.

- stretch it to whole array
- consider as weak update

*** DONE Execution patterns recognition

Reaching definitions analysis is often not the only analysis that is
part of a program analysis framework. More often than not, there are
more analyses that derive various properties of program or its
parts. Reaching definitions analysis can sometimes take advantage of
results of previously ran analyses and achieve better accuracy or
speed.

Consider the program in figure\nbsp{}\ref{fig:execPatterns}.

#+BEGIN_SRC c
  int foo(int a) {
      int c = 0;
      if (a < 0) {
          c = 1; // x
      }
      if (a >= 0) {
          c = 2; // y
      }
      return c; // z
  }
#+END_SRC
\label{fig:execPatterns}.

If an external analysis reports that there is no program execution
where $a < 0$, the reaching definitions analysis could take this into
account and derive that =x= is not a reaching definition of =c= at =z=
even despite the fact it is a definition of a simple
variable. Analysis that does not take it into account would report
that both =x= and =y= are reaching definitions of =c= at =z=.

In this case, an analysis that does not recognize execution patterns
yields an over-approximation, which is not a problem.

** TODO Dense reaching definitions analysis
\label{denseRDA} One of the ways to calculate reaching definitions is
to ``follow'' the control flow graph of the program while remembering
the last definition for each variable for each of its vertices. This
is a traditional approach used by many tools.

Figure \ref{fig:denseRDA} demonstrates the algorithm.

#+BEGIN_LaTeX
\begin{figure}[H]
  \begin{algorithm}
  \SetAlgoLined
  \KwData{Control Flow Graph as $V_{CFG}$ and $E_{CFG}$}
  \KwResult{Reaching Definitions}
  \While {not fixpoint} {
    \For{$v \in V_{CFG}$ in DFS order} {
      \For{$def(x) \in v.defs$} {
        $v.rd(x) \gets v.rd(x) \cup \{ v \}$ \;
      }
      \For{$u \in v.predecessors$} {
        \For{$def(x) \in u.defs$} {
          $v.rd(x) \gets v.rd(x) \cup \{ u \}$ \;
        }
      }
    }
  }
  \end{algorithm}
\caption{Dense reaching definitions analysis algorithm}
\label{fig:denseRDA}
\end{figure}
#+END_LaTeX

*** TODO Performance penalty of the dense algorithm
\label{densePP} While the dense algorithm is correct, it performs
excessive amount of work by copying information about reaching
definitions to nodes of CFG where it is not necessary at all. Even in
case the target CFG node $n$ uses some memory, it will get reaching
definitions for all variables defined somewhere on a path from entry
node to $n$. The dense algorithm certainly leaves some room for
optimization.

** TODO Analyzing programs that use pointers
One of the most important features of programming languages are
pointers. They can be utilized to implement dynamic data structures,
which are very widely used. However, pointers also add more ways the
program can fail. For example, dereferencing a pointer with invalid
value may cause the program to crash.

As pointers make it possible to create variables that 
# TODO change hold variables to something better
``hold variables'', they inherently make programs more difficult to
understand and analyze.

In order to compute reaching definitions for address-taken variables,
reaching definitions analysis uses points-to information from pointer
analysis.

*** TODO Pointer analysis
Pointer analysis\nbsp{}\cite{ChalupaPTA} is, similarly to reaching
definitions analysis, a part of static program analysis. It creates a
set $\mathcal V$ of variables for each pointer $p$. If $p$ may point
to some variable $v$, then $v \in \mathcal V$.

Reaching definitions analysis uses these data from pointer analysis to
recognize uses and definitions of variables. Accuracy of the reaching
definitions analysis, therefore, depends on accuracy of the underlying
pointer analysis. Namely, if the pointer analysis performs an
over-approximation, so will the reaching definitions analysis.

*** TODO Strong & weak definitions in programs with pointers
When processing an indirect(pointer-based) definition of a variable,
it is important to distinguish whether the pointer \textit{must} or
\textit{may} point to given variable. If the pointer \textit{must}
point to the variable, it is considered a strong update. Otherwise, it
is considered a weak update, as defined in\nbsp{}\ref{strongWeakUpdate}.

*** TODO Unknown memory
Pointer analysis may return an empty points-to set or it might be
unable to determine which variable the pointer points to. One of the
cases where it could happen is, that the pointer is returned from an
external function that is part of some library.

If the information is not known, the pointer analysis needs to perform an
over-approximation. That means, it has to assume that the pointer
could point to any variable in the program.

*** TODO Field-sensitivity
While analyzing programs with arrays or aggregate data structures,
distinguishing between definitions of individual elements of the data
structure makes the analysis more accurate.

Field-sensitivity of an RDA always depends on the field-sensitivity of
the underlying pointer analysis. If the pointer analysis is
field-insensitive, the RDA that uses its results has to be
field-insensitive, too.

*** TODO Unknown offset
Pointer arithmetic is commonly involved when using aggregated data
structures or arrays in the program. Arrays are commonly used with for
loops to iterated through all elements of the array. The indexing
variable of the for loop is then involved in pointer
arithmetic. Pointer analysis is unable to determine the offset,
because it changes with every iteration of the for loop.[fn::Assuming
for loop unrolling is not involved with the particular for loop]

** TODO Demand-driven reaching definitions analysis
In an attempt to avoid the performance penalty of the dense reaching
definitions analysis (discussed in \ref{denseRDA}), several other
algorithms have been introduced. This subchapter briefly introduces
demand-driven reaching definitions analysis\nbsp{}\cite{SootDDRDA}.

The main idea of this approach is to answer the question ``can a
definition $d$ of variable $v$ reach a program point $p$?''. This
question is referred to as /query/ and it is represented by a triple
$(d, p, v)$. After a query is generated, it is propagated backwards
along nodes of the CFG. Each node may either answer the query or
continue the propagation to its predecessors. If a node $x$ contains a
definition of $v$, the query propagation stops. The answer is yes, if
and only if $x = d$. If $x \ne d$, then node $x$ kills the
definition $d$ before it can reach $p$ along the path.

In case a program point $p$ has $n$ predecessors, it is sufficient
that the reachability of $d$ is reported by at least one of them.

With an inter-procedural CFG, this approach can be used with function
summaries\nbsp\cite{ipFS1}\nbsp{}\cite{ipFS2}. The function summaries enable
them to process functions once per variable in the program.

# TODO explain slicing criterion somewhere?

It is worth noting, that this approach has a special property that
makes it suitable for a slicer: It is able to start from the slicing
criterion and gradually find all definitions that affect the
criterion. This way, it can avoid computing irrelevant information.

# TODO nice equation with big \vee

** TODO Sparse dataflow analysis
\cite{MadsenSDAPR}
- SSA based
- frontier edge
- uses dominator tree
- fixpoint computation
** TODO Algorithms based on static single assignment form
\label{SSArd} Algorithms that transform a program into SSA form
replace modified variables in assignments by new, artificially-created
variables. They also replace variables in uses by the most recent
definition -- reaching definition. This property of SSA form can be
utilized while calculating reaching definitions.

# TODO maybe, define a simple framework for these algorithms
# so they can be plugged in to the final reaching definitions stage

# TODO program, SSA form, reaching definitions

For the purpose of this thesis, I have studied two algorithms for
computing SSA form. One of them has been introduced by
Cytron et al\nbsp{}\cite{CytronSSA} and relies on dominance frontiers.  The
second algorithm, invented by Braun et al\nbsp{}\cite{BraunSSA}, is simpler
and has been experimentally proven to be more efficient\nbsp{}\cite{BraunSSA}.
*** TODO Cytron algorithm

Algorithm introduced by Cytron et al.\nbsp{}\cite{CytronSSA} uses dominance
information to pre-calculate locations of \phi functions. In the later
phase, variables are numbered using a simple stack of counters and \phi
functions are filled with operands.

*** TODO Marker algorithm
\label{marker}

The marker algorithm\nbsp{}\cite{BraunSSA} has two phases: local value
numbering and global value numbering.

During *local value numbering*, it computes SSA form of every basic
block of the program. For every basic block, it iterates through all
instructions in execution order. If an instruction $I$ defines some
variable $\mathcal V$, $I$ is remembered as the current definition of
$\mathcal V$. If an instruction $I$ uses some variable $\mathcal V$,
the algorithm looks up its definition. If there is a current
definition $\mathcal D$, the use of variable $\mathcal V$ is replaced
by use of the numbered variable that corresponds to $\mathcal D$.

*Global value numbering* is involved once no definition for the
specified variable can be found in the current basic block. The
algorithm places a \phi function on top of the current block and starts
recursively searching the CFG for the latest definition in all
predecessors of the current block. Once a definition is found, it is
added as an operand to the \phi function.

**** TODO Recursive function
When looking up a definition of a variable from a predecessor block,
the block does not have to be processed yet. If that is the case, the
algorithm does not have any idea about which variables are defined in
that block. This happens when the program's CFG is cyclic -
e.g. recursive function is called or for loop is used.

Because of that, the algorithm remembers the last definition of
variable in a block during local value numbering. If there is no last
definition in a block, the lookup can continue to predecessors
recursively.

** TODO Chosen approach to reaching definitions analysis
In an attempt to avoid the performance penalty of the dense algorithm
as described in \ref{densePP}, I have decided to implement a reaching
definitions algorithm based on transformation to SSA form.
# TODO why?? need to document other approaches aswell

Thanks to the lazy nature of the Marker algorithm, I have concluded
that it is the perfect candidate for a new reaching definitions
algorithm. Another big advantage is, that it does not depend on having
pre-calculated other data structures, such as dominator tree,
dominance frontiers or others.

This thesis presents two algorithms for transformation into SSA
form. I have decided to use Marker algorithm. Marker algorithm is
simpler, easily extensible and there are multiple stages of
implementation which leaves room for further optimizations. It has
also been experimentally proven\nbsp{}\cite{BraunSSA} to be faster than
Cytron et al algorithm.

Rather than propagating all information to every single node of the CFG,
the amount of information propagated can be reduced by propagating
only variables that are dereferenced only to nodes that correspond to
instructions that use the variable.

*** TODO Sparse RD Graph
*** TODO Incorporating pointers
*** TODO Weak Updates
*** TODO Field sensitivity
Braun et al.\nbsp{}\cite{BraunSSA}, however, do not elaborate on field
sensitivity. In order to use it, it is necessary to modify it.
*** TODO Optimizations
**** TODO Start lookup only in load instructions
**** TODO Lookup only used variables
**** TODO Strong updates stop the lookup
* TODO Symbiotic
\label{ch:Symbiotic}
\sbt{} is a modular tool for formal verification of programs based
on the LLVM compiler infrastructure. It is being developed at
Faculty of Informatics, Masaryk University.

** TODO How Symbiotic works
\sbt{} works by combining three well-known techniques:
1. Instrumentation
2. Slicing
3. Symbolic Execution

Instrumentation is responsible for inserting memory access checks into
the program. It overrides memory allocation functions by its own, that,
besides performing the allocation itself, add the allocated memory
along with allocation size into a global data structure. When
dereferencing a pointer, instrumentation inserts a check to verify
whether this pointer is inside allocated bounds or not. There is an
assertion that crashes the program if a dereference is out of bounds
of allocated memory.

Slicing is a technique that reduces size of the program by removing
parts that do not influence its correctness with respect to given
criterion. 

Symbolic execution is the last step. It is a technique that decides
whether the program could violate a condition of some assertion in the
program.

* TODO Implementation
\label{ch:Implementation} This chapter starts by introducing the LLVM
infrastructure and the DG library. The introduction is followed by an
in-depth discussion of the new reaching definitions analysis
implementation.

** TODO LLVM
#+BEGIN_QUOTE
The LLVM Project is a collection of modular and reusable compiler and toolchain technologies. \\
-- https://llvm.org/
#+END_QUOTE

One of tools from the LLVM family is clang - compiler of C language to
the LLVM intermediate representation (IR). LLVM IR is guaranteed to be
in partial SSA form.

*** TODO LLVM Intermediate Representation
- define important instructions
*** TODO Partial static single assignment form
\label{partialSSA}
Partial SSA form means, that there is at most one definition for each
register. This form of program, however, makes no guarantees about
address-taken variables, which are *not* in SSA form.
# TODO some figure with partial SSA form

Thanks to the partial SSA transformation, LLVM already provides
 reaching definitions information for its register variables.

** TODO DG Library
The slicer used in \sbt{} uses the DG library to calculate dependence
graph and slice away unnecessary parts of verified program. New
reaching definitions analysis has been implemented to the DG library,
so it can be used with any software that uses DG.

DG itself provides multiple analyses that are independent of the
assembly code used. It contains instantiation of those analyses for
LLVM.

DG is able to calculate control dependencies using information about
reaching definitions. The old reaching definitions analysis in DG uses
the dense approach, as described in \ref{denseRDA}.

*** TODO Reaching definitions analysis framework in the DG library
Prior to the reaching definitions analysis itself, DG builds a
subgraph of program's control flow graph\index{CFG}. The subgraph does
not contain all types of instructions. Rather, it consists only of
store instructions, call instructions, return instructions and all
memory allocations. In spite of not containing all instructions, it
reflects structure of the program.

Each instruction in the subgraph that defines some memory object has
an associated points-to information from pointer analysis. Thanks to
this, it is possible to tell which variables are strongly or weakly
defined by a store function.

** TODO Reaching definitions analysis implementation approach
The new reaching definitions analysis is implemented in the DG
library. This chapter describes how the new reaching definitions
analysis has been implemented in the existing framework.

Thanks to LLVM's transformation to partial SSA form (as described in
\ref{partialSSA}), there is no need to take care of LLVM register
variables, as they are already taken care of while translating the C
program into LLVM Intermediate Representation. Therefore, the
implementation focuses on address-taken variables.

*** DONE Subgraph builder abstractions
As there are some modifications done to the subgraph builder, the
first step towards the implementation is to introduce an abstraction
for reaching definitions subgraph builder. The abstraction allows the
legacy subgraph builder to be preserved, while a new one can be added,
too.

The goal was to allow the user of =ReachingDefinitions= class to run
any reaching definitions analysis they would choose. The pointer
analysis framework in the DG library already allows the user to
specify pointer analysis to run using templates. Similar approach was
taken here with the reaching definitions analysis.

Each reaching definitions analysis in the DG library could require
different set of information about in the reaching definitions
subgraph. With that in mind, I have decided to allow each analysis to
use different subgraph builder. A subgraph builder builds a reaching
definitions subgraph from some representation. In this case, the
representation is always LLVM. Therefore, I have designed and
implemented an interface for subgraph builder called
=LLVMRDBuilder=. This interface allows to implement a =build=
function, that returns the root node of the reaching definitions
subgraph.

In order to enable the selection of reaching definitions subgraph
builder at the compile-time, I have introduced a helper class
=BuilderSelector= in the =detail= namespace. It is a simple templated
class with instantiation for each reaching definitions analysis in the
DG library. The default instantiation uses the original subgraph
builder, renamed to =LLVMRDBuilderDense=. Instantiation for the new
analysis uses different subgraph builder: =LLVMRDBuilderSemisparse=.


#+BEGIN_SRC cpp
  namespace detail {
      template <typename Rda>
      struct BuilderSelector {
          using BuilderT = LLVMRDBuilderDense;
      };

      template <>
      struct BuilderSelector<SemisparseRda> {
          using BuilderT = LLVMRDBuilderSemisparse;
      };
  }
#+END_SRC

Then, in the =ReachingDefinitions= class, that is responsible for
running reaching definitions analysis, I have enhanced the =run=
method with a template that allows the user to specify the type of the
desired reaching definitions analysis. The beginning of the =run=
method now looks like this:

#+BEGIN_SRC cpp
  template <typename RdaType>
  void run()
  {
      using BuilderT = typename detail::BuilderSelector<RdaType>::BuilderT;
      builder = std::unique_ptr<LLVMRDBuilder>(new BuilderT(m, pta, pure_funs));
      // ...
  }
#+END_SRC

*** DONE Adding use information to control flow graph

Now, the subgraph builder can add information about pointer
dereferences - that is, LLVM =load= instructions to the reaching
definitions subgraph. Pointer analysis is utilized here to find out
which variables are being used. As one pointer could simply point to
multiple variables, it is necessary to add information about all
variables that could potentially be used.

In the subgraph builder used with the new analysis,
=LLVMRDBuilderSemisparse=, I have instructed the subgraph builder to
include LLVM's load nodes. For each load node, it then queries the
pointer analysis for all variables its dereferenced pointer operator
could point to. For looking up the variables, it uses a
newly-introduced method =getPointsTo=, which fetches the information
from the pointer analysis.

The =load= instruction could possibly use a smaller portion of the
memory than the allocation size. This is the case when accessing an
individual element of a larger data structure. A field-sensitive
reaching definitions analysis requires the length to be set to the
length that is being used. This is done by determining size of the
type the value is being loaded to.

*** TODO Splitting basic blocks on function calls
- why splitting blocks?
- split basic block
- inline the function
*** TODO Treating unknown memory
Sometimes, pointer analysis was unable to tell where a pointer may
point, so the analysis has to make some conservative assumptions about
the program in order to be correct. In this case, the analysis assumes
that such pointer could point to any variable and treats the CFG node
as if it was a definition or a use of all variables in the
program. Whether it is a definition or a use is decided based on
semantics of the instructions and how the pointer is used.

After the subgraph is built, it is searched by a separate class
=AssignmentFinder=, which does exactly what was explained above. It
uses a two-phase algorithm to do that: In the first phase, all
variables in the program are added to a list. In the second phase,
every store to an unknown pointer and load from an unknown pointer
turn into weak definition of all variables in the program or use of
all variables in the program, respectively. Doing this removes some
complex handling of unknown pointers from the next phase of the analysis.
*** TODO Field-sensitivity
The Marker algorithm itself does not consider aggregate data
structures. In order to support analyzing them, it needs to be
modified a little.

Each definition or use of a variable have an associated interval of
affected bytes. This interval is later used to look up reaching
definitions of a variable. An interval has a start and a length.

Intervals act as an abstraction on top of definition sites. They make
it possible to calculate an intersection or union.

The first intermediate data structure that is part of this framework
is DisjointIntervalSet. The set allows to insert intervals while
maintaining an invariant, that all intervals inside are disjoint. When
inserting an interval that has a non-empty intersection with some of
the intervals inside, the set ensures that these two intervals are
converted into a single interval, which is a union of the two.

IntervalMap is the second important data structure of the
framework. It provides functionality that makes the analysis
field-sensitive. IntervalMap on the first sight looks similarly to
=std::map= available in C++. It allows to save arbitrary types under
=Interval= keys. The difference is in the lookup
functions. IntervalMap offers two lookup functions: =collect= and
=collectAll=. 

The =collect= function is designed to work with strong updates. It
searches the entries backwards, starting by the last entry added. 
# TODO this is true, but it is not what collect does
It collects all values from the interval map such, that the specified
interval is covered by union of key intervals of the values returned.

The =collectAll= works with weak updates. As opposed to =collect=, it
does not stop when the specified interval is subset of union of the
result key intervals. Rather, it searches the whole IntervalMap and
returns all values which are saved under intervals that overlap with
the specified interval.

*** TODO Weak updates
- clear the data structures when strong update is encountered
  - interval magic

The Marker algorithm maintains two main data structures for processing
the strong updates: =last_def=, =current_def=. To incorporate weak
updates, they have been duplicated with names =last_weak_def= and
=current_weak_def=.

=last_weak_def= is used during local value numbering to remember the
last weak definition in a block. When a strong definition is
encountered, overlapping weak definitions are either killed or have
their definition intervals shrank.

When a strong definition is encountered during global variable
numbering, current weak definitions that overlap with the strong
definition must be killed.

#+BEGIN_SRC cpp
  current_weak_def[var.target][block].killOverlapping(interval);
#+END_SRC

Encountering a weak update involves simply adding it to =current_def=
in global value numbering, or =last_def= in local value numbering.

*** TODO Treating unknown offset
- stretch to max interval
- weak update

*** TODO Constructing a sparse RD graph
- the Marker algorithm usually constructs SSA form
- but not in this case
- Marker algorithm is used only to find def-use chains and place \phi functions
- def-use chains, defs, uses, \phi nodes together form sparse RD graph

*** TODO Computing reaching definitions from a sparse RD graph
- start by lazily searching the SRG backwards from load nodes

* TODO Experimental evaluation of the new analysis
\label{ch:Experiment}
** TODO Time
** TODO Memory Used
** TODO Accuracy
- the new analysis is more accurate
  #+BEGIN_SRC c
    int a[] = {0, 1, 2, 3}; // A
    a[0] = 5; // B
    a[1] = 6; // C
    a[2] = 7; // D
    a[3] = 8; // E

    for (size_t i = 0; i < 4; ++i) {
        printf("%d\n", a[i]); // RD(a) = ???
    }
  #+END_SRC
  - the original analysis reports $RD(a) = \{ A, B, C, D, E\}$
  - the new analysis is able to tell that $\{B,C,D,E\}$ together
    over-write the whole range of =a= and therefore reports $RD(a) =
    \{B,C,D,E\}$
* TODO Conclusion
\label{ch:Summary}

** TODO Future Work

It is possible to further speed up computation of Reaching Definitions
by incorporating the trivial phi node removal
algorithm\nbsp{}\cite{BraunSSA}. The sparse graph contains many redundant \phi
functions that could be removed to speed up the final phase of
reaching definitions propagation.

As the algorithm is implemented in a slicer, could be improved even
further by starting at the slicing criterion and searching the control
flow graph backwards for definitions of variables that affect the
slicing criterion, which is what the slicer needs to derive the
control dependencies.

The =IntervalMap= data structure used in MarkerFS builder could be improved.

The reaching definitions analysis could benefit from additional
accuracy it could gain by considering different instances of
statements.

Newer versions of LLVM support a pass called
mem2reg[fn::https://llvm.org/docs/Passes.html#mem2reg-promote-memory-to-register]. This
pass is able to convert local variables into registers, which are in
SSA form. It would be interesting to use mem2reg pass whenever
possible and then run this analysis to obtain results for arrays and
other structures mem2reg is unable to handle.

Another interesting LLVM pass to test would be scalar replacement of
aggregates[fn::https://llvm.org/docs/Passes.html#sroa-scalar-replacement-of-aggregates]. This
pass replaces arrays and structures by scalar values in case it is
possible.

** TODO Summary of work done
*** TODO study reaching definitions analyses
*** TODO choose an SSA-based algorithm
*** TODO extend the algorithm with field-sensitivity
*** TODO extend the algorithm with weak updates
*** TODO extend the dg subgraph builder framework
*** TODO implement sparse RD graph builder
*** TODO implement the reaching definitions analysis
*** TODO experimental comparison of the new and old analysis

\printbibliography[heading=bibintoc]
