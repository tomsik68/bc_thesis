#+TITLE: Improvements of reaching definitions analysis in Symbiotic
#+AUTHOR: Tomáš Jašek
#+LATEX_CLASS:         fithesis
#+OPTIONS:             todo:nil toc:nil
#+LATEX_CLASS_OPTIONS: [nolot,nolof,color,oneside]
#+LATEX_HEADER:        \input{setup.tex}
* DONE Introduction

Nowadays, computer programs control gadgets everywhere around
us. Programs are used in wide variety of fields that impose diverse
requirements on their reliability. For example, in medical
applications it is crucial that a program which controls a piece of
medical equipment does not misbehave or crash under any circumstances.

On the other hand, programmers make mistakes very often. For example,
a programmer could easily forget to close a brace or add a semicolon
at the end of a line. Mistakes similar to those result into syntax
errors. Compiler is able to detect them while processing the program
and sometimes offer a suitable replacement, too. Compilers, however, do
not understand intents of a human programmer. Therefore, it is still
possible for the programmer to express their intents in an incorrect
way producing a logical error. Compiler is also unable to detect
mistakes that make the program access memory which is owned by, for
example, some other program. On modern operating systems, these
mistakes usually cause the program to crash.

Despite the availability of safe modern low-level programming
languages, such as Rust, the C programming language is still very
widespread. It is being used for critical purposes such as operating
systems, embedded systems and device drivers. With its flexibility, it
has some shortcomings that cause programmers to make mistakes
often. One of the shortcomings is, that memory management is left to
the programmer. A programmer could easily forget to free allocated
memory, unintentionally free the same memory twice or forget to check
the result of a memory allocation function, which may fail.

Several formal verification tools are able to detect such errors. One
of them is \sbt{}. \sbt{} preprocesses the program in
a specific way and then passes it to symbolic execution framework klee
which performs the verification itself. As formal verification is an
expensive process, it is suitable to remove parts of the program that
do not influence result of the verification process. \sbt{} uses
program slicing based on the DG library\nbsp{}\cite{ChalupaDG} for this.

One of the analyses used during slicing is reaching definitions
analysis. It identifies all instructions that may generate a value of
a variable before the place where the variable is used. Computing
reaching definitions in programs without pointers is trivial and is
done automatically by LLVM\nbsp{}\cite{LLVM} -- the compiler infrastructure
used by \sbt{}. However, computing reaching definitions for
address-taken variables is more complex, as the analysis needs to deal
with dynamic data structures, pointers that might point to multiple
variables and arrays.

The aim of this thesis is to study modern techniques of computing
reaching definitions, implement a faster reaching definitions analysis
to\nbsp{}\sbt{} and compare the new implementation with the original one
experimentally using a non-trivial set of benchmarks.

This thesis is divided into individual chapters as follows: Chapter
\ref{ch:ProgAnalysis} introduces the reader to program analysis in
general. Key terms, such as program, control flow graph or static
single assignment form are defined there. Chapter\nbsp{}\ref{ch:RDA} starts
by defining what a reaching definition is, what is a reaching
definitions analysis and discusses the simplest naive algorithm to
compute reaching definitions in programs without pointers. Analyzing
programs with pointers is discussed next. Then, the chapter continues
by discussing various algorithms to compute reaching definitions. In
Chapter\nbsp{}\ref{ch:Symbiotic}, \sbt{} is introduced and its architecture
is explained. Chapter\nbsp{}\ref{ch:Implementation} discusses LLVM, the DG
library and the chosen implementation approach based on Marker
algorithm\nbsp{}\cite{BraunSSA} is discussed. Chapter\nbsp{}\ref{ch:Experiment}
shows experimental comparison of the original implementation of
reaching definitions analysis and the new
implementation. Implementations are compared in terms of time and
accuracy. Chapter\nbsp{}\ref{ch:Experiment} offers experimental evaluation
of the new analysis and comparison with the original
implementation. Chapter\nbsp{}\ref{ch:Summary} presents possibilities for
future work and summarizes the work.

* DONE Introduction to program analysis
\label{ch:ProgAnalysis}

Tools that find errors in programs use results of program analysis to
decide whether or not there is an error in the program. There are two
sub-techniques of program analysis: dynamic analysis, static
analysis. An error-finding tool may use either approach or some
combination of both. The following sub-chapter briefly introduces both
techniques.

** DONE Static and dynamic program analysis

*Dynamic analysis* is capable of detecting an erroneous state in the
program while it is running. For example, one could replace C =malloc=
and =free= functions by functions that track the allocations. This is
how valgrind[fn::http://valgrind.org/] works. Another familiar example
of a dynamic analysis is unit testing. During unit testing, the
program(or its part) is started with a user-specified input and after
finishing, its output is checked. If a dynamic analysis uncovers an
error, there is definitely an error in the program. The main
disadvantage is that if the analysis did not uncover any error, there
could still be an input that produces an error.

*Static analysis* derives program properties from some program's
representation without directly executing it. It could be the source
code, compiled machine code or some kind of intermediate
representation (such as Java bytecode or LLVM bitcode). Static
analysis is able to uncover errors without knowing the input of the
program. If an error is discovered, static analysis can synthesize an
instance of input data for which the program enters the erroneous
state. The main advantage is, that this technique can discover errors
overlooked both by programmers and testers. Another advantage is, it
does not require instance of input data, as it is not based on running
the program. However, it can be very expensive, especially
when the analyzed program is too big or complex.

This thesis focuses on reaching definitions analysis, which is a
static analysis. The following sub-chapter continues with a more in-depth
introduction to the static analysis.

** DONE Key terms of static program analysis

A\nbsp{} /program/ is a sequence of elementary instructions.  Program's
structure is reflected in its /control flow graph/. Formally, /control
flow graph/ (CFG for short\index{CFG}) of a program $\mathcal P$ is a
graph $G = (V, E)$, where $V$ is a finite set of vertices and $E
\subseteq V \times V$ is a set of edges. Each instruction of $\mathcal P$
is represented by a vertex. If there exists a run of the program
$\mathcal P$ where instruction $I_2$ is executed immediately after
instruction $I_1$, then $(I_1, I_2) \in E$. We ignore labels on branches,
as they are not needed for reaching definitions
analysis. Figure\nbsp{}\ref{fig:programCFG} shows a simple program in C
language and its control flow graph.

A /path/ in a CFG $(V, E)$ is a sequence $v_1, v_2, v_3, \cdots, v_n$ such, that:
- $v_1,v_2, v_3, \cdots, v_n \in V$, where $n \in \mathbb N$
- $\forall 1 \le i < n: (v_i, v_{i+1}) \in E$

#+BEGIN_LaTeX
  \begin{figure}
    \begin{minipage}[b]{0.5\textwidth}
      \begin{lstlisting}[language=C]
        int $i$;
        scanf("%d", &i);
        if ($i$ % 2 == 0)
            puts("even");
        else
            puts("odd");
        puts("exit");
      \end{lstlisting}
    \end{minipage}
    \begin{minipage}[t]{0.5\textwidth}
      \begin{tikzpicture}
      \tikzstyle{arr} = [->,shorten <=1pt,>=stealth',semithick]
        \node[draw, rectangle] (A) at (0, 0) {int $i$};
        \node[draw, rectangle] (B) at (0, -1.2) {scanf("\%d", \&$i$)};
        \node[draw, rectangle] (C) at (0, -2.4) {if $i$ \% 2 == 0};
        \node[draw, rectangle] (D) at (-1.5, -3.6) {puts("even")};
        \node[draw, rectangle] (E) at (1.5, -3.6) {puts("odd")};
        \node[draw, rectangle] (F) at (0, -4.8) {puts("exit")};
        \draw[arr] (A) -- (B);
        \draw[arr] (B) -- (C);
        \draw[arr] (C) -- (D);
        \draw[arr] (C) -- (E);
        \draw[arr] (D) -- (F);
        \draw[arr] (E) -- (F);
      \end{tikzpicture}
    \end{minipage}
    \caption{Program in C language and its control flow graph}
    \label{fig:programCFG}
  \end{figure}
#+END_LaTeX

A\nbsp{} /variable/ is a fixed-size storage cell in memory. A\nbsp{}
/definition/ of a variable is any instruction that 
modifies its value. A\nbsp{} /use/ of a variable is any instruction
that reads its value. 

** DONE Static single assignment form
Programs may be transformed without changing their behaviour. One of
transformations that do not change program's behaviour is
transformation to Static Single Assignment form (or SSA for
short)\nbsp{}\cite{CytronSSA}. The transformation itself yields some useful data about the
program and the SSA form is particularly useful for compilers and code
analyzers.

A program $\mathcal P$ is in /Static Single Assignment form/ if, and
only if for each variable in $\mathcal P$, there is at most one
definition. Figure\nbsp{}\ref{fig:programSSA} shows a simple program and its
SSA form.

#+BEGIN_LaTeX
    \begin{figure}[]
    \begin{minipage}[t]{0.5\textwidth}
      \begin{lstlisting}[language=C]
        int $i$ = 1;
        int $j$ = 1;
        $i$ = $i$ + $j$;
        $j$ = $j$ + $i$;
        foo($i$, $j$);
      \end{lstlisting}
    \end{minipage}
    \begin{minipage}[t]{0.5\textwidth}
      \begin{lstlisting}[language=C]
      int $i_1$ = 3;
      int $j_1$ = 4;
      $i_2$ = $i_1$ + $j_1$;
      $j_2$ = $j_1$ + $i_2$;
      foo($i_2$, $j_2$);
      \end{lstlisting}
    \end{minipage}
    \caption{Program and its SSA form}
    \label{fig:programSSA}
    \end{figure}
#+END_LaTeX

Constructing the SSA form is a little more interesting in case the CFG of
a program contains loops. Consider program in Figure\nbsp{}\ref{fig:loop1}.

#+BEGIN_LaTeX
  \begin{figure}[h]
      \begin{lstlisting}[language=C]
  int $i$ = 0; $\encircle{1}$
  while ($i$ < 10) {
      printf("%d\n", $i$); $\encircle{2}$
      $i$++;  $\encircle{3}$
  }
      \end{lstlisting}

    \caption{Simple C program with loops}
    \label{fig:loop1}
    \end{figure}
#+END_LaTeX

\noindent While constructing SSA form of this program, the use of =i= variable
at location =B= could be replaced by the assignment to =i= at location
=A= or =C=. The problem is, that both of these statements contribute
to the value of =i= at location =B=. It is, therefore, necessary to
use some kind of combination of values from =A= and =C=. This is what
a \phi function is for. $i_3 \gets \phi(i_1, i_2)$ denotes, that the value
of $i_3$ could be either $i_1$ or $i_2$. After transforming that program
to SSA form, it looks as shown in figure\nbsp{}\ref{fig:loop2}.

#+BEGIN_LaTeX
  \begin{figure}
    \begin{lstlisting}[language=C]
      int $i_1$ = 0;
      int $i_2$;
      int $i_3$;

      while ($i_2 = \phi(i_1, i_3), i_2 < 10$) {
        printf("%d\n", $i_2$);
        $i_3$ = $i_2$ + 1;
      }
    \end{lstlisting}
\caption{SSA form of the program from figure~\ref{fig:loop1}}
\label{fig:loop2}
  \end{figure}
#+END_LaTeX

* Reaching Definitions Analysis
\label{ch:RDA}
# TODO invent notation for =z= is a reaching definition of =a= at =y=.
This chapter starts by explaining what a reaching definition is and
demonstrating the simplest naive algorithm for computing reaching
definitions. It continues by discussing properties of reaching
definitions analyses and introduces various algorithms to compute
reaching definitions.

\label{def:RD}Let $\mathcal P$ be a program. A /reaching definition/
\index{RD} of variable $\mathcal V$ used by instruction $I_1$ is an
instruction $I_2$ such, that:
+ $I_1, I_2$ are part of $\mathcal P$
+ $I_1$ is a use of variable $\mathcal V$
+ $I_2$ is a definition of variable $\mathcal V$
+ exists a run of $\mathcal P$ where the value of $\mathcal V$ was not modified
  by any instruction on path from $I_2$ to $I_1$ in the CFG

Figure \ref{fig:programRD} shows program and its CFG with reaching
definitions. The =scanf= function loads an integer from standard input
and stores it to the memory address $\&i$. Therefore, it is a
definition of $i$.

#+BEGIN_LaTeX
    \begin{figure}[h]
      \begin{minipage}[b]{0.5\textwidth}
        \begin{lstlisting}[language=C]
          int $i$;
          scanf("%d", &$i$);
          if ($i$ % 2 == 0)
              puts("even");
          else
              puts("odd");
          puts("exit");
        \end{lstlisting}
      \end{minipage}
      \begin{minipage}[t]{0.5\textwidth}
        \begin{tikzpicture}
        \tikzstyle{arr} = [->,shorten <=1pt,>=stealth',semithick]
  \tikzstyle{rd} = [->,shorten <=1pt,>=stealth',dashed]
          \node[draw, rectangle] (A) at (0, 0) {int $i$};
          \node[draw, rectangle] (B) at (0, -1.2) {scanf("\%d", \&$i$)};
          \node[draw, rectangle] (C) at (0, -2.4) {if $i$ \% 2 == 0};
          \node[draw, rectangle] (D) at (-1.5, -3.6) {puts("even")};
          \node[draw, rectangle] (E) at (1.5, -3.6) {puts("odd")};
          \node[draw, rectangle] (F) at (0, -4.8) {puts("exit")};
          \draw[arr] (A) -- (B);
          \draw[arr] (B) -- (C);
          \draw[arr] (C) -- (D);
          \draw[arr] (C) -- (E);
          \draw[arr] (D) -- (F);
          \draw[arr] (E) -- (F);
          \draw[rd]  (C.west) to [out=150,in=180] (B.west);
        \end{tikzpicture}
      \end{minipage}
      \caption{Program in C language, its CFG and reaching definition. Solid edges are part of CFG, dashed edge represents a reaching definition.}
      \label{fig:programRD}
    \end{figure}
#+END_LaTeX

** DONE Dense reaching definitions analysis
\label{denseRDA} One of the ways to calculate reaching definitions is
to ``follow'' the control flow graph of the program while remembering
the last definition for each variable for each of its vertices. This
is a traditional approach used by many tools.

Figure \ref{fig:denseRDA} demonstrates the algorithm.

#+BEGIN_LaTeX
  \begin{figure}[H]
    \begin{algorithm}[H]
      \SetAlgoLined
      \KwData{Control Flow Graph as $(V, E)$, for every $v \in V$, $v.defs$ is a set of variables defined in $v$}
      \KwResult{for every $v \in V$ and every variable $x$, $v.rd(x)$ is a set of reaching definitions for variable $x$ in $v$}
      
      \For{$v \in V$} {
        \For{$def(x) \in v.defs$} {
          $v.rd(x) \gets v.rd(x) \cup \{ v \}$ \;
        }
      }
      \While{\text{not fixpoint}} {
        \For{$v \in V$ in DFS order} {
          \For{$(u, v) \in E$} {
            \For{$def(x) \in u.defs$} {
              $v.rd(x) \gets v.rd(x) \cup \{ u \}$ \;
            }
          }
        }
      }
    \end{algorithm}
    \caption{Dense reaching definitions analysis algorithm}
    \label{fig:denseRDA}
  \end{figure}
#+END_LaTeX

The algorithm starts by adding reaching definitions to CFG nodes that
are definitions. Then, the reaching definitions are propagated
throughout the entire CFG of the program until fixpoint is
reached.

** DONE Properties of reaching definitions analyses
It is impossible for reaching definitions analyses to find precise
definitions of a specified ``variable''. Because of that, it is
necessary to perform an abstraction 


Reaching definitions analyses have some properties\nbsp{}\cite{rptRDA} that
affect their accuracy. Less accurate analyses need to make some
conservative assumptions about the program in order to be
correct. This sub-chapter describes three properties of reaching
definitions analyses: instance-wiseness, field sensitivity and ability
to recognize execution patterns.

*** DONE Instance-wise and statement-wise analysis
When analyzing programs with a cyclic CFG, there are multiple
/instances/ of instructions that can be executed repeatedly. Each
execution of an instruction creates a new instance of the instruction.

Along with the definition, use and variable, an instance-wise reaching
definitions analysis is able to tell which instance of the
instructions are involved. The information about instance might
involve for example the for loop indexing variable =i=. There might be
more variables in case the instruction is inside of a nested loop.

Differences between instance-wise analysis and statement-wise analysis
will be demonstrated on a simple program in figure \ref{fig:instWise}.

#+BEGIN_LaTeX
  \begin{figure}
    \begin{lstlisting}[language=C]
      int $a$ = 0; $\encircle{1}$

      for(int $i$ = 0; $i$ < 5; ++$i$) {
        int $b$ = $a$ + $i$; $\encircle{2}$
        $a$ = $b$; $\encircle{3}$
      }
      \end{lstlisting}
      \label{fig:instWise}
      \caption{Demonstration of differences between statement-wise and instance-wise analysis}
      \end{figure}
#+END_LaTeX

Reaching definitions for $a$ at location \encircle{2} are \encircle{1}
and \encircle{3}. However, there are multiple instances of
instructions \encircle{2} and \encircle{3}. Firstly, both
instance-wise and statement-wise analyses would report, that
\encircle{1} is a reaching definition of $a$ at \encircle{2}. The
difference is, how much information the analysis is able to provide
about the reaching definition \encircle{3} at
\encircle{2}. Statement-wise analysis would simply state, that
\encircle{3} is a reaching definition of $a$ at
\encircle{2}. Instance-wise analysis goes a little further by
reporting, that $\encircle{3}^{i+1}$ is a reaching definition of $a$ at
$\encircle{2}^i$. The upper index denotes the index of iteration.

*** DONE Field sensitivity
Usage of aggregated data structures, such as arrays or C language
=struct=-s introduces another issue that needs to be addressed by a
reaching definitions analysis. Precision of analysis for programs that
use aggregated data structures depends on whether the analysis can
distinguish between individual elements of the data structure.

Consider the program in Figure\nbsp{}\ref{fig:rdaFS}. Locations =x= and =y= in the
program define the first and the second element of $a$. After
that, location =z= contains a function call that uses the third
element of the array. This element has no definitions in the program,
so an accurate reaching definitions should find no definitions for it.

A field-sensitive analysis considers array indices and correctly
reports no reaching definitions for $a[2]$ at location =z=.

A field-insensitive analysis ignores indices of the array and for
location =z=, it would report, that reaching definitions of $a[2]$ are
=x= and =y=. This is an over-approximation that has to be performed by
the field-insensitive analysis.

#+BEGIN_LaTeX
  \begin{figure}[H]
    \begin{lstlisting}[language=C]
      int $a$[5];
      $a$[0] = 1; // x
      $a$[1] = 2; // y
      foo($a$[2]); // z
    \end{lstlisting}
    \caption{Demonstration of field-sensitive reaching definitions analysis}
    \label{fig:rdaFS}
    \end{figure}
#+END_LaTeX

# **** TODO Unknown offsets and over-approximation
# If there are multiple instances of an instruction (e.g. it is part of
# a loop or recursive function), it could happen, that the index is
# simply unknown. Consider a simple program:

# #+BEGIN_SRC c
#   int a[5];
# 
#   a[2] = 15;
#   for(int i = 0; i < 2; ++i) {
#      a[i] = i; // x
#  }
#
#  for(int i = 0; i < 3; ++i) {
#      printf("%d\n", a[i]); // y
#  }
# #+END_SRC
#
# Assuming that no for loop is unrolled by the compiler, reaching
# definitions analysis is unable to tell which part of =a= is being
# defined at location =x=.

# - stretch it to whole array
# - consider as weak update
\clearpage
*** DONE Execution patterns recognition

Reaching definitions analysis is often not the only analysis that is
part of a program analysis framework. More often than not, there are
more analyses that derive various properties of program or its
parts. Reaching definitions analysis can sometimes take advantage of
results of previously ran analyses and achieve better accuracy or
speed.

Consider the program in figure\nbsp{}\ref{fig:execPatterns}.

#+BEGIN_LaTeX
  \begin{figure}
    \begin{lstlisting}[language=C]
      int foo(int a) {
        int c = 0;
        if (a < 0) {
          c = 1; // x
        }
        if (a >= 0) {
          c = 2; // y
        }
        return c; // z
      }
    \end{lstlisting}
    \label{fig:execPatterns}
    \caption{Demonstration of effects of execution patterns recognition on reaching definitions analysis}
  \end{figure}
#+END_LaTeX

If an external analysis reports that there is no program execution
where $a < 0$, the reaching definitions analysis could take this into
account and derive that =x= is not a reaching definition of =c= at =z=
even despite the fact it is a definition of a simple
variable. Analysis that does not take it into account would report
that both =x= and =y= are reaching definitions of =c= at =z=.

In this case, an analysis that does not recognize execution patterns
yields an over-approximation, which is not a problem.
** TODO Analyzing programs that use pointers
One of the most important features of programming languages are
pointers. They can be utilized to implement dynamic data structures,
which are very widely used. However, pointers also add more ways the
program can fail. For example, dereferencing a pointer with invalid
value may cause the program to crash.

As pointers make it possible to create variables that 
# TODO change hold variables to something better
``hold variables'', they inherently make programs more difficult to
understand and analyze.

In order to compute reaching definitions for address-taken variables,
reaching definitions analysis uses information from pointer analysis
which took place prior to RDA.

*** DONE Pointer analysis
Pointer analysis\nbsp{}\cite{ChalupaPTA} is, similarly to reaching
definitions analysis, a static program analysis. It computes a set
$\mathcal V$ of variables for each pointer $p$. This set will be
referred to as /points-to/ set. If $p$ may point to some variable $v$,
then $v \in \mathcal V$.

Reaching definitions analysis uses these data from pointer analysis to
recognize uses and definitions of variables. Accuracy of the reaching
definitions analysis, therefore, depends on accuracy of the underlying
pointer analysis. Namely, if the pointer analysis performs an
over-approximation, so will the reaching definitions analysis.

*** TODO Strong and weak definitions, kill and gen sets
\label{strongWeakUpdate}

Each definition of a variable can be either /strong/ or /weak/. Strong
definition over-writes the variable with a new value. When a strong
definition is encountered, it invalidates all previous definitions of
the variable. Weak definition, on the other hand, does not necessarily
over-write the variable, so it does not invalidate previous
definitions. That means, for each use of a variable there might be
multiple weak definitions, but at most one strong definition.

Literature commonly uses $KILL$ and $GEN$ sets to represent strong and
weak definitions. For every vertex $x$ of control flow graph, $GEN(x)$
is set of variables for which $x$ is a definition. $KILL(x) \subseteq
GEN(x)$ is set of variables that are overwritten in this vertex.

In programs that do not use pointers, all definitions of variables are
strong.

*** DONE Field-sensitivity

Usage of aggregated data structures, such as arrays or C language
=struct=-s introduces another issue that needs to be addressed by a
reaching definitions analysis. Precision of analysis for programs that
use aggregated data structures depends on whether the analysis can
distinguish between individual elements of the data structure.

While analyzing programs with arrays or aggregate data structures,
distinguishing between definitions of individual elements of the data
structure makes the analysis more accurate.

Consider the program in Figure\nbsp{}\ref{fig:rdaFS}. Locations =x= and =y= in the
program define the first and the second element of the array =a=. After
that, location =z= contains a function call that uses the third
element of the array. This element has no definitions in the program,
so an accurate reaching definitions should find no definitions for it.

A field-sensitive analysis considers indices and correctly reports no
reaching definitions for =a[2]= at location =z=.

A field-insensitive analysis ignores indices of the array and for
location =z=, reports that reaching definitions of =a[2]= are
=x= and =y=. This is an over-approximation that has to be performed by
the field-insensitive analysis.

#+BEGIN_LaTeX
  \begin{figure}[H]
    \begin{lstlisting}[language=C]
      int a[5];
      a[0] = 1; // x
      a[1] = 2; // y
      foo(a[2]); // z
    \end{lstlisting}
    \caption{Demonstration of field-sensitive reaching definitions analysis}
    \label{fig:rdaFS}
    \end{figure}
#+END_LaTeX


Field-sensitivity of an RDA always depends on the field-sensitivity of
the underlying pointer analysis. If the pointer analysis is
field-insensitive, the RDA that uses its results will be
field-insensitive, too.

** DONE Demand-driven reaching definitions analysis
Apart from the dense algorithm, several other algorithms to compute
reaching definitions have been introduced. This subchapter briefly
introduces demand-driven reaching definitions
analysis\nbsp{}\cite{SootDDRDA}.

The main idea of this approach is to answer the question ``can a
definition $d$ of variable $v$ reach a program point $p$?''. This
question is referred to as /query/ and it is represented by a triple
$(d, p, v)$. After a query is generated, it is propagated backwards
along nodes of the CFG. Each node may either answer the query or
continue the propagation to its predecessors. If a node $x$ contains a
definition of $v$, the query propagation stops. The answer is yes, if
and only if $x = d$. If $x \ne d$, then node $x$ kills the
definition $d$ before it can reach $p$ along the path.

In case a program point $p$ has $n$ predecessors, it is sufficient
that the reachability of $d$ is reported by at least one of them.

With an inter-procedural CFG, this approach can be used with function
summaries\nbsp\cite{ipFS1}\nbsp{}\cite{ipFS2}. The function summaries enable
them to process functions once per variable in the program.

# TODO explain slicing criterion somewhere?

It is worth noting, that this approach has a special property that
makes it suitable for a slicer: It is able to start from the slicing
criterion and gradually find all definitions that affect the
criterion. This way, it can avoid computing irrelevant information.

** TODO Sparse dataflow analysis
Another approach to computing reaching definitions was introduced by
Madsen and M\o{}ller \cite{MadsenSDAPR}. 

- SSA based
- frontier edge
- uses dominator tree
- fixpoint computation
** TODO Algorithms based on static single assignment form
\label{SSArd} Algorithms that transform a program into SSA form
replace modified variables in assignments by new, artificially-created
variables. They also replace variables in uses by the most recent
definition -- reaching definition. This property of SSA form can be
utilized while calculating reaching definitions.

# TODO maybe, define a simple framework for these algorithms
# so they can be plugged in to the final reaching definitions stage

# TODO program, SSA form, reaching definitions

For the purpose of this thesis, I have studied two algorithms for
computing SSA form. One of them has been introduced by
Cytron et al\nbsp{}\cite{CytronSSA} and relies on dominance frontiers.  The
second algorithm, invented by Braun et al\nbsp{}\cite{BraunSSA}, is simpler
and has been experimentally proven to be more efficient\nbsp{}\cite{BraunSSA}.
*** TODO Cytron algorithm

Algorithm introduced by Cytron et al.\nbsp{}\cite{CytronSSA} uses dominance
information to pre-calculate locations of \phi nodes. In the later
phase, variables are numbered using a simple stack of counters and \phi
nodes are filled with operands.

This approach was proven to produce minimal SSA form.

*** TODO Marker algorithm
\label{marker}

The Marker algorithm\nbsp{}\cite{BraunSSA} has two phases: local value
numbering and global value numbering.

During *local value numbering*, it computes SSA form of every basic
block of the program. For every basic block, it iterates through all
instructions in execution order. If an instruction $I$ defines some
variable $\mathcal V$, $I$ is remembered as the current definition of
$\mathcal V$. If an instruction $I$ uses some variable $\mathcal V$,
the algorithm looks up its definition. If there is a current
definition $\mathcal D$, the use of variable $\mathcal V$ is replaced
by use of the numbered variable that corresponds to $\mathcal D$.

*Global value numbering* is involved once no definition for the
specified variable can be found in the current basic block. The
algorithm places a \phi node on top of the current block and starts
recursively searching the CFG for the latest definition in all
predecessors of the current block. Once a definition is found, it is
added as an operand to the \phi node.

When looking up a definition of a variable from a predecessor block,
the block does not have to be processed yet. If that is the case, the
algorithm does not have any idea about which variables are defined in
that block. This happens when the program's CFG is cyclic --
e.g. recursive function is called or for loop is used. Because of
that, the algorithm remembers the last definition of variable in a
block during local value numbering. If there is no last definition in
a block, the lookup continues to all predecessors recursively.

Along with this algorithm, Braun et al. present a way to reduce the
number of added \phi nodes, which helps their algorithm produce minimal
SSA form.

** TODO Chosen approach to reaching definitions analysis
\label{densePP} While the dense algorithm is correct, it performs
excessive amount of work by copying information about reaching
definitions to nodes of CFG where it is not necessary at all. Even in
case the target CFG node $n$ uses some memory, it will get reaching
definitions for all variables defined somewhere on a path from entry
node to $n$.

In an attempt to avoid the performance penalty of the dense algorithm
, I have decided to implement a reaching definitions algorithm based
on transformation to SSA form.
# TODO why?? need to document other approaches aswell

Thanks to the lazy nature of the Marker algorithm, I have concluded
that it is the perfect candidate for a new reaching definitions
algorithm. Another big advantage is, that it does not depend on having
pre-calculated other data structures, such as dominator tree,
dominance frontiers or others.

This thesis presents two algorithms for transformation into SSA
form. I have decided to use Marker algorithm. Marker algorithm is
simpler, easily extensible and there are multiple stages of
implementation which leaves room for further optimizations. It has
also been experimentally proven\nbsp{}\cite{BraunSSA} to be faster than
Cytron et al algorithm.

Rather than propagating all information to every single node of the CFG,
the amount of information propagated can be reduced by propagating
only variables that are dereferenced only to nodes that correspond to
instructions that use the variable.

*** TODO Field sensitivity
Braun et al.\nbsp{}\cite{BraunSSA}, however, do not elaborate on field
sensitivity. In order to use it, it is necessary to modify it.
*** TODO Optimizations
**** TODO Start lookup only in load instructions
**** TODO Lookup only used variables
**** TODO Strong updates stop the lookup

* DONE Symbiotic
\label{ch:Symbiotic}
\sbt{} is a modular tool for formal verification of programs based
on the LLVM compiler infrastructure\nbsp{}\cite{LLVM}. It is being developed at
Faculty of Informatics, Masaryk University.

** DONE How Symbiotic works
\sbt{} works by combining three well-known techniques:
1. Instrumentation
2. Slicing
3. Symbolic Execution

*Instrumentation* is responsible for inserting memory access checks into
the program. It overrides memory allocation functions by its own, that,
besides performing the allocation itself, add the allocated memory
along with allocation size into a global data structure. When
dereferencing a pointer, instrumentation inserts a check to verify
whether this pointer is inside allocated bounds or not. There is an
assertion that crashes the program if a dereference is out of bounds
of allocated memory.

*Slicing* is a technique that reduces size of the program by removing
parts that do not influence its correctness with respect to given
criterion. For slicing to work, there has to be a /slicing criterion/
specified. Slicing criterion is an =assert= instruction. The slicer
computes which instructions the slicing criterion is dependent on. For
that, it uses results of reaching definitions analysis.

*Symbolic execution* is the last step. It is a technique that decides
whether the program could violate a condition of some assertion in the
program.

* TODO Implementation
\label{ch:Implementation} This chapter starts by introducing the LLVM
infrastructure and the DG library\nbsp{}\cite{ChalupaDG}. The introduction
is followed by an in-depth discussion of the new reaching definitions
analysis implementation.

** TODO DG Library
The slicer used in \sbt{} uses the DG library to calculate dependence
graph and slice away unnecessary parts of verified program. New
reaching definitions analysis has been implemented to the DG library,
so it can be used with any software that uses DG.

DG itself provides multiple analyses that are independent of the
assembly code used. It contains instantiation of those analyses for
LLVM.

*** TODO LLVM

One of tools from the LLVM family is clang -- a compiler of C language
to the LLVM intermediate representation (IR).

LLVM defines its own intermediate representation of a program. The
intermediate representation

- not source, not machine code. like assembler
- example: C program, LLVM IR
- define important instructions

**** TODO Partial static single assignment form
\label{partialSSA}
Partial SSA form means, that there is at most one definition for each
register. This form of program, however, makes no guarantees about
address-taken variables. Those are *not* in SSA form.
# TODO some figure with partial SSA form

Thanks to the partial SSA transformation, LLVM already provides
 reaching definitions information for its register variables.

*** TODO Pointer analysis in DG
**** TODO unknown memory
**** TODO unknown offset
*** TODO Reaching definitions analysis framework in the DG library
DG uses reaching definitions analysis to calculate control
dependencies between instructions. The old reaching definitions
analysis in DG uses the dense approach, as described in
\ref{denseRDA}.

Prior to the reaching definitions analysis itself, DG builds a
subgraph of program's control flow graph\index{CFG}. The subgraph does
not contain all types of instructions. Rather, it consists only of
store instructions, call instructions, return instructions and all
memory allocations. In spite of not containing all instructions, it
reflects structure of the program.

Each instruction in the subgraph that defines some memory object has
an associated points-to information from pointer analysis. Thanks to
this, it is possible to tell which variables are strongly or weakly
defined by a store instruction.

** TODO Reaching definitions analysis implementation approach
The new reaching definitions analysis is implemented in the DG
library. This chapter describes how the new reaching definitions
analysis has been implemented in the existing framework.

Thanks to LLVM's transformation to partial SSA form (as described in
\ref{partialSSA}), there is no need to take care of LLVM register
variables, as they are already taken care of while translating the C
program into LLVM Intermediate Representation. Therefore, the
implementation focuses on address-taken variables.

*** DONE Subgraph builder abstractions
As there are some modifications done to the subgraph builder, the
first step towards the implementation is to introduce an abstraction
for reaching definitions subgraph builder. The abstraction allows the
legacy subgraph builder to be preserved, while a new one can be added,
too.

The goal was to allow the user of =ReachingDefinitions= class to run
any reaching definitions analysis they would choose. The pointer
analysis framework in the DG library already allows the user to
specify pointer analysis to run using templates. Similar approach was
taken here with the reaching definitions analysis.

Each reaching definitions analysis in the DG library could require
different set of information about in the reaching definitions
subgraph. With that in mind, I have decided to allow each reaching
definitions analysis to use different subgraph builder. A subgraph
builder builds a reaching definitions subgraph from some
representation. Therefore, I have designed and implemented an interface for
subgraph builder called =LLVMRDBuilder=. This interface allows to
implement a =build= function, that returns the entry node of the
reaching definitions subgraph.

*** DONE Adding use information to control flow graph

Now, the subgraph builder can add information about pointer
dereferences -- that is, LLVM =load= instructions to the reaching
definitions subgraph. Pointer analysis is utilized here to find out
which variables are being used. As one pointer could simply point to
multiple variables, it is necessary to add information about all
variables that could potentially be used.

In the subgraph builder used with the new analysis,
=LLVMRDBuilderSemisparse=, I have instructed the subgraph builder to
include LLVM's load nodes. For each load node, it then queries the
pointer analysis for all variables its dereferenced pointer operator
could point to. For looking up the variables, it uses a
newly-introduced method =getPointsTo=, which fetches the information
from the pointer analysis.

The =load= instruction could possibly use a smaller portion of the
memory than the allocation size. This is the case when accessing an
individual element of a larger data structure. A field-sensitive
reaching definitions analysis requires the length to be set to the
length that is being used. This is done by determining size of the
type the value is being loaded to.

*** TODO Splitting basic blocks on function calls
- why splitting blocks?
- split basic block
- inline the function
*** DONE Treating unknown memory
Sometimes, pointer analysis was unable to tell where a pointer may
point, so the analysis has to make some conservative assumptions about
the program in order to be correct. In this case, the analysis assumes
that such pointer could point to any variable and treats the CFG node
as if it was a definition or a use of all variables in the
program. Whether it is a definition or a use is decided based on
semantics of the instructions and how the pointer is used.

After the subgraph is built, it is searched by a separate class
=AssignmentFinder=, which does exactly what was explained above. It
uses a two-phase algorithm to do that: In the first phase, all
variables in the program are added to a list. In the second phase,
every store to an unknown pointer and load from an unknown pointer
turn into weak definition of all variables in the program or use of
all variables in the program, respectively. Doing this removes some
complex handling of unknown pointers from the next phase of the analysis.

*** TODO Field-sensitivity
\label{chap:intervals}
The Marker algorithm itself does not consider aggregate data
structures. In order to support analyzing them, it needs to be
modified a little.

Each definition or use of a variable have an associated interval of
affected bytes. This interval is later used to look up reaching
definitions of a variable. An interval has a start and a length.

Intervals act as an abstraction on top of definition sites. They make
it possible to calculate an intersection or union.

The first intermediate data structure that is part of this framework
is DisjointIntervalSet. The set allows to insert intervals while
maintaining an invariant, that all intervals inside are disjoint. When
inserting an interval that has a non-empty intersection with some of
the intervals inside, the set ensures that these two intervals are
converted into a single interval, which is a union of the two.

IntervalMap is the second important data structure of the
framework. It provides functionality that makes the analysis
field-sensitive. IntervalMap on the first sight looks similarly to
=std::map= available in C++. It allows to save arbitrary types under
=Interval= keys. The difference is in the lookup
functions. =IntervalMap= offers 3 main functions: =collect=,
=collectAll= and =killOverlapping=.

The =collect= function is designed to work with strong updates. It
searches the entries backwards, starting by the last entry added. 
It collects all values from the interval map such, that the specified
interval is covered by union of key intervals of the values returned.

=collectAll= works with weak updates. As opposed to =collect=, it
does not stop when the specified interval is subset of union of the
result key intervals. Rather, it searches the whole IntervalMap and
returns all values which are saved under intervals that overlap with
the specified interval.

=killOverlapping= deletes definitions with intervals that overlap with
specified interval. After =killOverlapping=, calling =collectAll= with the
same interval or any of its subsets returns an empty result.

*** DONE Weak updates
The Marker algorithm maintains two main data structures for processing
the strong updates: =last_def=, =current_def=. To incorporate weak
updates, they have been duplicated with names =last_weak_def= and
=current_weak_def=.

=last_weak_def= is used during local value numbering to remember the
last weak definition in a block. When a strong definition is
encountered, overlapping weak definitions are either killed or have
their definition intervals shrank. The =killOverlapping= function of
=IntervalMap= introduced in\nbsp{}\ref{chap:intervals} is used for that.

When a strong definition is encountered during global variable
numbering, current weak definitions that overlap with the strong
definition must be killed.

#+BEGIN_SRC cpp
  current_weak_def[var.target][block].killOverlapping(interval);
#+END_SRC

Encountering a weak update involves simply adding it to =current_def=
in global value numbering, or =last_def= in local value numbering.

*** TODO Treating unknown offset
The offset of a pointer can sometimes be unknown. This usually happens
when accessing an array using an indexing variable while its value is
unknown. 

- stretch to max interval
- weak update

*** TODO Computing reaching definitions from Marker algorithm
The Marker algorithm is capable of computing SSA form of a
program. The program, however, does not need to be transformed into
SSA form. The only requirement is to obtain reaching definitions.

I have decided to split up the computation into two phases:
constructing a sparse RD graph and propagating definitions.

In the first phase, the implementation constructs a sparse RD
graph. Sparse RD graph is a graph, where for every reaching definition
$(I_1, I_2)$ exists a path from $I_1$ to $I_2$. The path may consist of
multiple \phi nodes, but it might be trivial as well. The construction is
fairly straightforward: whenever a variable use $u$ is encountered,
lookup all definitions of the variable using =readVariable=. Then, for
every definition $d$ of the variable, add an edge $(u, d)$ to the
sparse RD graph.

In the second phase, the control flow graph $(V, E)$ of the program is
traversed once again. For every use $u \in V$, a BFS search of the
sparse RD graph is started in $u$. If the definition found is not a \phi
node, it is added as a reaching definition.

* TODO Experimental evaluation of the new analysis
\label{ch:Experiment} 
** TODO Time
** TODO Memory Used
- the interval framework occasionally uses too much memory
- should try if field-insensitive analysis would perform better
** TODO Accuracy
Thanks to the ``interval framework'' introduced
in\nbsp{}\ref{chap:intervals}, the new implementation of semi-sparse
analysis is more accurate than the original implementation. Consider this program:

  #+BEGIN_SRC c
    int a[] = {0, 1, 2, 3}; // A
    a[0] = 5; // B
    a[1] = 6; // C
    a[2] = 7; // D
    a[3] = 8; // E

    for (size_t i = 0; i < 4; ++i) {
        printf("%d\n", a[i]); // RD(a) = ???
    }
  #+END_SRC
  - the original analysis reports $RD(a) = \{ A, B, C, D, E\}$
  - the new analysis is able to tell that $\{B,C,D,E\}$ together
    over-write the whole range of =a= and therefore reports $RD(a) =
    \{B,C,D,E\}$
* TODO Conclusion
\label{ch:Summary}

** TODO Possible future work

It is possible to further speed up computation of Reaching Definitions
by incorporating the trivial phi node removal
algorithm\nbsp{}\cite{BraunSSA}. The sparse graph contains many redundant \phi
functions that could be removed to speed up the final phase of
reaching definitions propagation.

As the algorithm is implemented in a slicer, could be improved even
further by starting at the slicing criterion and searching the control
flow graph backwards for definitions of variables that affect the
slicing criterion, which is what the slicer needs to derive the
control dependencies.

The =IntervalMap= data structure used in MarkerFS builder could be improved.

The reaching definitions analysis could benefit from additional
accuracy it could gain by considering different instances of
statements.

Newer versions of LLVM support a pass called
mem2reg[fn::https://llvm.org/docs/Passes.html#mem2reg-promote-memory-to-register]. This
pass is able to convert local variables into registers, which are in
SSA form. It would be interesting to use mem2reg pass whenever
possible and then run this analysis to obtain results for arrays and
other structures mem2reg is unable to handle.

Another interesting LLVM pass to test would be scalar replacement of
aggregates[fn::https://llvm.org/docs/Passes.html#sroa-scalar-replacement-of-aggregates]. This
pass replaces arrays and structures by scalar values in case it is
possible.

** TODO Summary of work done
As a part of this thesis, I studied four algorithms for computing
reaching definitions. Then, I chose to implement an SSA-based
algorithm into \sbt{}. Prior to implementation, I have designed
modifications for the algorithm to work with aggregate data structures
and weak updates. The new implementation is then
compared with the original implementation.

\printbibliography[heading=bibintoc]
