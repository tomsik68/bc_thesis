#+TITLE: Improvements of reaching definitions analysis in Symbiotic
#+AUTHOR: Tomáš Jašek
#+LATEX_CLASS:         fithesis
#+OPTIONS:             todo:nil toc:nil
#+LATEX_CLASS_OPTIONS: [nolot,nolof,twoside]
#+LATEX_HEADER:        \input{setup.tex}
* DONE Introduction

Nowadays, computer programs control gadgets everywhere around
us. Programs are used in wide variety of fields that impose diverse
requirements on their reliability. For example, in medical
applications it is crucial that a program which controls a piece of
medical equipment does not misbehave or crash under any circumstances.

On the other hand, programmers make mistakes very often. For example,
a programmer could easily forget to close a brace or add a semicolon
at the end of a line. Mistakes similar to those result into syntax
errors. Compiler is able to detect them while processing the program
and sometimes offer a suitable replacement, too. Compilers, however,
do not understand intents of a human programmer. Therefore, it is
still possible for the programmer to express their intents in an
incorrect way producing a logical error. Usually, compiler is also
unable to detect mistakes that make the program access memory which is
owned by, for example, some other program. On modern operating
systems, these mistakes usually cause the program to crash.

Despite the availability of safe modern low-level programming
languages, such as Rust, the C programming language is still very
widespread. It is being used for critical purposes such as operating
systems, embedded systems and device drivers. With its flexibility, it
has some shortcomings that cause programmers to make mistakes
often. One of the shortcomings is, that memory management is left to
the programmer. A programmer could easily forget to free allocated
memory, unintentionally free the same memory twice or forget to check
the result of a memory allocation function, which may fail.

Several formal verification tools are able to detect such errors. One
of them is \sbt{}. \sbt{} firstly inserts error checks into the
program, then reduces the size by removing irrelevant parts of the
program and passes it to symbolic execution framework klee which
performs the verification itself. As formal verification is an
expensive process, it is suitable to remove parts of the program that
do not influence result of the verification process. \sbt{} uses
program slicing based on the DG library\nbsp{}\cite{ChalupaDG} for this.

One of the analyses used during slicing is reaching definitions
analysis. It identifies all instructions that may generate a value of
a variable before the place where the value is used. Computing
reaching definitions in programs without pointers is trivial. However,
computing reaching definitions for in the presence of pointers is more
complex, as the analysis needs to deal with dynamic data structures,
pointers that might point to multiple variables and arrays.

The aim of this thesis is to study modern techniques of computing
reaching definitions, implement a faster reaching definitions analysis
to\nbsp{}\sbt{} and compare the new implementation with the original one
experimentally using a non-trivial set of benchmarks.

This thesis is divided into individual chapters as follows: Chapter
\ref{ch:ProgAnalysis} introduces the reader to program analysis in
general. Key terms, such as program, control flow graph or static
single assignment form are defined there. Chapter\nbsp{}\ref{ch:RDA} starts
by defining what a reaching definition is, what is a reaching
definitions analysis and discusses the simplest naive algorithm to
compute reaching definitions in programs without pointers. Analyzing
programs with pointers is discussed next. Then, the chapter continues
by discussing various algorithms to compute reaching definitions. In
Chapter\nbsp{}\ref{ch:Symbiotic}, \sbt{} is introduced and its architecture
is explained. Chapter\nbsp{}\ref{ch:Implementation} discusses LLVM, the DG
library and the chosen implementation approach based on Marker
algorithm\nbsp{}\cite{BraunSSA}. Chapter\nbsp{}\ref{ch:Experiment}
shows experimental comparison of the original implementation of
reaching definitions analysis and the new
implementation. Implementations are compared in terms of time and
accuracy. Chapter\nbsp{}\ref{ch:Experiment} offers experimental evaluation
of the new analysis and comparison with the original
implementation. Chapter\nbsp{}\ref{ch:Summary} presents possibilities for
future work and summarizes the thesis.

* DONE Introduction to program analysis
\label{ch:ProgAnalysis}

Tools that find errors in programs use results of program analysis to
decide whether or not there is an error in the program. There are two
kinds of program analysis: dynamic analysis and static analysis. An
error-finding tool may use either approach or some combination of
both. The following sub-chapter briefly introduces both.

** DONE Static and dynamic program analysis

*Dynamic analysis* is capable of detecting an erroneous state in the
program while it is running. For example, one could replace C =malloc=
and =free= functions by functions that track the allocations. This is
how valgrind[fn::http://valgrind.org/] works. Another familiar example
of a dynamic analysis is unit testing. During unit testing, the
program(or its part) is started with a user-specified input and after
finishing, its output is checked. If a dynamic analysis uncovers an
error, there is definitely an error in the program. The main
disadvantage is that if the analysis did not uncover any error, there
could still be an input that produces an error.

*Static analysis* derives program properties from some program's
representation without directly executing it. It could be the source
code, compiled machine code or some kind of intermediate
representation (such as Java bytecode or LLVM bitcode). Static
analysis is able to uncover errors without knowing the input of the
program. If an error is discovered, static analysis can synthesize an
instance of input data for which the program enters the erroneous
state. The main advantage is, that this technique can discover errors
overlooked both by programmers and testers. Another advantage is, it
does not require instance of input data, as it is not based on running
the program. However, it can be very expensive, especially
when the analyzed program is too big or complex.

This thesis focuses on reaching definitions analysis, which is a
static analysis. The following sub-chapter continues with a more in-depth
introduction to the static analysis.

** DONE Key terms of static program analysis
\label{ch:KTPA}
#+BEGIN_LaTeX
  \begin{figure}
    \begin{minipage}[b]{0.5\textwidth}
      \begin{lstlisting}[language=C]
        int $i$;
        scanf("%d", &i);
        if ($i$ % 2 == 0)
            puts("even");
        else
            puts("odd");
        puts("exit");
      \end{lstlisting}
    \end{minipage}
    \begin{minipage}[t]{0.5\textwidth}
      \begin{tikzpicture}
      \tikzstyle{arr} = [->,shorten <=1pt,>=stealth',semithick]
        \node[draw, rectangle] (A) at (0, 0) {int $i$};
        \node[draw, rectangle] (B) at (0, -1.2) {scanf("\%d", \&$i$)};
        \node[draw, rectangle] (C) at (0, -2.4) {if $i$ \% 2 == 0};
        \node[draw, rectangle] (D) at (-1.5, -3.6) {puts("even")};
        \node[draw, rectangle] (E) at (1.5, -3.6) {puts("odd")};
        \node[draw, rectangle] (F) at (0, -4.8) {puts("exit")};
        \draw[arr] (A) -- (B);
        \draw[arr] (B) -- (C);
        \draw[arr] (C) -- (D);
        \draw[arr] (C) -- (E);
        \draw[arr] (D) -- (F);
        \draw[arr] (E) -- (F);
      \end{tikzpicture}
    \end{minipage}
    \caption{Program in C language and its control flow graph}
    \label{fig:programCFG}
  \end{figure}
#+END_LaTeX

A\nbsp{} /program/ is a sequence of elementary instructions.  Program's
structure is reflected in its /control flow graph/. Formally, /control
flow graph/ (CFG for short\index{CFG}) of a program $\mathcal P$ is a
graph $G = (V, E)$, where $V$ is a finite set of vertices and $E
\subseteq V \times V$ is a set of edges. Each instruction of $\mathcal P$
is represented by a vertex. If there exists a run of the program
$\mathcal P$ where instruction $I_2$ is executed immediately after
instruction $I_1$, then $(I_1, I_2) \in E$. We ignore labels on branches,
as they are not needed for reaching definitions
analysis.
A /path/ in a CFG $(V, E)$ is a sequence $v_1, v_2, v_3, \cdots, v_n$ such, that:

- $v_1,v_2, v_3, \cdots, v_n \in V$, where $n \in \mathbb N$
- $\forall 1 \le i < n: (v_i, v_{i+1}) \in E$

Figure\nbsp{}\ref{fig:programCFG} shows a simple program in C
language and its control flow graph.

# TODO explain why this is here
Let $(V, E)$ be a CFG. $x \in V$ /dominates/ $y \in V$ if and only if $x \in
V$ is on every path from the entry node to $y$. If $x$ dominates $y$
and $x \ne y$, then $x$ is a /strict dominator/ of $y$. The closest
strict dominator of $y$ is the /immediate dominator/ of $y$ on any
path from entry node to $y$ in CFG. Dominator tree is a graph $(V,
E_d)$, where vertices are from CFG and $(x, y) \in E_d$ if and only if $x$
is immediate dominator of $y$.

Executable programs that are not libraries have an entry point. In C
programs, this is a function called =main=. The first CFG node of the
main function is referred to as /entry node/.

Programs are commonly divided into /basic blocks/. A basic block is a
maximum sequence of elementary instruction that does not contain a
jump instruction. Every instruction of the program is part of some
basic block.

A\nbsp{} /variable/ is a fixed-size storage cell in memory. A\nbsp{}
/definition/ of a variable is any instruction that 
modifies its value. A\nbsp{} /use/ of a variable is any instruction
that reads its value.

** DONE Static single assignment form
Programs may be transformed without changing their behaviour. One of
transformations that do not change program's behaviour is
transformation to Static Single Assignment form (or SSA for
short)\nbsp{}\cite{CytronSSA}. The transformation itself yields some useful data about the
program and the SSA form is particularly useful for compilers and code
analyzers.

#+BEGIN_LaTeX
    \begin{figure}[]
    \begin{minipage}[t]{0.5\textwidth}
      \begin{lstlisting}[language=C]
        int $i$ = 1;
        int $j$ = 1;
        $i$ = $i$ + $j$;
        $j$ = $j$ + $i$;
        foo($i$, $j$);
      \end{lstlisting}
    \end{minipage}
    \begin{minipage}[t]{0.5\textwidth}
      \begin{lstlisting}[language=C]
      int $i_1$ = 1;
      int $j_1$ = 1;
      $i_2$ = $i_1$ + $j_1$;
      $j_2$ = $j_1$ + $i_2$;
      foo($i_2$, $j_2$);
      \end{lstlisting}
    \end{minipage}
    \caption{Program and its SSA form}
    \label{fig:programSSA}
    \end{figure}
#+END_LaTeX

A program $\mathcal P$ is in /Static Single Assignment form/ if, and
only if for each variable in $\mathcal P$, there is exactly one
definition\nbsp{}\cite{RosenGVNRC}. Figure\nbsp{}\ref{fig:programSSA} shows a simple program and its
SSA form.

#+BEGIN_LaTeX
  \begin{figure}
      \begin{lstlisting}[language=C]
  int $i$ = 0; $\encircle{1}$
  while ($i$ < 10) {
      printf("%d\n", $i$); $\encircle{2}$
      $i$++;  $\encircle{3}$
  }
      \end{lstlisting}

    \caption{Simple C program with loops}
    \label{fig:loop1}
    \end{figure}
#+END_LaTeX

Constructing the SSA form is a little more interesting in case the CFG of
a program contains loops. Consider program in Figure\nbsp{}\ref{fig:loop1}.

\noindent While constructing SSA form of this program, the use of $i$ variable
at location \encircle{2} could be replaced by the assignment to $i$ at location
\encircle{1} or \encircle{3}. The problem is, that both of these statements contribute
to the value of $i$ at location \encircle{2}. It is, therefore, necessary to
use some kind of combination of values from \encircle{1} and \encircle{3}. This is what
a \phi function is for. $i_3 = \phi(i_1, i_2)$ denotes, that the value
of $i_3$ could be either $i_1$ or $i_2$. After transforming the program from figure\nbsp{}\ref{fig:loop1}
to SSA form, it looks as shown in figure\nbsp{}\ref{fig:loop2}.

#+BEGIN_LaTeX
  \begin{figure}[h]
    \begin{lstlisting}[language=C]
      int $i_1$ = 0;
      int $i_2$;
      int $i_3$;

      while ($i_2 = \phi(i_1, i_3), i_2 < 10$) {
        printf("%d\n", $i_2$);
        $i_3$ = $i_2$ + 1;
      }
    \end{lstlisting}
\caption{SSA form of the program from figure~\ref{fig:loop1}}
\label{fig:loop2}
  \end{figure}
#+END_LaTeX

* TODO Reaching Definitions Analysis
\label{ch:RDA}
This chapter starts by explaining what a reaching definition is and
demonstrating the simplest naive algorithm for computing reaching
definitions. It continues by discussing properties of reaching
definitions analyses and introduces various algorithms to compute
reaching definitions.

\label{def:RD}Let $\mathcal P$ be a program. A /reaching definition/
\index{RD} of variable $\mathcal V$ used by instruction $I_1$ is an
instruction $I_2$ such, that:
+ $I_1, I_2$ are part of $\mathcal P$
+ $I_1$ is a use of variable $\mathcal V$
+ $I_2$ is a definition of variable $\mathcal V$
+ there exists a run of $\mathcal P$ where the value of $\mathcal V$ was not
  overwritten by any instruction on path from $I_2$ to $I_1$ in the CFG

#+BEGIN_LaTeX
  \begin{figure}[h]
    \begin{minipage}[b]{0.5\textwidth}
      \begin{lstlisting}[language=C]
        int i = 5;
        int j = 4;
        
        if (i == 0) {
          j = 1;
        } else if (i == 2) {
          j = 3;
        }
        printf("%d", j);
      \end{lstlisting}
    \end{minipage}
    \begin{minipage}[t]{0.5\textwidth}
      \begin{tikzpicture}

        \tikzstyle{arr} = [->,shorten <=1pt,>=stealth',semithick];
        \tikzstyle{rd} = [->,shorten <=1pt,>=stealth',dashed];

        \node[draw, rectangle] (declI) at (0, 0) {int $i = 5$};
        \node[draw, rectangle] (declJ) at (0, -1.2) {int $j = 4$};

        \node[draw, rectangle] (C) at (0, -2.4) {if $i$ == 0};
        \node[draw, rectangle] (D) at (-2.0, -3.6) { j = 1 };
        \node[draw, rectangle] (E) at (1.5, -3.6) { if $i == 2$ };
        \node[draw, rectangle] (F) at (0, -4.8) { $j = 3$ };
        \node[draw, rectangle] (G) at (-1.0, -6) { printf("\%d", $j$ ) };

        \draw [arr] (declI) -- (declJ);
        \draw [arr] (declJ) -- (C);
        \draw [arr] (C) -- (D);
        \draw [arr] (C) -- (E);
        \draw [arr] (D) -- (G);
        \draw [arr] (E) -- (F);
        \draw [arr] (F) -- (G);
        \draw [arr] (E.south) to [out=-90,in=0] (G.east);
        \draw [rd]  (C.west) to [out=150,in=180] (declI.west);
        \draw [rd]  (E.east) to [out=0,in=0] (declI.east);
        \draw [rd] (G.north) to [out=90,in=0] (D.east);
        \draw [rd] (G.east) to [out=0,in=0] (F.east);
        \draw [rd] (G.east) to [out=0,in=0] (declJ.east);
      \end{tikzpicture}
    \end{minipage}
    \caption{Program in C language, its CFG and reaching definitions. Solid edges are part of CFG, dashed edges represent reaching definitions.}
    \label{fig:programRD}
  \end{figure}
#+END_LaTeX

Figure \ref{fig:programRD} shows program and its CFG with reaching
definitions.

** TODO Dense reaching definitions analysis
\label{denseRDA} One of the ways to compute reaching definitions is
to ``follow'' the control flow graph of the program while remembering
the last definition for each variable for each of its vertices. This
is a traditional approach used by many tools.

Literature commonly uses $KILL$ and $GEN$ sets to describe what the
RDA does. For every vertex $x$ of control flow graph, $GEN(x)$ is set
of variables for which $x$ is a definition. $KILL(x) \subseteq GEN(x)$
is set of variables that are overwritten in this vertex.


Figure \ref{fig:denseRDA} demonstrates the algorithm.

#+BEGIN_LaTeX
  \begin{figure}[H]
    \begin{algorithm}[H]
      \SetAlgoLined
      \KwData{Control Flow Graph as $(V, E)$, for every $v \in V$, $v.defs$ is a set of variables defined in $v$}
      \KwResult{for every $v \in V$ and every variable $x$, $v.rd(x)$ is a set of reaching definitions for variable $x$ in $v$}
      
      \For{$v \in V$} {
        \For{$def(x) \in v.defs$} {
          $v.rd(x) \gets v.rd(x) \cup \{ v \}$ \;
        }
      }
      \While{\text{not fixpoint}} {
        \For{$v \in V$ in DFS order} {
          \For{$(u, v) \in E$} {
            \For{$def(x) \in u.defs$} {
              $v.rd(x) \gets v.rd(x) \cup \{ u \}$ \;
            }
          }
        }
      }
    \end{algorithm}
    \caption{Dense reaching definitions analysis algorithm}
    \label{fig:denseRDA}
  \end{figure}
#+END_LaTeX

The algorithm starts by adding reaching definitions to CFG nodes that
are definitions. Then, the reaching definitions are propagated
throughout the entire CFG of the program until fixpoint is
reached.

** DONE Properties of reaching definitions analyses
It is impossible for reaching definitions analyses to find precise
definitions of a specified ``variable''. Because of that, it is
necessary to perform an abstraction 

Reaching definitions analyses have some properties\nbsp{}\cite{rptRDA} that
affect their accuracy. Less accurate analyses need to make some
conservative assumptions about the program in order to be
correct. This sub-chapter describes three properties of reaching
definitions analyses: instance-wiseness, field sensitivity and ability
to recognize execution patterns.

*** DONE Instance-wise and statement-wise analysis
When analyzing programs with a cyclic CFG, there are multiple
/instances/ of instructions that can be executed repeatedly. Each
execution of an instruction creates a new instance of the instruction.

Along with the definition, use and variable, an instance-wise reaching
definitions analysis is able to tell which instance of the
instructions are involved. The information about instance might
involve for example the for loop indexing variable $i$. There might be
more variables in case the instruction is inside of a nested loop.

#+BEGIN_LaTeX
  \begin{figure}
    \begin{lstlisting}[language=C]
      int $a$ = 0; $\encircle{1}$

      for(int $i$ = 0; $i$ < 5; ++$i$) {
        int $b$ = $a$ + $i$; $\encircle{2}$
        $a$ = $b$; $\encircle{3}$
      }
      \end{lstlisting}
      \caption{Demonstration of differences between statement-wise and instance-wise analysis}
      \label{fig:instWise}
      \end{figure}
#+END_LaTeX

Differences between instance-wise analysis and statement-wise analysis
will be demonstrated on a simple program in figure
\ref{fig:instWise}. Reaching definitions for $a$ at location
\encircle{2} are \encircle{1} and \encircle{3}. However, there are
multiple instances of instructions at \encircle{2} and
\encircle{3}. Firstly, both instance-wise and statement-wise analyses
would report, that \encircle{1} is a reaching definition of $a$ at
\encircle{2}. The difference is, how much information the analysis is
able to provide about the reaching definition \encircle{3} at
\encircle{2}. Statement-wise analysis would simply state, that
\encircle{3} is a reaching definition of $a$ at
\encircle{2}. Instance-wise analysis goes a little further by
reporting, that $\encircle{3}^{i+1}$ is a reaching definition of $a$ at
$\encircle{2}^i$. The upper index denotes the index of iteration.

*** DONE Field sensitivity
Usage of aggregated data structures, such as arrays or C language
=struct=-s introduces another issue that needs to be addressed by a
reaching definitions analysis. Precision of analysis for programs that
use aggregated data structures depends on whether the analysis can
distinguish between individual elements of the data structure.

#+BEGIN_LaTeX
  \begin{figure}
    \begin{lstlisting}[language=C]
      int $a$[5];
      $a$[0] = 1; $\encircle{1}$
      $a$[1] = 2; $\encircle{2}$
      foo($a$[2]); $\encircle{3}$
    \end{lstlisting}
    \caption{Demonstration of field-sensitive reaching definitions analysis}
    \label{fig:rdaFS}
    \end{figure}
#+END_LaTeX

Consider the program in Figure\nbsp{}\ref{fig:rdaFS}. Locations \encircle{1}
and \encircle{2} in the program define the first and the second
element of $a$. After that, location \encircle{3} contains a function
call that uses the third element of the array. This element has no
definitions in the program, so an accurate reaching definitions should
find no definitions for it.

A field-sensitive analysis considers array indices and correctly
reports no reaching definitions for $a[2]$ at location \encircle{3}.

A field-insensitive analysis ignores indices of the array and for
location \encircle{3}, it would report, that reaching definitions of
$a[2]$ are \encircle{1} and \encircle{2}. This is an
over-approximation that has to be performed by the field-insensitive
analysis.

*** DONE Execution patterns recognition

Reaching definitions analysis is often not the only analysis that is
part of a program analysis framework. More often than not, there are
more analyses that derive various properties of program or its
parts. Reaching definitions analysis can sometimes take advantage of
results of previously ran analyses and achieve better accuracy or
speed.

Consider the program in figure\nbsp{}\ref{fig:execPatterns}.

#+BEGIN_LaTeX
  \begin{figure}
    \begin{lstlisting}[language=C]
      int foo(int $a$) {
        int $c$ = 0;
        if ($a$ < 0) {
          $c$ = 1; $\encircle{1}$
        }
        if (a >= 0) {
          $c$ = 2; $\encircle{2}$
        }
        return $c$; $\encircle{3}$
      }
    \end{lstlisting}
    \label{fig:execPatterns}
    \caption{Demonstration of effects of execution patterns recognition on reaching definitions analysis}
  \end{figure}
#+END_LaTeX

If an external analysis reports that there is no program execution
where $a < 0$, the reaching definitions analysis could take this into
account and derive that \encircle{1} is not a reaching definition of
$c$ at \encircle{3} even despite the fact it is a definition of a
simple variable. Analysis that does not take it into account would
report that both \encircle{1} and \encircle{2} are reaching
definitions of $c$ at \encircle{3}.

In this case, an analysis that does not recognize execution patterns
yields an over-approximation, which is not a problem.
** TODO Analyzing programs that use pointers
One of the most important features of programming languages are
pointers. They can be utilized to implement dynamic data structures,
which are very widely used. As pointers make it possible to create
variables that refer to variables, they inherently make programs more
difficult to understand and analyze. In order to compute reaching
definitions in programs that use pointers, an RDA may use information
from pointer analysis which took place prior to the RDA.

*** DONE Pointer analysis
Pointer analysis\nbsp{}\cite{ChalupaPTA} is, similarly to reaching
definitions analysis, a static program analysis. It computes a set
$\mathcal V$ of variables for each pointer $p$. This set will be
referred to as /points-to/ set. If $p$ may point to some variable $v$,
then $v \in \mathcal V$.

Reaching definitions analysis uses these data from pointer analysis to
recognize possible uses and definitions of variables. Accuracy of the
reaching definitions analysis, therefore, depends on accuracy of the
underlying pointer analysis. Namely, when the pointer analysis
performs an over-approximation, so will the reaching definitions
analysis.

*** TODO Strong and weak definitions
\label{strongWeakUpdate}

Each definition of a variable can be either /strong/ or /weak/. Strong
definition over-writes the variable with a new value. When a strong
definition is encountered, it invalidates all previous definitions of
the variable. Weak definition, on the other hand, does not necessarily
over-write the variable, so it does not invalidate previous
definitions. 

** DONE Demand-driven reaching definitions analysis
Apart from the dense algorithm, several other algorithms to compute
reaching definitions have been introduced. Other algorithms are
generally based on traversing the CFG of a program and processing only
definitions and uses of variables. They also attempt to eliminate need
to use fixpoint in the computation. This subchapter briefly introduces
demand-driven reaching definitions analysis\nbsp{}\cite{SootDDRDA}.

The main idea of this approach is to answer the question ``can a
definition $d$ of variable $v$ reach a program point $p$?''. This
question is referred to as /query/ and it is represented by a triple
$(d, p, v)$. After a query is generated, it is propagated backwards
along nodes of the CFG. Each node may either answer the query or
continue the propagation to its predecessors. If a node $x$ contains a
definition of $v$, the query propagation stops. The answer is yes, if
and only if $x = d$. If $x \ne d$, then node $x$ kills the
definition $d$ before it can reach $p$ along the path.

In case a program point $p$ has $n$ predecessors, it is sufficient
that the reachability of $d$ is reported by at least one of them.

It is worth noting, that this approach has a special property that
makes it suitable for a slicer: It is able to start from the slicing
criterion and gradually find all definitions that affect the
criterion. This way, it can avoid computing irrelevant information.

** TODO Sparse dataflow analysis
Another approach to computing reaching definitions was introduced by
Madsen and M\o{}ller \cite{MadsenSDAPR}. This approach requires
pre-computing dominator tree\nbsp{}\cite{CytronSSA} for nodes of the
CFG. Before going more in depth, it is necessary to define dominator
tree and related relations.

When the algorithm encounters a use of a variable, it searches
dominator tree of the program backwards until it finds a definition of
the same variable. The triple $(d, v, u)$ where $d$ is a definition of
a variable $v$ and $u$ is a use of $v$, is then added to $DU$ set.

When a new definition $d_n$ of variable $v$ is encountered, the
algorithm finds a set $\mathcal D_p$ of previous definitions of
$v$. Then, for each $d_p \in \mathcal D_p$ where $d_n$ is a strict
dominator of $d_p$, all triples $(d_p, v, u) \in DU$ are removed from
$DU$.

While processing definitions and uses, the algorithm places \phi nodes
for variables when necessary. As a side-effect, SSA form of the
program is produced.

- SSA based
- fixpoint computation
** TODO Algorithms based on static single assignment form
\label{SSArd} Algorithms that transform a program into SSA form
replace modified variables in assignments by new, artificially-created
variables representing a new ``version'' of the variable. They also
replace variables in uses by the most recent definition -- reaching
definition. In a program that is already transformed into SSA form, it
is possible

# TODO maybe, define a simple framework for these algorithms
# so they can be plugged in to the final reaching definitions stage

# TODO program, SSA form, reaching definitions

For the purpose of this thesis, we have studied two algorithms for
computing SSA form. One of them has been introduced by Cytron et
al\nbsp{}\cite{CytronSSA}.  The second algorithm, invented by Braun et
al\nbsp{}\cite{BraunSSA}, is simpler and has been experimentally proven to
be slightly more efficient\nbsp{}\cite{BraunSSA}.
*** TODO Cytron et al algorithm

Algorithm introduced by Cytron et al.\nbsp{}\cite{CytronSSA} uses dominance
information to pre-calculate locations of \phi nodes. In the later
phase, variables are numbered using a simple stack of counters and \phi
nodes are filled with operands.

This approach was proven to produce minimal SSA form.

*** TODO Braun et al algorithm
\label{marker}

Algorithm by Braun et al.\nbsp{}\cite{BraunSSA} operates in two phases:
local value numbering and global value numbering. Both of these phases
process basic blocks of the program in the execution order.

During *local value numbering*, it computes SSA form of every basic
block of the program. For every basic block, it iterates through all
instructions in execution order. If an instruction $I$ defines some
variable $\mathcal V$, $I$ is remembered as the current definition of
$\mathcal V$. If an instruction $I$ uses some variable $\mathcal V$,
the algorithm looks up its definition. If there is a current
definition $\mathcal D$, the use of variable $\mathcal V$ is replaced
by use of the numbered variable that corresponds to $\mathcal D$.

*Global value numbering* is involved once no definition for the
specified variable can be found in the current basic block. The
algorithm places a \phi node on top of the current basic block and starts
recursively searching the CFG for the latest definition in all
predecessors of the current basic block. Once a definition is found,
it is added as an operand to the \phi node.

When looking up a definition of a variable from a predecessor basic
block, the basic block might not be processed by global value
numbering. If that is the case, the algorithm does not have any idea
about which variables are defined in that basic block. This happens
when the program's CFG is cyclic -- e.g. recursive function is called
or for loop is used. Because of that, the algorithm remembers the last
definition of variable in basic blocks during local value
numbering. If there is no last definition in a block, the lookup
continues to all predecessors recursively.

Along with reaching definitions algorithm, Braun et al. present a way
to reduce the number of added \phi nodes, which allows their algorithm to
produce minimal SSA form.

* DONE Symbiotic
\label{ch:Symbiotic} \sbt{} is a modular tool for formal verification
of programs working on top of the LLVM compiler
infrastructure\nbsp{}\cite{LLVM}. It is being developed at Faculty of
Informatics, Masaryk University. \sbt{} works by combining three
well-known techniques:

1. *Instrumentation* is responsible for inserting various error checks
   into the program. For example, when checking memory access errors,
   instrumentation is responsible for adding the allocated memory
   along with allocation size into a global data structure. When
   dereferencing a pointer, instrumentation inserts a check to verify
   whether this pointer is inside allocated bounds or not. An
   assertion that crashes the program if a dereference is out of
   bounds of allocated memory is inserted, too.
2. *Slicing*\nbsp{}\cite{ChalupaDG} is a technique that reduces the size of
   the program by removing parts that do not influence its behaviour
   with respect to a specified /slicing criterion/. Slicing criterion is an =assert=
   instruction. The slicer computes which instructions the slicing
   criterion is dependent on. For that, it uses results of reaching
   definitions analysis.
3. *Symbolic execution* is the last step. It is a technique that
   decides whether the program could violate a condition of some
   assertion in the program. Rather than requiring user input, it uses
   so-called symbolic values. Whenever there is a program branching
   based on the symbolic value, the symbolic virtual machine remembers
   a constraint of the value based on the branching condition. When an
   erroneous state is reached, the symbolic virtual machine reports
   the path in the program that leads to the error.

* TODO Implementation
\label{ch:Implementation} This chapter starts by DG
library and the LLVM infrastructure. The
introduction is followed by an in-depth discussion of the new reaching
definitions analysis implementation.

** TODO DG Library
The slicer used in \sbt{} uses the DG library\nbsp{}\cite{ChalupaDG} to
calculate dependence graph and slice away unnecessary parts of
verified program. New reaching definitions analysis has been
implemented to the DG library, so it can be used with any software
that uses DG.

Before processing any program, DG loads the program into its own
framework. Analyses that are independent of the program
representation, because they only use DG framework which handles the
details. DG currently supports only LLVM intermediate representation.

*** TODO LLVM

LLVM\nbsp{}\cite{LLVM} is an infrastructure for compilers and
optimization. It consists of multiple libraries and tools. One of the
tools is clang -- a compiler of C language. Clang is especially
useful, because it compiles C programs to the LLVM intermediate
representation, which DG can work with.

LLVM defines its own intermediate representation of a program. It
looks very similar to assembler. Despite being low-level, it is not
dependent on any particular processor type.

- define important instructions
- example: C program, LLVM IR
#+BEGIN_LaTeX
  \begin{figure}
    \begin{minipage}[t]{0.5\textwidth}
      \begin{lstlisting}[language=C]
      \end{lstlisting}
    \end{minipage}
    \begin{minipage}[t]{0.5\textwidth}
      \begin{lstlisting}[language=LLVM]
      \end{lstlisting}
    \end{minipage}
  \end{figure}
#+END_LaTeX

\label{partialSSA}
Partial SSA form means, that there is at most one definition for each
register. This form of program, however, makes no guarantees about
address-taken variables. Those are *not* in SSA form.
# TODO some figure with partial SSA form

Thanks to the partial SSA transformation, LLVM already provides
 reaching definitions information for its register variables.

*** DONE Pointer analysis in DG
The new reaching definitions analysis requires information from a
pointer analysis. DG already contains a pointer analysis, which can be
utilized. However, there are two important implementation details that
need to be adressed by any RDA that uses results of this pointer
analysis.

In some cases, the pointer analysis is unable to determine which
variables to pointer points to. It happens for example in case the
pointer is returned from a function from an external library that is
not part of the program. The pointer analysis returns that the pointer
points to a virtual node called ``unknown memory''. This has to be
addressed later in the reaching definitions analysis.


The pointer analysis in DG is field-sensitive, which opens a
possibility to implement a field-sensitive RDA as well. There are
multiple approaches to addressing field-sensitivity. One of them
involves considering each element of an aggregated data structure as a
separate variable. The pointer analysis in DG uses another approach:
it reports which memory object is being accessed and what part of the
object is being accessed. The part of the object is specified by a
pair $(offset, length)$, where both $offset$ and $length$ are in
bytes. In some cases, the $offset$ can be unknown.
# For example, when an array is accesed using an indexing variable with value unknown at compile time.

*** DONE Reaching definitions analysis framework in the DG library
DG uses reaching definitions analysis to calculate control
dependencies between instructions. The original reaching definitions
analysis in DG uses the dense approach, as described in
\ref{denseRDA}.

Prior to the reaching definitions analysis itself, DG builds a
subgraph of program's control flow graph\index{CFG}. The subgraph does
not contain all types of instructions. Rather, it consists only of
store instructions, call instructions, return instructions and all
memory allocations. In spite of not containing all instructions, it
reflects structure of the program. Each instruction in the subgraph
that defines some memory object already has an associated points-to
information from pointer analysis. Thanks to this, it is possible to
tell which variables are strongly or weakly defined in a particular
CFG node.

** TODO Reaching definitions analysis implementation approach
The new reaching definitions analysis is implemented in the DG
library. This chapter describes how the new reaching definitions
analysis has been implemented in the existing framework.

Thanks to LLVM's transformation to partial SSA form (as described in
\ref{partialSSA}), there is no need to take care of LLVM register
variables, as they are already taken care of while translating the C
program into LLVM Intermediate Representation (LLVM IR). Therefore,
the implementation focuses on address-taken variables.

*** DONE Subgraph builder abstractions
As there are some modifications done to the subgraph builder, the
first step towards the implementation is to introduce an abstraction
for reaching definitions subgraph builder. The abstraction allows the
legacy subgraph builder to be preserved, while a new one can be added,
too.

The goal is to allow the user of =ReachingDefinitions= class to run
any reaching definitions analysis they would choose. The pointer
analysis framework in the DG library already allows the user to
specify pointer analysis to run using templates. Similar approach was
taken here with the reaching definitions analysis.

Each reaching definitions analysis in the DG library could require
different set of information in the reaching definitions
subgraph. With that in mind, we have decided to allow each RDA to use
different subgraph builder. A subgraph builder builds a reaching
definitions subgraph from some representation. Therefore, we have
designed and implemented an interface for subgraph builders from the
LLVM IR called =LLVMRDBuilder=. This interface allows us to implement
a =build= function, that returns the entry node of the reaching
definitions subgraph. The implementation of the new subgraph builder
is very similar to the original implementation, with two major
differences. The new subgraph builder splits up LLVM basic blocks when
a function call is encountered and it also adds information about
which memory is used in which CFG node. These additions are discussed
in the following two sections.

*** TODO Splitting basic blocks on function calls
As the Marker algorithm remembers last definitions for the current
basic block, it is crucial to handle basic blocks correctly. The basic
block used by LLVM IR is more or less the same, with an important
difference: When a function is called, the call instruction does not
end a LLVM IR basic block. This is against the definition of a basic
block introduced in\nbsp{}\ref{ch:KTPA}, as a call instruction is a jump to
a different address.

# TODO figure with this block:
#+BEGIN_SRC prog
  store 1, a
  call foo(&a)
  store 2, a
#+END_SRC

Consider the program ... The block calling the function would be
processed first and =foo= would then see the =store 2, a= instruction
as a reaching definition of =a=. This is, however, not correct as the
instruction has not been executed yet. Because of that, we have
decided to split up an LLVM IR basic block with every call statement,
too.

*** DONE Adding use information to control flow graph

Now, the subgraph builder can add information about pointer
dereferences -- that is, LLVM =load= instructions to the reaching
definitions subgraph. Pointer analysis is utilized here to find out
which variables are being used. As one pointer could simply point to
multiple variables, it is necessary to add information about all
variables that could potentially be used.

In the subgraph builder used with the new analysis,
=LLVMRDBuilderSemisparse=, we have instructed the subgraph builder to
include LLVM's load nodes. For each load node, it then queries the
pointer analysis for all variables its dereferenced pointer operator
could point to. For looking up the variables, it uses a
newly-introduced method =getPointsTo=, which fetches the information
from the pointer analysis.

The =load= instruction could possibly use a smaller portion of the
memory than the allocation size. This is the case when accessing an
individual element of a larger data structure. A field-sensitive
reaching definitions analysis requires the length to be set to the
length that is being used. This is done by determining size of the
type the value is being loaded to.

*** DONE Treating unknown memory
Sometimes, pointer analysis was unable to tell where a pointer may
point, so the analysis has to make some conservative assumptions about
the program in order to be correct. In this case, the analysis assumes
that such pointer could point to any variable and treats the CFG node
as if it was a definition or a use of all variables in the
program. Whether it is a definition or a use is decided based on
semantics of the instructions and how the pointer is used.

After the subgraph is built, it is searched by a separate class
=AssignmentFinder=, which does exactly what was explained above. It
uses a two-phase algorithm to do that: In the first phase, all
variables in the program are added to a list. In the second phase,
every store to an unknown pointer and load from an unknown pointer
turn into weak definition of all variables in the program or use of
all variables in the program, respectively. Doing this removes some
complex handling of unknown pointers from the next phase of the
analysis.

*** TODO Field-sensitivity
\label{chap:intervals} The Marker algorithm itself does not consider
aggregate data structures. In order to support analyzing them, it
needs to be modified. Each definition or use of a variable have an
associated interval of affected bytes. This interval is later used to
look up reaching definitions of a variable. An interval has a start
and a length. Intervals act as an abstraction on top of DG's
=DefSite=-s. They make it possible to calculate an intersection or
union.

The first intermediate data structure that is part of this framework
is =DisjointIntervalSet=. The set allows to insert intervals while
maintaining an invariant, that all intervals inside are disjoint. When
inserting an interval that has a non-empty intersection with some of
the intervals inside, the set ensures that these two intervals are
united into a single interval.

=IntervalMap= is the second important data structure of the
framework. It provides functionality that makes the analysis
field-sensitive. IntervalMap on the first sight looks similarly to
=std::map= available in C++. It allows to save arbitrary types under
=Interval= keys. The difference is in the lookup
functions. =IntervalMap= offers 3 main functions: =collect=,
=collectAll= and =killOverlapping=.

The =collect= function is designed to work with strong updates. It
searches the entries backwards, starting by the last entry added. 
It collects all values from the interval map such, that the specified
interval is covered by union of key intervals of the values returned.

=collectAll= works with weak updates. As opposed to =collect=, it
does not stop when the specified interval is subset of union of the
result key intervals. Rather, it searches the whole IntervalMap and
returns all values which are saved under intervals that overlap with
the specified interval.

=killOverlapping= deletes definitions with intervals that overlap with
specified interval. After =killOverlapping=, calling =collectAll= with the
same interval or any of its subsets returns an empty result.
*** TODO Treating unknown offset
When the pointer analysis returns an unknown offset of variable for a
definition or a use, the RDA needs to address it somehow. In case
there is a definition of an unknown offset of a variable, it could be
definition of any of its bytes, so the analysis has to perform an
over-approximation. In the over-approximation, it assumes that the
whole variable is defined. 

#+BEGIN_SRC c
  int i, j;
  int a[10];
  a[i] = 0;
  a[j] = 1;
  printf("%d", a[0]); \encircle{3}
#+END_SRC

Assuming =i= and =j= are unknown values, both of those definitions
could reach the =printf= at \encircle{3}. Thus, they have to be
assumed to be weak update regardless of the fact that =a= points to a
single memory object -- the array.

*** DONE Weak updates
The Marker algorithm maintains two main data structures for processing
the strong updates: =last_def=, =current_def=. To incorporate weak
updates, they have been duplicated with names =last_weak_def= and
=current_weak_def=.

=last_weak_def= is used during local value numbering to remember the
last weak definition in a block. When a strong definition is
encountered, overlapping weak definitions are either killed or have
their definition intervals shrank. The =killOverlapping= function of
=IntervalMap= introduced in\nbsp{}\ref{chap:intervals} is used for that.

When a strong definition is encountered during global variable
numbering, current weak definitions that overlap with the strong
definition must be killed.

#+BEGIN_SRC cpp
  current_weak_def[var.target][block].killOverlapping(interval);
#+END_SRC

Encountering a weak update involves simply adding it to =current_def=
in global value numbering, or =last_def= in local value numbering.

*** TODO Strong updates
- strong updates stop the lookup
*** DONE Computing reaching definitions from Marker algorithm
The Marker algorithm is capable of computing SSA form of a
program. The program, however, does not need to be transformed into
SSA form. The only task of an RDA is to obtain reaching
definitions. We have decided to split up the computation into two
phases:
1. In the first phase, the implementation constructs a sparse RD
   graph. Sparse RD graph is a graph, where for every reaching
   definition $(I_1, I_2)$ exists a path from $I_1$ to $I_2$. The path may
   consist of multiple \phi nodes, but it might be trivial as well. The
   construction is fairly straightforward: whenever a variable use $u$
   is encountered, lookup all definitions of the variable using
   =readVariable=. Then, for every definition $d$ of the variable, add
   an edge $(u, d)$ to the sparse RD graph.
2. In the second phase, the control flow graph $(V, E)$ of the program
   is traversed once again. For every use $u \in V$, a BFS search of the
   sparse RD graph is started in $u$. If the definition found is not a
   \phi node, it is added as a reaching definition.

* TODO Experimental evaluation of the new analysis
\label{ch:Experiment} Using sv-comp benchmarks for comparisons.
** TODO Time
The goal of this thesis is to design and implement a *faster* reaching
definitions analysis. This subchapter compares the new and the
original implementation in terms of speed. 

In order to evaluate time, we have run the same set of benchmarks with
the same version of \sbt{} twice. The only difference between the two
runs was the reaching definitions analysis used.

** TODO Memory Used
During the experiments, there have been cases where the new
implementation ran out of memory faster than the original one. There
are two possible causes for this:
1. Unknown memory treatment consumes too much memory.
2. Intervals framework used for field-sensitivity consumes too much
   memory. Field-insensitive analysis would certainly use less memory.

** TODO Accuracy
Thanks to the ``interval framework'' introduced
in\nbsp{}\ref{chap:intervals}, the new implementation of semi-sparse
analysis is more accurate than the original implementation. Consider
this program:

  #+BEGIN_SRC c
    int a[] = {0, 1, 2, 3}; // A
    a[0] = 5; // B
    a[1] = 6; // C
    a[2] = 7; // D
    a[3] = 8; // E

    for (size_t i = 0; i < 4; ++i) {
        printf("%d\n", a[i]); // RD(a) = ???
    }
  #+END_SRC
  - the original analysis reports $RD(a) = \{ A, B, C, D, E\}$
  - the new analysis is able to tell that $\{B,C,D,E\}$ together
    over-write the whole range of =a= and therefore reports $RD(a) =
    \{B,C,D,E\}$

* TODO Conclusion
\label{ch:Summary}

** TODO Possible future work

It is possible to further speed up computation of Reaching Definitions
by incorporating the trivial phi node removal
algorithm\nbsp{}\cite{BraunSSA}. The sparse graph contains many redundant \phi
functions that could be removed to speed up the final phase of
reaching definitions propagation.

As the algorithm is implemented in a slicer, could be improved even
further by starting at the slicing criterion and searching the CFG
backwards only for definitions of variables that affect the slicing
criterion, which is what the slicer needs to derive the control
dependencies.

The =IntervalMap= data structure used in MarkerFS builder could be
improved.

The reaching definitions analysis could benefit from additional
accuracy it could gain by considering different instances of
statements.

Newer versions of LLVM support a pass called
mem2reg[fn::https://llvm.org/docs/Passes.html#mem2reg-promote-memory-to-register]. This
pass is able to convert local variables into registers, which are in
SSA form. It would be interesting to use mem2reg pass whenever
possible and then run this analysis to obtain results for arrays and
other structures mem2reg is unable to handle.

Another interesting LLVM pass to test would be scalar replacement of
aggregates[fn::https://llvm.org/docs/Passes.html#sroa-scalar-replacement-of-aggregates]. This
pass replaces arrays and structures by scalar values in case it is
possible.

** DONE Summary of work done
As a part of this thesis, we studied four algorithms for computing
reaching definitions. Then, we chose to implement an SSA-based
algorithm into \sbt{}. Prior to implementation, we have designed
modifications for the algorithm to work with aggregate data structures
and weak updates. The modified algorithm has been implemented into
\sbt{}. The new implementation is then compared with the original
implementation in terms of accuracy, time and memory used.

\printbibliography[heading=bibintoc]
