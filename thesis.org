#+TITLE: Improvements of reaching definitions analysis in Symbiotic
#+AUTHOR: Tomáš Jašek
#+LATEX_CLASS:         fithesis
#+OPTIONS:             todo:nil toc:nil
#+LATEX_CLASS_OPTIONS: [nolot,nolof,digital,twoside]
#+LATEX_HEADER:        \input{setup.tex}
#+BEGIN_SRC emacs-lisp :exports none
  (setq org-babel-inline-result-wrap "%s")
#+END_SRC

* DONE Introduction

Nowadays, computer programs control gadgets everywhere around
us. Programs are used in wide variety of fields that impose diverse
requirements on their reliability. For example, in medical
applications it is crucial that a program which controls a piece of
medical equipment does not misbehave or crash under any circumstances.

An undesired behaviour of a program might be caused by a programmer
mistake. There are already tools to detect various programmer
mistakes. However, most of the tools do not understand intents of a
human programmer. Therefore, it is still possible for the programmer
to express their intentions in an incorrect way producing a logical
error. For example, a programmer mistake might result in the program
attempting to access a part memory which is not owned by the
program. On modern operating systems, this usually causes the program
to crash.

Formal verification tools can detect a subset of logical
errors. \sbt{}\nbsp{}\cite{Symbiotic} is one of the tools. \sbt{} firstly
inserts error checks into the program, then reduces the size by
removing irrelevant parts of the program and passes it to symbolic
execution framework \textsc{klee} [fn::http://klee.github.io/] which
performs the verification itself. As formal verification is an
expensive process, it is suitable to remove parts of the program that
do not influence result of the verification process. \sbt{} uses
program slicing based on the DG library\nbsp{}\cite{ChalupaDG} for this.

One of the analyses used during slicing is reaching definitions
analysis. It identifies all instructions that may generate a value of
a variable before the place where the value is used. Computing
reaching definitions in programs without pointers is trivial. However,
computing reaching definitions for in the presence of pointers is more
complex, as the analysis needs to deal with dynamic data structures,
pointers that might point to multiple variables and arrays.

The aim of this thesis is to study modern techniques of computing
reaching definitions, implement a faster reaching definitions analysis
to\nbsp{}\sbt{} and compare the new implementation with the original one
experimentally using a non-trivial set of benchmarks.

This thesis is divided into individual chapters as follows: Chapter
\ref{ch:ProgAnalysis} introduces the reader to program analysis in
general. Key terms, such as program, control flow graph or static
single assignment form are defined there. Chapter\nbsp{}\ref{ch:RDA} starts
by defining what a reaching definition is, what is a reaching
definitions analysis and discusses the simplest naive algorithm to
compute reaching definitions in programs without pointers. Analyzing
programs with pointers is discussed next. Then, the chapter continues
by discussing various algorithms to compute reaching definitions. In
Chapter\nbsp{}\ref{ch:Symbiotic}, \sbt{} is introduced and its architecture
is explained. Chapter\nbsp{}\ref{ch:Implementation} discusses LLVM, the DG
library and the chosen implementation approach based on Marker
algorithm\nbsp{}\cite{BraunSSA}. Chapter\nbsp{}\ref{ch:Experiment}
shows experimental comparison of the original implementation of
reaching definitions analysis and the new
implementation. Implementations are compared in terms of time and
accuracy. Chapter\nbsp{}\ref{ch:Experiment} offers experimental evaluation
of the new analysis and comparison with the original
implementation. Chapter\nbsp{}\ref{ch:Summary} presents possibilities for
future work and summarizes the thesis.

* DONE Program analysis
\label{ch:ProgAnalysis}

Tools that find errors in programs use results of program analysis to
decide whether or not there is an error in the program. There are two
kinds of program analysis: dynamic analysis and static analysis. A
verification tool may use either approach or some combination of
both. The following sub-chapter briefly introduces both.

** DONE Static and dynamic program analysis

*Dynamic analysis* is based on watching behaviour of a program while
the program is running. For example, one could replace C =malloc= and
=free= functions by functions that track the allocations. This is how
valgrind[fn::http://valgrind.org/] works. Another familiar example of
a dynamic analysis is unit testing. During unit testing, the program
(or its part) is started with a user-specified input and after
finishing, its output is checked. Dynamic analysis is sufficient for
exploring individual runs of the program. However, this method is
insufficient for exploring every possible run of the program.

\noindent *Static analysis* derives program properties from some
program's representation without directly executing it. It could be
the source code, compiled machine code or some kind of intermediate
representation (such as Java bytecode or LLVM bitcode). Static
analysis often has problems balancing between performance and
accuracy. Decreasing accuracy of the analysis yields imprecise
results, while increasing accuracy also increases the complexity of
the analysis.

This thesis focuses on reaching definitions analysis, which is a
static analysis. The following sub-chapter continues with a more
in-depth introduction to the static analysis.

\clearpage
** DONE Key terms of static program analysis
\label{ch:KTPA}
#+BEGIN_LaTeX
  \begin{figure}
    \begin{minipage}[b]{0.5\textwidth}
      \begin{lstlisting}[language=C]
        int $i$;
        scanf("%d", &i);
        if ($i$ % 2 == 0)
            puts("even");
        else
            puts("odd");
        puts("exit");
      \end{lstlisting}
    \end{minipage}
    \begin{minipage}[t]{0.5\textwidth}
      \begin{tikzpicture}
      \tikzstyle{arr} = [->,shorten <=1pt,>=stealth',semithick]
        \node[draw, rectangle] (A) at (0, 0) {int $i$};
        \node[draw, rectangle] (B) at (0, -1.2) {scanf("\%d", \&$i$)};
        \node[draw, rectangle] (C) at (0, -2.4) {if $i$ \% 2 == 0};
        \node[draw, rectangle] (D) at (-1.5, -3.6) {puts("even")};
        \node[draw, rectangle] (E) at (1.5, -3.6) {puts("odd")};
        \node[draw, rectangle] (F) at (0, -4.8) {puts("exit")};
        \draw[arr] (A) -- (B);
        \draw[arr] (B) -- (C);
        \draw[arr] (C) -- (D);
        \draw[arr] (C) -- (E);
        \draw[arr] (D) -- (F);
        \draw[arr] (E) -- (F);
      \end{tikzpicture}
    \end{minipage}
    \caption{Program in C language and its control flow graph}
    \label{fig:programCFG}
  \end{figure}
#+END_LaTeX

A\nbsp{} /program/ is a sequence of elementary instructions.  Program's
structure is reflected in its /control flow graph/. Formally, /control
flow graph/ (CFG for short\index{CFG}) of a program $\mathcal P$ is a
graph $G = (V, E)$, where $V$ is a finite set of vertices and $E
\subseteq V \times V$ is a set of edges. Each instruction of $\mathcal P$
is represented by a vertex. If there exists a run of the program
$\mathcal P$ where instruction $I_2$ is executed immediately after
instruction $I_1$, then $(I_1, I_2) \in E$. We ignore labels on branches,
as they are not needed for reaching definitions analysis. Notice that
we ignore subprocedure boundaries -- i.e. CFG is interprocedural in
our case. A /path/ in a CFG $(V, E)$ is a sequence $v_1, v_2, v_3, \cdots, v_n$
such, that:

- $v_1,v_2, v_3, \cdots, v_n \in V$, where $n \in \mathbb N$
- $\forall 1 \le i < n: (v_i, v_{i+1}) \in E$

Figure\nbsp{}\ref{fig:programCFG} shows a simple program in C
language and its control flow graph.


Executable programs that are not libraries have an entry point. In C
programs, this is a function called =main=. The first CFG node of the
main function is referred to as /entry node/. We assume that every CFG
has exactly one entry node from which all other CFG nodes are
reachable.

\label{domTree} Algorithms that process CFG of a program may make use
of some relations defined on nodes of the
CFG. Dominance\nbsp{}\cite{TarjanDom} is one of the most important relations
for algorithms presented in this thesis. Let\nbsp{}$(V, E)$ be a CFG. $x \in
V$ /dominates/ $y \in V$ if and only if $x \in V$ is on every path from
the entry node to $y$. If $x$ dominates $y$ and $x \ne y$, then $x$ is a
/strict dominator/ of $y$. The closest strict dominator of $y$ is the
/immediate dominator/ of $y$ on any path from entry node to $y$ in
CFG. /Dominator tree/ is a graph $(V, E_d)$, where vertices are from
CFG and $(x, y) \in E_d$ if and only if $x$ is immediate dominator of
$y$.  If $x \in V$, then /dominance frontier/ $DF(X)$ is a set of all $y
\in V$ such that $x$ dominates a predecessor of $y$, but $x$ does not
strictly dominate $y$.

Programs are commonly divided into /basic blocks/. A basic block is a
maximum sequence of elementary instructions that does not contain a
jump instruction. Every instruction of the program is part of exactly
one basic block.

A\nbsp{} /variable/ is a fixed-size storage cell in memory. A\nbsp{}
/definition/ of a variable is any instruction that 
modifies its value. A\nbsp{} /use/ of a variable is any instruction
that reads its value.

** DONE Static single assignment form
Programs may be transformed without changing their behaviour. One of
transformations that do not change program's behaviour is
transformation to Static Single Assignment form (or SSA for
short)\nbsp{}\cite{CytronSSA}. The transformation itself yields some useful
data about the program and the SSA form is particularly useful for
compilers and code analyzers.

#+BEGIN_LaTeX
    \begin{figure}[H]
    \begin{minipage}[t]{0.5\textwidth}
      \begin{lstlisting}[language=C]
        int $i$ = 1;
        int $j$ = 1;
        $i$ = $i$ + $j$;
        $j$ = $j$ + $i$;
        foo($i$, $j$);
      \end{lstlisting}
    \end{minipage}
    \begin{minipage}[t]{0.5\textwidth}
      \begin{lstlisting}[language=C]
      int $i_1$ = 1;
      int $j_1$ = 1;
      $i_2$ = $i_1$ + $j_1$;
      $j_2$ = $j_1$ + $i_2$;
      foo($i_2$, $j_2$);
      \end{lstlisting}
    \end{minipage}
    \caption{Program and its SSA form}
    \label{fig:programSSA}
    \end{figure}
#+END_LaTeX
A program $\mathcal P$ is in /Static Single Assignment form/ if, and
only if the program contains exactly one assignment to each
variable\nbsp{}\cite{RosenGVNRC}. Figure\nbsp{}\ref{fig:programSSA} shows a simple
program and its SSA form.
\clearpage
#+BEGIN_LaTeX
  \begin{figure}
      \begin{lstlisting}[language=C]
int $i$ = 0; /\encircle{1}/
while ($i$ < 10) {
    printf("%d\n", $i$); /\encircle{2}/
    $i$++;  /\encircle{3}/
}
      \end{lstlisting}

    \caption{Simple C program with loops}
    \label{fig:loop1}
    \end{figure}
#+END_LaTeX
Constructing SSA form is a little more interesting in case the CFG of
a program contains cycles. Consider program in
Figure\nbsp{}\ref{fig:loop1}. While constructing SSA form of this program,
the use of variable $i$ at location \encircle{2} could take value of
the right side of the assignment to $i$ at location \encircle{1} or
\encircle{3}. The problem is, that both of these statements contribute
to the value of $i$ at location \encircle{2} and there must be exactly
one assignment to $i$ in SSA form. It is, therefore, necessary to use
some kind of combination of values from \encircle{1} and
\encircle{3}. This is what a \Phi function is for. $i_3 = \Phi(i_1, i_2)$
denotes, that the value of $i_3$ could be either $i_1$ or
$i_2$. Transforming the program from Figure\nbsp{}\ref{fig:loop1} to SSA form
results in the program shown in Figure\nbsp{}\ref{fig:loop2}.

#+BEGIN_LaTeX
    \begin{figure}[h]
      \begin{lstlisting}[language=C]
int $i_1$ = 0;
int $i_2$;
int $i_3$;

while ($i_2 = \Phi(i_1, i_3), i_2 < 10$) {
    printf("%d\n", $i_2$);
    $i_3$ = $i_2$ + 1;
}
      \end{lstlisting}
  \caption{SSA form of the program from figure~\ref{fig:loop1}}
  \label{fig:loop2}
    \end{figure}
#+END_LaTeX

* DONE Reaching definitions analysis
\label{ch:RDA}
This chapter starts by explaining what a reaching definition is and
demonstrates the simplest naive algorithm for computing reaching
definitions. It continues by discussing properties of reaching
definitions analyses and introduces various algorithms to compute
reaching definitions.

\label{def:RD}Let $\mathcal P$ be a program. A /reaching definition/
\index{RD} of variable $\mathcal V$ used by instruction $I_1$ is an
instruction $I_2$ such, that:
+ $I_1, I_2$ are part of $\mathcal P$
+ $I_1$ is a use of variable $\mathcal V$
+ $I_2$ is a definition of variable $\mathcal V$
+ there exists a run of $\mathcal P$ where $I_1$ is executed after $I_2$
  and there is no other instruction $I_3 \neq I_2$ that overwrites
  variable $\mathcal V$ and is executed after $I_2$ and before $I_1$

#+BEGIN_LaTeX
    \begin{figure}[hbtp]
      \begin{subfigure}{0.5\textwidth}
        \centering
        
        \begin{lstlisting}[language=C]
int i = 5; /\encircle{1}/
int j = 4; /\encircle{2}/
          
if (i == 0) {
    j = 1; /\encircle{3}/
} else if (i == 2) {
    j = 3; /\encircle{4}/
}
printf("%d", j);
        \end{lstlisting}
      \end{subfigure}
      \begin{subfigure}{0.5\textwidth}
        \centering
        \begin{tikzpicture}

          \tikzstyle{arr} = [->,shorten <=1pt,>=stealth',semithick];
          \tikzstyle{rd} = [->,shorten <=1pt,>=stealth',dashed];

          \node[draw, rectangle] (declI)               { int $i = 5$};
          \node[draw, rectangle] (declJ) [below of=declI] { int $j = 4$};

          \node[draw, rectangle] (C) [below of=declJ] { if $i$ == 0};
          \node[text]                [right of=C, right of=C, right of=C] { $i \rightarrow \{ \encircle{1} \}$};
          \node[draw, rectangle] (E) [below of=C] { if $i == 2$ };
          \node[text]                [right of=E, below of=E] { $i \rightarrow \{ \encircle{1} \}$};
          \node[draw, rectangle] (D) [right of=E, right of=E] { j = 1 };
          \node[draw, rectangle] (F) [below of=E, left of=E] { $j = 3$ };
          \node[draw, rectangle] (G) [below of=F, below of=E] { printf("\%d", $j$ ) };
          \node[text]                [below of=G] { $j \rightarrow \{ \encircle{2}, \encircle{3}, \encircle{4} \}$};

          \draw [arr] (declI) -- (declJ);
          \draw [arr] (declJ) -- (C);
          \draw [arr] (C) -| (D);
          \draw [arr] (C) -- (E);
          \draw [arr] (D) |- (G);
          \draw [arr] (E) -- (F);
          \draw [arr] (F) -- (G);
          \draw [arr] (E) -- (G);
        \end{tikzpicture}

      \end{subfigure}
      \caption{Program in C language, its CFG and reaching definitions. Solid edges are part of CFG, reaching definitions are represented by set of line numbers where the definitions are.}
      \label{fig:programRD}
    \end{figure}
#+END_LaTeX

Figure \ref{fig:programRD} shows program and its CFG with reaching
definitions.

** DONE Dense reaching definitions analysis
\label{denseRDA} 

Computing reaching definitions in program precisely is usually
infeasible. It is, however, possible if the reaching definitions
analysis (RDA) performs certain over-approximations.

One of the ways to compute reaching definitions is to ``follow'' the
control flow graph of the program while remembering the last
definition for each variable for each of its vertices. Classical
dataflow analysis\nbsp{}\cite{TonellaDenseRDA} maintains two working sets
for each $v \in V$: $IN_v$ and $OUT_v$. $IN_v$ and $OUT_v$ contain incoming
resp. outgoing reaching definitions. Both of them are mappings of
variables to CFG nodes. Figure \ref{fig:denseRDA} demonstrates the
algorithm.

In order to translate the semantics of instructions into something an
RDA can understand, $KILL$ and $GEN$ sets are commonly used. Let $(V,
E)$ be a CFG. For every $x \in V$, $GEN_x$ is set of variables for which
$x$ is a definition. $KILL_x \subseteq GEN_x$ is set of variables that
are overwritten in this vertex. The $KILL$ and $GEN$ sets do *not*
change during the process of the analysis.

#+BEGIN_LaTeX
  \begin{figure}[H]
    \begin{algorithm}[H]
      \SetAlgoVlined
      \KwData{Control Flow Graph as $(V, E)$, for every $v \in V$, $GEN_v$ and $KILL_v$ are known based on instruction semantics, $pred(v)$ is a set of predecessors of $v$ in the CFG}
      \KwResult{for every $v \in V$, $IN_v$ and $OUT_v$ are computed}
      
      \While{\text{not fixpoint}} {
        \For{$v \in V$} {
          $IN_v \gets \bigcup\limits_{u \in pred(v)} OUT_u$ \;
          $OUT_v \gets GEN_v \cup (IN_v \setminus KILL_v)$ \;
        }
      }
    \end{algorithm}
    \caption{Dense reaching definitions analysis algorithm}
    \label{fig:denseRDA}
  \end{figure}
#+END_LaTeX

** DONE Properties of reaching definitions analyses

Reaching definitions analyses have some properties\nbsp{}\cite{rptRDA} that
affect their accuracy. Less accurate analyses need to make some
conservative assumptions about the program in order to be
correct. This sub-chapter describes three properties of reaching
definitions analyses: instance-wiseness, field sensitivity and ability
to recognize execution patterns.

*** DONE Instance-wise and statement-wise analysis
When analyzing programs with a cyclic CFG, there are multiple
/instances/ of instructions that can be executed repeatedly. Each
execution of an instruction creates a new instance of the instruction.

Along with the definition, use and variable, an instance-wise reaching
definitions analysis\nbsp{}\cite{rptRDA} is able to tell which instance of the
instructions are involved. The information about instance might
involve for example the for loop indexing variable $i$. There might be
more variables in case the instruction is inside of a nested loop.

#+BEGIN_LaTeX
  \begin{figure}[H]
    \begin{lstlisting}[language=C]
int $a$ = 0; /\encircle{1}/

for(int $i$ = 0; $i$ < 5; ++$i$) {
    int $b$ = $a$ + $i$; /\encircle{2}/
    $a$ = $b$; /\encircle{3}/
}
      \end{lstlisting}
      \caption{Demonstration of differences between statement-wise and instance-wise analysis}
      \label{fig:instWise}
      \end{figure}
#+END_LaTeX

Differences between instance-wise analysis and statement-wise analysis
are demonstrated on a simple program in figure
\ref{fig:instWise}. Reaching definitions for $a$ at location
\encircle{2} are \encircle{1} and \encircle{3}. The difference is how
much information the analysis is able to provide about the reaching
definition \encircle{3} at \encircle{2}. Statement-wise analysis would
simply state, that \encircle{3} is a reaching definition of $a$ at
\encircle{2}. Instance-wise analysis goes a little further by
reporting, that $\encircle{3}^{i+1}$ is a reaching definition of $a$ at
$\encircle{2}^i$. The upper index denotes the index of iteration.

*** DONE Field sensitivity
Usage of aggregated data structures, such as arrays or C language
=struct=-s introduces another issue that needs to be addressed by a
reaching definitions analysis. Precision of analysis for programs that
use aggregated data structures depends on whether the analysis can
distinguish between individual elements of the data structure\nbsp{}\cite{rptRDA}.

#+BEGIN_LaTeX
  \begin{figure}
    \begin{lstlisting}[language=C]
int $a$[5];
$a$[0] = 1; /\encircle{1}/
$a$[1] = 2; /\encircle{2}/
foo($a$[2]); /\encircle{3}/
    \end{lstlisting}
    \caption{Demonstration of field-sensitive reaching definitions analysis}
    \label{fig:rdaFS}
    \end{figure}
#+END_LaTeX

Consider the program in Figure\nbsp{}\ref{fig:rdaFS}. Locations \encircle{1}
and \encircle{2} in the program define the first and the second
element of $a$. After that, location \encircle{3} contains a function
call that uses the third element of the array. This element has no
definitions in the program, so an accurate reaching definitions should
find no definitions for it.

A field-sensitive analysis considers array indices and correctly
reports no reaching definitions for $a[2]$ at location \encircle{3}.

A field-insensitive analysis ignores indices of the array and for
location \encircle{3}, it would report, that reaching definitions of
$a[2]$ are \encircle{1} and \encircle{2}. This is an
over-approximation that has to be performed by the field-insensitive
analysis.
*** DONE Execution patterns recognition

#+BEGIN_LaTeX
  \begin{figure}
    \begin{lstlisting}[language=C]
int foo(int $a$) {
int $c$ = 0;
if ($a$ < 0) {
  $c$ = 1; /\encircle{1}/
}
if (a >= 0) {
  $c$ = 2; /\encircle{2}/
}
return $c$; /\encircle{3}/
}
    \end{lstlisting}
    \caption{Demonstration of effects of execution patterns recognition on reaching definitions analysis}
    \label{fig:executionPatterns}
  \end{figure}
#+END_LaTeX

Reaching definitions analysis is often not the only analysis that is
part of a program analysis framework. More often than not, there are
more analyses that derive various properties of program or its
parts. Reaching definitions analysis can sometimes take advantage of
results of previously ran analyses and achieve better accuracy or
speed\nbsp{}\cite{rptRDA}.

Consider the program in figure\nbsp{}\ref{fig:executionPatterns}. If an external
analysis reports that there is no program execution where $a < 0$, the
reaching definitions analysis could take this into account and derive
that \encircle{1} is not a reaching definition of $c$ at \encircle{3}
even despite the fact it is a definition of a simple
variable. Analysis that does not take it into account would report
that both \encircle{1} and \encircle{2} are reaching definitions of
$c$ at \encircle{3}.

In this case, an analysis that does not recognize execution patterns
yields an over-approximation, which is not a problem.

*** DONE Using strong and weak definitions
The mentioned properties increase accuracy of an RDA. Accuracy of the
analysis comes at the cost of performance. Because of that, it is
desirable to trade accuracy for better performance in some cases. In
order not to sacrifice too much accuracy, analyses distinguish between
/strong/ and /weak/ definitions.

A\nbsp{}strong definition over-writes the variable with a new value. When
a\nbsp{}strong definition is encountered, it invalidates all previous
definitions of the variable. Weak definition, on the other hand, does
not necessarily over-write the variable, so it does not invalidate
previous definitions. In the dense algorithm discussed in
Section\nbsp{}\ref{denseRDA}, strong definitions are both in the $KILL$
sets.

** DONE Analyzing programs that use pointers
One of the most important features of programming languages are
pointers. They can be utilized to implement dynamic data structures,
which are very widely used. As pointers make it possible to create
variables that refer to variables, they inherently make programs more
difficult to understand and analyze. In order to compute reaching
definitions in programs that use pointers, an RDA must use information
from pointer analysis which took place prior to the RDA.

*** DONE Pointer analysis
Pointer analysis\nbsp{}\cite{ChalupaPTA} is, similarly to reaching
definitions analysis, a static program analysis. It computes a set
$\mathcal V$ of variables for each pointer $p$. This set will be
referred to as /points-to/ set. If $p$ may point to some variable $v$,
then $v \in \mathcal V$.

Reaching definitions analysis uses the data from pointer analysis to
recognize possible uses and definitions of variables. Accuracy of the
reaching definitions analysis, therefore, depends on accuracy of the
underlying pointer analysis. Namely, when the pointer analysis
performs an over-approximation, so will the reaching definitions
analysis.

*** DONE Weak definitions in programs with pointers
\label{strongWeakUpdate} Reaching definitions analyses that process
programs with pointers need to use weak definitions. Had they used
strong definitions, they could yield incorrect results.

The first case is, that a pointer could point to multiple
variables. In this case, every definition via such pointer must be
considered as a weak definition, because it could over-write either of
the memory objects while leaving the other untouched.

#+BEGIN_LaTeX
    \begin{figure}
      \begin{lstlisting}[language=C]
  int *foo() {
      return malloc(sizeof(int)); /\encircle{3}/
  }
    
int *a = foo();
int *b = foo();
*a = 1; /\encircle{1}/
*b = 2; /\encircle{2}/
      \end{lstlisting}
      \caption{Demonstration of weak definitions of heap-allocated memory. Source: DG library documentation}
      \label{fig:heapWeak}
    \end{figure}
#+END_LaTeX

Consider the program in figure\nbsp{}\ref{fig:heapWeak}. The program could
case problems if the pointer analysis used by RDA uses statements to
identify memory objects. Two objects allocated by the same statement
are then treated as the same memory. This is not accurate, as $a$ and
$b$ are two distinct memory objects. If \encircle{2} is labeled as
a\nbsp{}strong update, the definition at \encircle{1} would be over-written
be the definition at \encircle{2}, because they were allocated by the
same statement - =malloc= at \encircle{3}. As a consequence, the RDA
has to treat definitions of heap-allocated memory as weak definitions.

Apart from the dense algorithm, several other algorithms to compute
reaching definitions have been introduced. Other algorithms are
generally based on traversing the CFG of a program and processing only
definitions and uses of variables. They also attempt to eliminate need
to use fixpoint in the computation. The following section briefly
introduces demand-driven reaching definitions
analysis.

** DONE Demand-driven reaching definitions analysis

The main idea of demand-driven approach\nbsp{}\cite{SootDDRDA} is to answer
the question ``can a definition $d$ of variable $v$ reach a program
point $p$?''. This question is referred to as /query/ and it is
represented by a triple $(d, p, v)$. After a query is generated, it is
propagated backwards along nodes of the CFG. Each node may either
answer the query or continue the propagation to its predecessors. If a
node $x$ contains a definition of $v$, the query propagation
stops. The answer is yes, if and only if $x = d$. If $x \ne d$, then
node $x$ kills the definition $d$ before it can reach $p$ along the
path.

In case a program point $p$ has $n$ predecessors, it is sufficient
that the reachability of $d$ is reported by at least one of them.

It is worth noting, that this approach has a special property that
makes it suitable for a slicer: It is able to start from the slicing
criterion and gradually find all definitions that affect the
criterion. This way, it can avoid computing irrelevant information.

** DONE Sparse dataflow analysis
Another approach to computing reaching definitions was introduced by
Madsen and M\o{}ller \cite{MadsenSDAPR}. This approach requires
pre-computing dominator tree\nbsp{}\cite{CytronSSA} for nodes of the
CFG, as explained in section\nbsp{}\ref{domTree}.

When the algorithm encounters a use of a variable, it searches
dominator tree of the program backwards until it finds a definition of
the same variable. The triple $(d, v, u)$ where $d$ is a definition of
a variable $v$ and $u$ is a use of $v$, is then added to $DU$ set.

When a new definition $d_n$ of variable $v$ is encountered, the
algorithm finds a set $\mathcal D_p$ of previous definitions of
$v$. Then, for each $d_p \in \mathcal D_p$ where $d_n$ is a strict
dominator of $d_p$, all triples $(d_p, v, u) \in DU$ are removed from
$DU$.

While processing definitions and uses, the algorithm places \Phi nodes
for variables when necessary. As a side-effect, SSA form of the
program is produced.

The input program is processed by the algorithm until fixpoint -- there is no new use
discovered.

** DONE Algorithms based on static single assignment form
\label{SSArd} Algorithms that transform a program into SSA form
replace modified variables in assignments by new, artificially-created
variables representing a new ``version'' of the variable. They also
replace variables in uses by the most recent definition -- reaching
definition. Reaching definitions are a side-effect of transformation
to SSA form.

For the purpose of this thesis, we have studied two algorithms for
computing SSA form. One of them has been introduced by Cytron et
al\nbsp{}\cite{CytronSSA}.  The second algorithm, invented by Braun et
al\nbsp{}\cite{BraunSSA}, is simpler and has been experimentally proven to
be as fast as the Cytron et al. algorithm\nbsp{}\cite{BraunSSA}.

*** DONE Cytron et al algorithm

Algorithm introduced by Cytron et al.\nbsp{}\cite{CytronSSA} uses dominance
information to find locations of \Phi nodes, so it requires the dominator
tree of nodes in the CFG to be computed already. It also requires to
have a set $\mathcal A(\mathcal V)$ for every variable $\mathcal V$,
that contains all definitions of $\mathcal V$.

The algorithm starts by computing dominance frontiers from a dominator
tree. Dominance frontiers are then used to compute where in the
program should \Phi nodes be placed. \Phi node positions are computed for
each variable individually. After positions of \Phi nodes are computed,
the CFG is traversed once again and value numbering takes place for
all variables at once.

This approach is proven to produce minimal SSA form\nbsp{}\cite{CytronSSA}.

*** DONE Braun et al algorithm
\label{marker}

Algorithm by Braun et al.\nbsp{}\cite{BraunSSA} will be used as a base for
implementation of the new analysis, so it is discussed more in depth.
The algorithm operates in two phases: /local value numbering/ and /global
value numbering/. Both of these phases process basic blocks of the
program in the execution order.

During *local value numbering*, it computes SSA form of every basic
block of the program. For every basic block, it iterates through all
instructions in execution order. If an instruction $I$ defines some
variable $\mathcal V$, $I$ is remembered as the current definition of
$\mathcal V$. If an instruction $I$ uses some variable $\mathcal V$,
the algorithm looks up its definition. If there is a current
definition $\mathcal D$, the use of variable $\mathcal V$ is replaced
by use of the numbered variable that corresponds to $\mathcal D$.

*Global value numbering* is involved once no definition for the
specified variable can be found in the current basic block. The
algorithm places a \Phi node on top of the current basic block and starts
recursively searching the CFG for the latest definition in all
predecessors of the current basic block. Once a definition is found,
it is added as an operand to the \Phi node.

When looking up a definition of a variable from a predecessor basic
block, the basic block might not be processed by global value
numbering. If that is the case, the algorithm does not have any idea
about which variables are defined in that basic block. This happens
when the program's CFG is cyclic -- e.g. recursive function is called
or for loop is used. Because of that, the algorithm remembers the last
definition of variable in basic blocks during local value
numbering. If there is no last definition in a block, the lookup
continues to all predecessors recursively.

The key part of the algorithm can be seen in
Figure\nbsp{}\ref{fig:braunSSA}. Braun et al. present a way to reduce the
number of added \Phi nodes, which allows their algorithm to produce
minimal SSA form. That part of the algorithm is not too important, so
we can assume that a call to =tryRemoveTrivialPhi(phi)= always returns
=phi= for simplicity.

#+BEGIN_LaTeX
  \begin{figure}[H]
    \begin{algorithm}[H]
      \SetAlgoVlined
      \SetKw{In}{in}
      \SetKw{Not}{not}
      \SetKw{New}{new}
      \SetKw{Contains}{contains}
      \SetKwFunction{WriteVariable}{writeVariable}
      \SetKwFunction{ReadVariable}{readVariable}\
      \SetKwFunction{ReadVariableRecursive}{readVariableRecursive}
      \SetKwFunction{AddPhiOperands}{addPhiOperands}
      \SetKwFunction{TryRemoveTrivialPhi}{tryRemoveTrivialPhi}
      \SetKwFunction{NewPhi}{Phi}

      \Fn{\WriteVariable{$variable, block, value$}} {
        $currentDef[variable][block] \gets value$ \;
      }
      \Fn{\ReadVariable{$variable, block$}}{
        \If{$currentDef[variable]$ \Contains $block$} {
          \Return $currentDef[variable][block]$ \;
        }
        \Return \ReadVariableRecursive{$variable, block$} \;
      }
      \Fn{\ReadVariableRecursive{$variable, block$}} {
        \uIf{$block$ \Not \In $sealedBlocks$} {
          $val \gets$ \New \NewPhi{block} \;
          $incompletePhis[block][variable] \gets val$ \;
        } \uElseIf{$\lvert block.preds \rvert = 1$} {
          $val \gets$ \ReadVariable($variable, block.preds[0]$) \;
        } \Else{
          $val \gets$ \New \NewPhi{$block$} \;
          \WriteVariable{$variable, block, val$} \;
          $val \gets$ \AddPhiOperands{$variable, val$} \;
        }
      }
      \Fn{\AddPhiOperands{$variable, phi$}} {
        \For{$pred \in phi.block.preds$} {
          phi.appendOperand(\ReadVariable{$variable, pred$}) \;
        }
        \Return \TryRemoveTrivialPhi{phi} \;
      }
    \end{algorithm}
    \caption{Braun et al. algorithm pseudocode. Source: Braun et al.~\cite{BraunSSA}}
    \label{fig:braunSSA}
  \end{figure}
#+END_LaTeX

* DONE Symbiotic

\label{ch:Symbiotic} \sbt{} is a modular tool for formal verification
of programs working. It is being developed at Faculty of
Informatics, Masaryk University. \sbt{} works by combining three
well-known techniques:

1. *Instrumentation* is responsible for inserting various error checks
   into the program. For example, when checking memory access errors,
   instrumentation is responsible for registering the allocated memory
   along with allocation size to a global data structure. When
   dereferencing a pointer, instrumentation inserts a check to verify
   whether this pointer is inside allocated bounds or not. An
   assertion that crashes the program if a dereference is out of
   bounds of allocated memory is inserted, too.
2. *Program Slicing*\nbsp{}\cite{ChalupaDG} is a technique that reduces the
   size of the program by removing parts that do not influence its
   behaviour with respect to a specified /slicing criterion/. Slicing
   criterion consists of several =assert= calls. The slicer computes
   which instructions the slicing criterion is dependent on. For that,
   it uses results of reaching definitions analysis.
3. *Symbolic execution* is the last step. It is a technique that
   decides whether the program could violate a condition of some
   assertion in the program. Rather than requiring user input, it uses
   so-called symbolic values. Whenever there is a program branching
   based on the symbolic value, the symbolic virtual machine remembers
   a constraint of the value based on the branching condition. When an
   erroneous state is reached, the symbolic virtual machine reports
   the path in the program that leads to the error.

\sbt{} is based on the LLVM compiler infrastructure\nbsp{}\cite{LLVM}. LLVM
is introduced in the following section.

** DONE LLVM
LLVM\nbsp{}\cite{LLVM} is an infrastructure for compilers and optimizers. It
consists of multiple libraries and tools. One of the tools is clang[fn::https://clang.llvm.org/] -- a compiler of C language.

LLVM defines its own intermediate representation(LLVM IR) of a program. The
representation looks very similar to assembler.

\label{partialSSA} Any program in LLVM IR is guaranteed to be in
/partial SSA form/. Partial SSA form means, that there is at most one
definition for each register. This form of program, however, makes no
guarantees about variables in memory. Those are *not* in SSA
form. Thanks to the partial SSA transformation, LLVM already provides
reaching definitions information for its register variables.

* DONE Implementation
\label{ch:Implementation} This chapter starts by introduction of the
DG library and the LLVM infrastructure, continues by discussing
designed modifications of the Marker algorithm and finally, the new
reaching definitions analysis implementation.

** DONE DG Library
The slicer used in \sbt{} uses the DG library\nbsp{}\cite{ChalupaDG} to
create dependence graph and slice away unnecessary parts of
verified program. New reaching definitions analysis has been
implemented to the DG library, so it can be used with any software
that uses DG.

Before processing any program, DG loads the program into its own
framework. Analyses that are part of DG are independent of the program
representation, because they only use DG framework which handles the
details. However, DG currently supports only LLVM intermediate
representation.

*** DONE Pointer analysis in DG
The new reaching definitions analysis requires information from a
pointer analysis. DG already contains a pointer analysis, which can be
utilized. However, there are two important implementation details that
need to be adressed by any RDA that uses results of this pointer
analysis.

In some cases, the pointer analysis is unable to determine which
variables to pointer points to. It happens for example in case the
pointer is returned from a function from an external library that is
not part of the program. The pointer analysis returns that the pointer
points to a virtual node called ``unknown memory''. This has to be
addressed later in the reaching definitions analysis.


The pointer analysis in DG is field-sensitive, which opens a
possibility to implement a field-sensitive RDA as well. There are
multiple approaches to addressing field-sensitivity. One of them
involves considering each element of an aggregated data structure as a
separate variable. The pointer analysis in DG uses another approach:
it reports which memory object is being accessed and what part of the
object is being accessed. The part of the object is specified by a
pair $(offset, length)$, where both $offset$ and $length$ are in
bytes. In some cases, the $offset$ can be unknown. This case needs to
be addressed by the RDA, too.

*** DONE Reaching definitions analysis framework in the DG library
DG uses reaching definitions analysis to calculate data dependencies
between instructions. The original reaching definitions analysis in DG
uses the dense approach, as described in section \ref{denseRDA}.

Prior to the reaching definitions analysis itself, DG builds a
subgraph of program's control flow graph\index{CFG} from the program
representation. The subgraph does not contain all types of
instructions. Rather, it consists only of store instructions, call
instructions, return instructions and all memory allocations. In spite
of not containing all instructions, it reflects structure of the
program. Each instruction in the subgraph that defines some memory
object already has an associated points-to information from pointer
analysis. Thanks to this, it is possible to tell which variables are
strongly or weakly defined in a particular CFG node.

** DONE Implemented reaching definitions analysis algorithm

The implemented reaching definitions analysis is based on the Marker
algorithm\nbsp{}\cite{BraunSSA}. As described in\nbsp{}\ref{marker}, the algorithm
transforms a program into SSA form, which is not exactly what we
need. We start by adapting the algorithm to compute reaching
definitions.

*** DONE Computing reaching definitions from Marker algorithm
In SSA form, every use of a variable has exactly one reaching
definition. Thanks to this property, it is trivial to compute reaching
definitions in a program that is in SSA form. Thus, transforming
memory operations in the program into SSA form yields reaching
definitions. We split up the computation into two phases:
1. In the first phase, the implementation constructs a /sparse RD
   graph/ separately for every allocated variable. Sparse RD graph is
   a graph, where for every reaching definition $(I_1, I_2)$ exists a
   path $P = (p_1, p_2, \cdots, p_n)$ where $p_1 = I_1$ and $p_n = I_2$. Each node
   $p \in P$ is either a definition, use or a \Phi node. The path may
   consist of multiple \Phi nodes, but it might be trivial as well. The
   construction is straightforward: whenever a variable use $u$ is
   encountered, lookup the definition of the variable (using
   =readVariable=). If a \Phi node $y$ is created as a result, add an
   edge $(x, y)$ to the sparse RD graph for each operand $x$ of
   $y$. Then, for the definition $d$ of the variable returned by
   =readVariable=, add an edge $(u, d)$ to the sparse RD graph.
2. In the second phase, the control flow graph $(V, E)$ of the program
   is traversed once again. For every use $u \in V$ of variable $v$, a
   BFS search of the sparse RD graph for $v$ is started in $u$. If the
   definition found is not a \Phi node, it is added as a reaching
   definition. If it is a \Phi node, the search continues to its
   predecessors.

The original dense analysis is field-sensitive. In the next section,
we modify the new algorithm to be field-sensitive too.

*** DONE Field sensitivity
\label{ch:implFieldSens} Every definition and use have an associated
interval of bytes in memory that is being accessed by the
instruction. Data structure used for =current_def= and =last_def= does
not consider the interval when looking up definitions in
=readVariable=. We have decided to design a new custom data structure
that considers the intervals while looking up variables. The data
structure works similarly to a map which maps intervals to values of
some type -- in this case CFG nodes. We call it =IntervalMap=.

When a definition is encountered, it is necessary to save the interval
of the definition along with the CFG node where the definition is to
the =IntervalMap=.

When a use is encountered, modified =readVariable= function looks up
overlapping definitions from the =IntervalMap=. =readVariable=
is modified to return a set of definitions rather than a single
definition. That is because two or more subintervals of the used
interval could be defined by different instructions and all of the
instructions are reaching definitions, as they do not over-write one
another completely.

When =readVariable= finds a definition in the current block of a
subinterval $i_S$, which is smaller than the use interval $i_U$, the
lookup must continue to predecessor blocks. In each predecessor block,
it attempts to find a set of intervals $\mathcal I$ such that $(i_U
\setminus i_S) \subseteq \bigcup_{i \in \mathcal I}$. In other words, find
definitions for the ``missing'' parts of the interval. The search for
definition ends once the set is found for every predecessor basic
block of the current basic block or when the entry node of the CFG is
reached.

The =readVariableRecursive= function adds \Phi nodes for the variable
when necessary. Whenever a \Phi node is created, the definition and use
represented by the \Phi node have the same interval as the use it is
created for.

Sometimes, the accessed interval of memory is not known at the time of
compilation. In this case, the interval is stretched to the whole
interval of allocation variable, if known. If the allocation size is
not known either, maximum allocation size is used. When there is a
definition of an unknown interval, the analysis must assume it could
be definition of any part of the interval. Multiple definitions of
unknown intervals should not kill each other, as they could both be
reaching definitions for all uses reachable in the CFG by a path where
the whole range of the variable is not over-written. This issue is
addressed in the following section.

*** DONE Strong and weak definitions
As the algorithm needs to remember multiple definitions in case the
interval is unknown or a pointer might point to multiple variables, we
use weak definitions to achieve that. Marker algorithm again needs to
be modified to consider them.

We extend the Marker algorithm with two new map structures:
=current_weak_def= and =last_weak_def=. The semantics is similar to
=current_def= and =last_def= from the Marker algorithm.

In =writeVariable=, the choice of the structure where to save the
definition gets a little more complex again. Weak updates will be
saved to =last_weak_def= or =current_weak_def= depending on the
context, while strong updates will be saved to =last_def= or
=current_def=. When encountering a strong definition, intervals of
weak definitions need to be modified not to overlap with the strong
definition. This way, the strong definition ``kills'' the weak
definition. We extend the =IntervalMap= data structure to allow this.

In the previous section, we have mentioned that =readVariable= can
stop the search for definitions once it finds a set of definitions
that ``covers'' the interval of use. We may not add the weak
definition in the set of intervals $\mathcal I$, but we add it to the
result as a reaching definition. Only strong definitions are added to
the set of intervals. We demonstrate why using a simple program in
Figure\nbsp{}\ref{fig:weakUnknown}.

#+BEGIN_LaTeX
  \begin{figure}
    \begin{lstlisting}[language=C]
int $a$[10];
int $b$ = rand() % 10;

$a$[0] = 5; /\encircle{1}/
$a$[$b$] = 1; /\encircle{2}/

printf("%d", $a$[0]); /\encircle{3}/      
    \end{lstlisting}
    \caption{Demonstration of weak definitions of unknown offsets}
    \label{fig:weakUnknown}
  \end{figure}
#+END_LaTeX

Let us assume, the =rand= function returns a nondeterministic random
integer. In runs of the program where $b = 0$, it is correct to report
that reaching definitions fo $a[0]$ at \encircle{3} is only
\encircle{2}. However, the value of $b$ is unknown before the program
is started. The value of $b$ could be non-zero, so it would be
incorrect to over-write the definition at \encircle{2} by the
definition at \encircle{1}. Thus, the definition at \encircle{2} has
to be weak.

*** DONE Sealed blocks
The Marker algorithm is capable of constructing SSA form of programs
while loading the program representation from a file. Because of this,
it maintains a set of blocks called =sealedBlocks=, that holds all
blocks that already have all their predecessors added. In our case, we
already have the whole program loaded, so we can consider all of our
basic blocks to be sealed\nbsp{}\cite{BraunSSA}.

** DONE New reaching definitions analysis implementation
This chapter describes how the new reaching definitions analysis has
been implemented in the existing framework. A technical guide on how
to run the implementation can be found in Appendix\nbsp{}\ref{ch:testing}.

Thanks to LLVM's transformation to partial SSA form (as described in
\ref{partialSSA}), there is no need to compute reaching definitions of
LLVM register variables. Reaching definitions for register variables
have already been computed while translating the C program into LLVM
Intermediate Representation (LLVM IR). Therefore, the implementation
focuses on address-taken variables.

*** DONE Subgraph builder abstractions
Each reaching definitions analysis in the DG library could require
different set of information in the reaching definitions subgraph. The
new analysis requires information about uses in the graph, which are
not added by the current subgraph builder. With that in mind, we have
decided to allow each RDA to use different subgraph builder. A
subgraph builder builds a reaching definitions subgraph from some
representation.

The goal is to allow the user of =ReachingDefinitions= class to run
any reaching definitions analysis they choose. The pointer analysis
framework in the DG library already allows the user to specify pointer
analysis to run using templates. We will do something similar to the
reaching definitions analysis.

We have designed and implemented an interface for subgraph builders
from the LLVM IR called =LLVMRDBuilder=. This interface allows us to
implement a =build= function, that returns the entry node of the
reaching definitions subgraph. The implementation of the new subgraph
builder is very similar to the original implementation, with two major
differences. The new subgraph builder splits up LLVM basic blocks when
a function call is encountered and it also adds information about
which memory is used in which CFG node. These additions are discussed
in the following two sections.

*** DONE Adding use information to control flow graph
Now, the subgraph builder can add information about uses of variables
to the reaching definitions subgraph. Pointer analysis is utilized
here to find out which variables are being used. As one pointer could
simply point to multiple variables, it is necessary to add information
about all variables that could potentially be used.

In the new subgraph builder used with the new analysis, we have
included LLVM's instructions that use memory pointed to by a
pointer. For each node that is a use of some memory, it queries the
underlying pointer analysis for all variables the pointer operand
could point to. For looking up the variables, it uses a
newly-introduced method =getPointsTo=, which fetches the information
from the pointer analysis.

The instruction that is a use could possibly use a smaller portion of
the memory than the allocation size. This is the case when accessing
an individual element of a larger data structure. A field-sensitive
reaching definitions analysis requires the length to be set to the
length that is being used. This is done by determining size of the
type the value is being loaded to.

*** DONE Splitting basic blocks on function calls
The original RDA does not need information about basic blocks in the
program. This is required by the new analysis, so the new
implementation of subgraph builder has to add the information into the
subgraph.

The basic block used by LLVM IR is more or less suitable for the new
analysis, with a major problem: When a function is called, the call
instruction does not end a LLVM IR basic block. This is against the
definition of a basic block introduced in\nbsp{}\ref{ch:KTPA}, as a call
instruction is a jump to a different address.

#+BEGIN_LaTeX
  \begin{figure}
    \begin{lstlisting}[language=LLVM]
%1 = alloca i32 align 4
store i32 1, i32* %1
call void foo(i32* %1)
store i32 2, i32* %1
    \end{lstlisting}
    \caption{Demonstration of an LLVM basic block. All instructions shown here are in the same LLVM basic block.}
    \label{fig:llvmBlocks}
  \end{figure}
#+END_LaTeX

Consider the program in figure\nbsp{}\ref{fig:llvmBlocks}. The block calling the function would be
processed first and =foo= would then see the =store i32 2, %1= instruction
as a reaching definition of =%1=. This is, however, not correct as the
instruction has not been executed yet. Because of that, we have
decided to split up an LLVM IR basic block with every call statement,
too.

#+BEGIN_LaTeX
  \begin{figure}
    \begin{lstlisting}[language=LLVM]
      /\hline/
      /\encircle{1}/
%1 = alloca i32
store i32 1, i32* %1
      /\hline/
call foo(i32* %1)
      /\hline/
      /\encircle{2}/
store i32 2, i32* %1
      /\hline/
    \end{lstlisting}
    \caption{Demonstration of program division into basic blocks in the new subgraph builder. Horizontal lines show block borders.}
    \label{fig:basicBlocks}
  \end{figure}
#+END_LaTeX

Figure\nbsp{}\ref{fig:basicBlocks} the way of splitting basic blocks of
program in Figure\nbsp{}\ref{fig:llvmBlocks} in the new implementation. The
new implementation of subgraph builder splits up a basic block when
there is a function call. Block \encircle{1} gets one predecessor,
which is the first basic block of the function =foo=. Basic block
\encircle{2} is then added as a successor of the last basic block of
the function =foo=.

Basic block splitting is only necessary if the function's definition
is part of the program. In case the function is external, there is no
need to split up the basic block, because the instructions in the
block are not known. The call instruction is in this case treated as a
use of all pointer operands and optionally also definition of all
pointer operands.

*** DONE Treating unknown memory
\label{ch:unknownMemory}
Sometimes, pointer analysis is unable to tell where a pointer may
point, so the analysis has to make some conservative assumptions about
the program in order to be correct. In this case, the analysis assumes
that such pointer could point to any variable and treats the CFG node
as if it was a definition or a use of all variables in the
program. Whether it is a definition or a use is decided based on
semantics of the instructions and how the pointer is used.

After the subgraph is built, it is searched by a separate class
=AssignmentFinder=, which does exactly what was explained above. It
uses a two-phase algorithm to do that: In the first phase, all
variables in the program are added to a list. In the second phase,
every store to an unknown pointer and load from an unknown pointer is
turned into a weak definition of all variables in the program or use of
all variables in the program, respectively. Doing this removes some
complex handling of unknown pointers from the next phase of the
analysis.

*** DONE Using intervals to handle field-sensitivity
\label{chap:intervals} The Marker algorithm itself does not consider
aggregate data structures. We have introduced several modifications in
order to incorporate it. As mentioned before in
section\nbsp{}\ref{ch:implFieldSens}, we use a different data structure for
the work structures of the Marker algorithm. This section describes
how the new data structure is implemented and used.

=IntervalMap= is the most important data structure of the
framework. =IntervalMap= on the first sight looks similarly to
=std::map= available in C++. It allows to save arbitrary types under
=Interval= keys. The difference is in the lookup
functions. =IntervalMap= offers 3 main functions: =collect=,
=collectAll= and =killOverlapping=.

The =collect= function is designed to work with strong definitions. It
searches the entries backwards, starting by the last entry added.  It
collects all values from the interval map such, that the specified
interval is covered by union of key intervals of the values returned.

=collectAll= works with weak updates. As opposed to =collect=, it
does not stop when the specified interval is subset of union of the
result key intervals. Rather, it searches the whole IntervalMap and
returns all values which are saved under intervals that overlap with
the specified interval.

=killOverlapping= deletes definitions with intervals that overlap with
specified interval. After =killOverlapping=, calling =collectAll= with the
same interval or any of its subsets returns an empty result.

Each definition or use of a variable have an associated interval of
affected bytes. This interval is later used to look up reaching
definitions of a variable. An interval has a start and a length.

The first intermediate data structure that is part of the new
framework is =DisjointIntervalSet=. The set allows to insert intervals
while maintaining an invariant, that all intervals inside are
disjoint. When inserting an interval that has a non-empty intersection
with some of the intervals inside, the set ensures that these two
intervals are united into a single interval.

=IntervalMap= is used as a data structure for structures that are
needed by Braun et al. algorithm -- that is =current_def=,
=current_weak_def=, =last_def= and =last_weak_def=. This way, the
field-sensitivity is considered in the phase of building the sparse RD
graph.

The =IntervalMap=, is unable to handle an unknown offset. Addressing
unknown offset requires further modifications of the algorithm which
are discussed in the following section.

\clearpage
*** DONE Treating unknown offset
When the pointer analysis returns an unknown offset of a definition or
a use of a variable, the RDA needs to address it. In case there is a
definition of an unknown offset of a variable, it could be definition
of any of its bytes, so the new analysis performs an
over-approximation. In the over-approximation, the analysis assumes
that the whole variable is defined. However, this definition may not
be considered as a strong definition.

#+BEGIN_LaTeX
    \begin{figure}
      \begin{lstlisting}[language=C]
int $i$, $j$;
int $a$[10];
$a$[$i$] = 0;
$a$[$j$] = 1;
printf("%d", $a$[0]); /\encircle{1}/
      \end{lstlisting}
      \caption{Using weak definitions to handle unknown offset}
      \label{fig:unknownOffset}
      \end{figure}
#+END_LaTeX

Consider the program in figure\nbsp{}\ref{fig:unknownOffset}. Assuming the
values of $i$ and $j$ are unknown, both of those definitions could be
reaching definitions of $a[0]$ at \encircle{1}. Thus, the analysis has
to assume they are weak definitions despite the fact that $a$
points to a single memory object -- the array.

* TODO Experimental evaluation of the new analysis
\label{ch:Experiment} In this chapter, the new implementation is
evaluated experimentally. For the evaluation, we have used a subset of
benchmarks from the software verification competition
SV-COMP[fn::https://sv-comp.sosy-lab.org]. Each benchmark is given as
a C program with a list of properties it satisfies. After running
single benchmark, output of \sbt{} is inspected and compared with the
expected output. We measure the CPU time it took \sbt{} to compute
individual phases of the process, including reaching definitions
analysis.

** TODO Time
- the new analysis is slower
- explain why
  - redundant phi nodes
  - def/use of unknown memory
  - interval map
** TODO Out of memory errors
# TODO maybe extend this section?
Experiments revealed some cases where the new implementation ran out
of memory on benchmarks where the original implementation did not. We
have identified two possible causes for this:

1. Handling of definitions and uses of unknown memory (discussed
   in\nbsp{}\ref{ch:unknownMemory}) requires too much memory.
2. Intervals framework used for field-sensitivity consumes too much
   memory. Replacing it with a bit vector could result in more optimal
   memory consumption.

** DONE Accuracy
There should be no difference between the new and the original
analysis in terms of accuracy. However, we have noticed different
results of slicing with the new implementation. Thanks to the interval
framework introduced in\nbsp{}\ref{chap:intervals}, the new implementation
of semi-sparse analysis is more accurate than the original
implementation.

#+BEGIN_LaTeX
  \begin{figure}
    \centering
    \begin{lstlisting}[language=C]
int $a$[] = {0, 1, 2, 3}; /\encircle{1}/
$a$[0] = 5; /\encircle{2}/
$a$[1] = 6; /\encircle{3}/
$a$[2] = 7; /\encircle{4}/
$a$[3] = 8; /\encircle{5}/

for (size_t $i$ = 0; $i$ < 4; ++$i$) {
    printf("%d\n", $a$[$i$]); /\encircle{6}/
}

    \end{lstlisting}
    \caption{Demonstration of accuracy of the old and the new implementation}
    \label{fig:strongCoverage}
  \end{figure}
#+END_LaTeX


Consider the program in Figure\nbsp{}\ref{fig:strongCoverage}. Now, let us
investigate what instructions should be reaching definitions of
$a[i]$ at \encircle{6}. The offset is unknown, so our implementation
looks for definitions for the whole array. It finds $\{ \encircle{5},
\encircle{4}, \encircle{3}, \encircle{2} \}$. The array only has four
elements, so the search for definitions is stopped at
\encircle{2}. However, the original implementation does not stop the
search for definitions, so it finds $\{ \encircle{5}, \encircle{4},
\encircle{3}, \encircle{2}, \encircle{1} \}$.

With our new implementation, the slicer can slice away the
instructions that initialize the array (at \encircle{1}), because they
are over-written by \encircle{2}, \cdots \encircle{5}. As a result, the
slicer is able to produce slightly smaller programs in cases similar
to the one presented above.

* DONE Conclusion
\label{ch:Summary} This chapter summarizes the work done as part of
this thesis and presents future work.

** DONE Summary of work done
As a part of this thesis, we studied four algorithms for computing
reaching definitions. Then, we chose to implement an algorithm based
on the Braun et al. algorithm into \sbt{}. Prior to implementation, we
have designed modifications for the algorithm to compute reaching
definitions, work with aggregate data structures and weak updates. The
modified algorithm has been implemented into \sbt{}. The new
implementation is then compared with the original implementation in
terms of accuracy, time and memory used. We found that the new
analysis consumes too much memory in some cases and identified the
cause of it. The new implementation also turned out to be more
accurate than the original one. We also experimentally proved the new
implementation to be faster than the original one in majority of
cases.

** DONE Future work

Uses of unknown memory in the current implementation consume too much
memory and also time. In the future, we will optimize how our
implementation treats definitions and uses of unknown
memory.

It is possible to further speed up computation of reaching definitions
by incorporating the trivial phi node removal algorithm introduced by
Braun et al\nbsp{}\cite{BraunSSA}. The sparse graph contains many redundant
\Phi nodes that could be removed to speed up the final phase of reaching
definitions propagation.

As the algorithm is implemented in a slicer, it could be optimized
even further by increasing its laziness. We can do something to what
Lu, Zhang and Zhao\nbsp{}\cite{SootDDRDA} did with their analysis. The
analysis would start at the slicing criterion and search the CFG
backwards only for definitions of variables that really affect the
slicing criterion.

Performance of the =IntervalMap= could be definitely improved by using
a different data structure in the background. A good candidate would
be a bit vector.

The RDA algorithm itself is not the only place for optimization. Newer
versions of LLVM support a pass called
mem2reg[fn::https://llvm.org/docs/Passes.html\#mem2reg-promote-memory-to-register]. This
pass is able to convert local pointer-based variables into registers,
which are already in SSA form. It would be interesting to use mem2reg
pass whenever possible and then run this analysis to obtain results
for arrays and other structures mem2reg is unable to handle. We think
using the pass could reduce the number of variables our analysis needs
to process.

Another interesting LLVM pass to test would be scalar replacement of
aggregates[fn::https://llvm.org/docs/Passes.html\#sroa-scalar-replacement-of-aggregates]. This
pass replaces arrays and structures by scalar values in case it is
possible.

\appendix

* TODO Running the new implementation
\label{ch:testing} The new reaching definitions analysis is part of
the DG library. The source code along with build files can be found in
=dg.zip=. This guide targets Ubuntu system of version at least
16.04. It has been tested on a fresh installation of Ubuntu 18.04
inside Docker.

** DONE Running inside DG

*** DONE Build dependencies
Before compiling the sources, it is necessary to install the following
ubuntu packages:

#+BEGIN_SRC sh
#  apt install build-essential cmake libz-dev llvm clang
#+END_SRC

*** DONE Compile
Now, it is necessary to unzip the attached =dg.zip=. This guide
assumes it has been unzipped to =~/dg=. As a next step, =cmake= needs
to be started in the =~/dg= directory:
#+BEGIN_SRC sh
  $ cmake .
#+END_SRC

After that, it is possible to simply run =make= from the same
directory to compile the library itself.

*** DONE Running DG tests
Regressive tests of DG are located in =~/dg/tests= directory. Running
=make= from that directory compiles the tests and running =make test=
runs them. =make= needs to be run before =make test=, or some of the
tests would not execute. Tests use the original implementation by
default. The environment variable =DG_TESTS_RDA= controls which
implementation is used. If it is set to =ss=, the new implementation
is used. Running the tests with the new implementation can be achieved
for example like this:
#+BEGIN_SRC sh
  $ DG_TESTS_RDA=ss make test
#+END_SRC

*** DONE Using the new RDA with slicer
DG library contains a simple LLVM bitcode slicer that can be used to
test the new implementation. It has to be compiled in order to use
it. Running =make= from =~/dg/tools= directory compiles it. The slicer uses the
original implementation by default. Command line parameter =-rda ss=
can be specified to use the new implementation of RDA.

There are many small example programs slicer can be tested with in the
=~/dg/tests/sources/= directory. Slicer requires to have a slicing
criterion specified via =-c= command line parameter. The test sources
always use =-c test_assert=.

Every source program has to be compiled to LLVM IR before it can be
sliced. This is done using the =clang= tool from LLVM:
#+BEGIN_SRC sh
  $ cd ~/dg/tests/
  $ clang -emit-llvm -c -include "test_assert.h" sources/test1.c
#+END_SRC

That produces =main.bc= in the current directory. In order to run the
sliced program, the slicing criterion has to be compiled and linked
into the program. This is done using combination of =clang= and
=llvm-link= tools:
#+BEGIN_SRC sh
  $ cd ~/dg/tests/
  $ clang -emit-llvm -c "test_assert.c" -o "test_assert.bc"
  $ llvm-link "test1.bc" "test_assert.bc" -o "test1_linked.bc"
#+END_SRC

The linked file can then be run via =lli=:
#+BEGIN_SRC sh
  $ lli test1_linked.bc
#+END_SRC

** TODO Compiling \sbt{} with the custom version of DG
\sbt{} itself requires the following build dependencies to be compiled:
#+BEGIN_SRC sh
  # apt-get install curl rsync bison flex gcc-multilib \
    libjson-c-dev build-essential cmake libz-dev llvm clang
#+END_SRC

*** TODO Clone \sbt{} from GitHub
\sbt{} can be found on
github[fn::https://github.com/staticafi/symbiotic]. The SHA-1 of the
commit used for testing this guide is:
=cff7857e7978a263ea923547b3c00a4544d4bbb0=. To choose that particular
commit, \sbt{} needs to be cloned like this:

#+BEGIN_SRC sh
$ cd ~
$ git clone https://github.com/staticafi/symbiotic
$ cd symbiotic
$ git checkout cff7857e7978a263ea923547b3c00a4544d4bbb
$ git submodule init
$ git submodule update
#+END_SRC

In order to use the new implementation inside \sbt{}, it is necessary
to add the modified version of dg library into \sbt{}. This guide
assumes the =dg.zip= from the thesis attachment is unzipped at =~/dg=.
#+BEGIN_SRC sh
$ cd ~/symbiotic
$ cp -r ~/dg/src/ dg/
#+END_SRC

*** DONE Compiling \sbt{}
Building symbiotic only involves running the =build.sh= script:
#+BEGIN_SRC sh
  $ cd ~/symbiotic
  $ ./build.sh -j8
#+END_SRC

As the script also builds LLVM, it is highly recommended to use the
=-j= parameter of =make= to utilize multiple CPU cores while
building. The build might take very long time -- about an hour.


If there were no problems, the command =symbiotic --version= should return the following:
#+BEGIN_SRC

#+END_SRC

*** DONE Running \sbt{}
When running symbiotic, the =--slicer-params= option can be used to
enable the new implementation of RDA. Here is a simple example:
#+BEGIN_SRC sh
$ cd ~/symbiotic/
$ ./scripts/symbiotic --slicer-params="-rda ss" \ 
  --prp MemSafety file.c
#+END_SRC

\printbibliography[heading=bibintoc]
